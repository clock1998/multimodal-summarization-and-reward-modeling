[
  {
    "article": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient\nLong Video Narratives\nAbstract\nThe core challenge for streaming video generation is maintaining content consistency over long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design not only accurately sources the context needed to maintain visual consistency, but also ensures semantic coherence even as new events unfold or scenes transition. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.\nüñÇ Corresponding Author.\n1 Introduction\nVideo generation has attained remarkable quality [yang2024cogvideox, kling, kong2024hunyuanvideo, wan2025wan, sora], making its extension to long durations critical for advancing creative and cinematic applications. While Diffusion Transformer (DiT) models [dit] leverage bidirectional attention to capture complex spatiotemporal dependencies, their inherent computational costs and GPU memory limits constrain them to short video generation. Autoregressive (AR) diffusion models [magi-1, causvid, self-forcing, skyreels-v2] offer a promising alternative by decomposing long videos into sequential clips, which alleviates the computation bottleneck through a reduced attention window.\nInteractive video generation has emerged as a crucial application for enabling users to guide narratives with streaming prompt inputs. Most existing works conduct chunk-level autoregressive generation, where new video segments are streamingly generated based on previously generated content and newly-provided text prompts. This interactive paradigm with dynamic prompt transitions allows the introduction of new elements and scene switches across extended temporal horizons. However, it also poses difficulties in effectively preserving memory for long-range content consistency due to complex inter-clip dependencies. First, since different to-generate video chunks should refer to different historical cues, the memory is required to adaptively provide relevant context according to streaming prompts; Second, the capacity of stored memory must be highly constrained, a necessity dictated by both the hardware limits of GPU memory and the demands of generation efficiency.\nWhile the necessity of such adaptive and efficient memory module is evident, many existing approaches have been overly simplistic, failing to fully address the dual challenges outlined above. They preserve memory in predefined paradigms, some only employ the first video chunk as memory sink [yang2025longlive], some attempt to store more historical frames through fixed compression schemes [framepack, far, xiao2025captain], some try to bake context implicitly with trainable memory modules [jiang2025lovic, test-time-training, hong2024slowfast]. However, those rigid strategies struggle to dynamically provide historical content corresponding to different prompt inputs, especially for new element emergence or scenario switches in prompt transitions.\nThus, we innovatively design Narrative Adaptive Memory (NAM), a memory mechanism that adaptively retrieves relevant historical content for interactive streaming video generation. Specifically, we introduce a memory bank aggregating historical visual token (KV cache) from streamingly generated chunks. During the sequential generation of each chunk, we first retrieve the context which aligns with the current prompt most, by calculating the attention score between textual token of the prompt and visual token from memory. The context frame with higher score is considered to be semantically relevant with current chunk generation, and will be integrated to update the memory along with a condensed representation of the immediately preceding chunk. This design enables the current chunk to utilize historical cues, which have truly relevant content with the new prompt. Our NAM is effective in preserving narrative coherence even if new event happens or scenario switches, which is hard to satisfy with fixed memory strategies.\nHowever, the introduction of memory inevitably brings an extra computation burden, which hinders real-time generation. Thus, we propose Sparse Memory Activation (SMA), which strategically activates only the most relevant tokens in attention layers according to the attention scores calculated from query (current chunk) and key (context in memory) by top- selection. Subsequent attention is then applied within these selected tokens, which effectively accelerates inference by reducing computation cost while preserving quality.\nIn this way, our MemFlow effectively maintains contextual consistency over long durations and adeptly balances the memory‚Äìefficiency trade-off. It achieves state-of-the-art quality for interactive video generation with only 7.9% speed reduction compared with memory-free baseline. Our framework sustains 18.7 FPS on a single NVIDIA H100, demonstrating a clear advantage in producing narrative-coherent, long-term consistent videos with complex character and scene switching.\n2 Related Works\nLong Video Generation. Prior efforts to extend video generation to longer durations can be broadly categorized into three approaches. Autoregressive-diffusion hybrid approaches generate long videos by iteratively predicting frames [causvid, self-forcing, skyreels-v2, magi-1, framepack, yang2025longlive]. Diffusion-Forcing [chen2025diffusionforcing] mitigates error propagation by adjusting denoising schedules. CausVid [causvid] distills bidirectional models into efficient few-step causal models, with Self Forcing [self-forcing] further addressing the train‚Äìtest gap. MAGI-1 [magi-1] and SkyReels-V2 [skyreels-v2] successfully scale up this AR-diffusion paradigm. Multistage methods [xiang2025macro, zhuang2024vlogger, huang2025filmaster, xiao2025captain] decompose a long video into multiple clips to be generated separately. They either first synthesize a sequence of coherent keyframes followed by video infilling for each clip [zhou2024storydiffusion, iclora, xiao2025captain], or draft sequential prompts and use a T2V model to synthesize individual segments [zhao2025moviedreamer, long2024videostudio]. A fundamental limitation of these approaches is the isolated nature of clip generation, which often leads to a lack of temporal coherence over long horizons. The third category applies efficient architectures to manage computational costs. TTTVideo [test-time-training] and LaCT [zhang2025lact] learn context using neural networks with linear attention. TokensGen [ouyang2025tokensgen] represents video clips with condensed tokens. Mixture of Contexts [cai2025mixture] dynamically selects relevant context for attention computation. These methods often sacrifice visual fidelity for efficiency.\nMemory Mechanisms in Video Generation. Effective memory mechanisms are crucial for maintaining consistency in long video generation. Action-guided video generation often relies on geometric and spatial dependencies [zhai2025stargen, chen2025learning, xiao2025worldmem]. Worldmem [xiao2025worldmem] and Context as memory [yu2025context] conduct memory retrieval based on Field of View (FOV) overlap between conditioned camera poses. VMem [li2025vmem] introduces Surfel-Indexed View Memory for efficient retrieval by indexing past views with 3D surface elements. These methods, however, are highly dependent on spatial priors, thus lack generalizability. General video generation primarily maintains memory through context compression [framepack, far, hong2024slowfast]. FramePack [framepack] compresses input frames into a fixed-size context to manage memory and efficiency. FAR [far] and StreamingT2V [henschel2025streamingt2v] combine short- and long-term memory via multiscale compression and learnable modules, respectively. These methods often maintain memory without adaptive retrieval, making it challenging to build dynamic connections between relevant context and the currently generated clip.\n3 Method\nMemFlow enhances long-video narrative consistency by incorporating a novel dynamic memory bank into a streaming video generation framework (Sec 3.1). To dynamically recall relevant historical context according to current prompt, we first adopt (1) Narrative Adaptive Memory (NAM) mechanism for memory retrieval and updating (Sec 3.2); then (2) Sparse Memory Activation (SMA) is employed for memory selection to address memory-efficiency trade-off (Sec 3.3). The refreshed memory is subsequently utilized by AR-diffusion model to synthesize the current video chunk, after which this process continues to roll out over extended temporal horizon. The entire framework is trained end-to-end using a streaming long-tuning strategy, enabling the model to learn how to effectively manage its memory during long-duration rollouts. Figure 2 provides a high-level illustration of our framework.\n3.1 Overall Framework\nBaseline. Our work builds upon a hybrid autoregressive-diffusion framework that integrates autoregressive chunk-wise video generation with denoising diffusion [causvid, magi-1, self-forcing]. At each generation iteration, the model produces a chunk of frames, conditioned on the immediately preceding frames. This autoregressive process naturally produces Key-Value (KV) cache from previous iterations, which is leveraged as the foundational structure for our memory bank. This design allows us to store historical context efficiently without incurring extra computational overhead. In a standard setup, the autoregressive attention mechanism operates over () local frames (the last preceding frames and current generated frames). By integrating our memory bank containing frames, the attention operation is extended to cover () frames, seamlessly blending short-term dependencies and long-term memory.\nTraining Mechanism. We train our memory-augmented AR-diffusion model using a distillation-based approach, specifically adopting the Self-Forcing [self-forcing] paradigm. Specifically, we adopt Distribution Matching Distillation (DMD) loss [dmd] that minimizes the gap between the student and teacher generator‚Äôs output distribution, to distill a pretrained bidirectional model into a few-step causal model. To equip the model with long-context capabilities, we employ a streaming long-tuning strategy [yang2025longlive]. During this phase, the generator samples a short video clip (e.g., 5s) in each round conditioned on previous clips, and the teacher provides reliable supervision on this newly generated short clip via DMD. We can repeat this rolling extension for generating long sequences until the video reaches a preset maximum length, with supervision applied throughout the entire rollout. Crucially, we integrate our memory mechanism (NAM and SMA) into this tuning process: employing NAM in the streaming tuning allows the model to learn how to retrieve relevant history from self-generated frames during training, aligning training with inference and improving long-range consistency; while SMA mitigates the computational overhead introduced by memory, incurring only 7.9% efficiency loss compared to the memory-free baseline.\n3.2 Narrative Adaptive Memory (NAM)\nWe first formulate the components of our memory bank in NAM. At each iteration, a new chunk is generated autoregressively, during which it is processed by the DiT to produce key-value (KV) representations at each transformer layer , where denotes the index of chunk generation iteration. At the beginning of next iteration, the memory is updated as for subsequent computation. Our memory mechanism aims to provide content-aligned context for incoming generation, which necessitates its ability to retrieve history relevant to incoming prompt and incorporate the most recently generated content for updating. To avoid excessive expansion of the memory bank as generation proceeds, we introduce two synergistic techniques: (i) Semantic Retrieval, which retrieve most informative context based on the cross-attention relevance between textual queries and visual keys, and (ii) Redundant Removal, which leverages temporal redundancy to select the KV feature of first latent frame as prototype for the entire local chunk.\nSemantic Retrieval. During generation, each transformer layer produces key‚Äìvalue representations for the current chunk while attending across the present sequence, the KV cache in local window, and the global memory bank. The retrieval criterion is derived from cross-attention scores between the textual tokens as query and visual tokens from KV cache as key, which has proven effective in prior works [chen2024image, wang2025adaretake, yang2025streammem] of large vision-language models. In our design, the textual tokens are computed from the prompt of chunk to be generated, thus the visual tokens with higher scores are semantically-aligned with this chunk. By retrieving those KV cache in the memory bank, we expect the model to attend to content-relevant visual features.\nLet be the textual query of the current text prompt at layer . For each of the frames stored in the memory bank, represented by its key of KV cache where , we compute a semantic relevance score, :\nwhere the computes attention weights, and is mean pooling here to produce a scalar score . Then we can identify the top- most semantically aligned frames to be retained.\nRedundant Removal. Following Semantic Retrieval, the immediately preceding chunk is consolidated into a representative prototype before being integrated into the memory. Instead of adopting computationally intensive context-merging techniques [he2024ma, yang2025streammem, zhang2025beyond] that rely on importance weighting, we propose a highly efficient heuristic. We leverage the high temporal redundancy inherent in short video chunks, where visual information exhibits strong similarity across consecutive frames. We posit that a single frame is sufficient to encapsulate the core visual content of the entire chunk. Therefore, we simply select the KV pair of the first frame from the preceding chunk to serve as its compact prototype. The updated memory bank is then formed by concatenating the selected historical frames with the newly consolidated local prototype. The two strategies ensure that the memory is semantically relevant and real-time updated, enabling the model to build long-term and short-term dependencies crucial for narrative coherence.\n3.3 Sparse Memory Activation (SMA)\nDirectly extending local context window to incorporate a memory bank introduces computational burden, as attention complexity scales with context size. While rigidly compressing the context can improve efficiency, it often compromises memory quality, as critical historical cues may be discarded indiscriminately. To address this memory‚Äìefficiency trade-off, we introduce Sparse Memory Activation, a relevance-gated memory selection technique for dynamic memory pruning before attention computation.\nOur approach operates on the principle of selective attention, where query token from the current video chunk attends only to a subset of the most relevant historical frames in memory. Formally, we first partition key () and value () of the memory bank into frames. We then compute a compact descriptor for both the query () from current chunk and key of each frame using mean pooling over the token dimension, which is highly sufficient and expressive for generation tasks and has demonstrated by prior works [cai2025mixture]. This yields a single query descriptor, , and a set of frame-wise key descriptors, , the chunk index is omitted here for simplicity. The relevance between the current query and frame-wise key in memory is then determined by the inner product of their respective descriptors:\nBased on these relevance scores, we identify the set of indices corresponding to the top- most relevant frames:\nThis formulation selects the subset of indices of size that maximizes the sum of relevance scores. Finally, the attention computation for query is restricted to the key-value pairs belonging to the selected top- chunks:\nwhere and are the concatenated key and value tensors from the chunks indexed by the set .\nBy activating part of the memory bank, SMA reduces computational latency while retaining the most pertinent historical information. This strategy enables the model to selectively recall the right context at the right time, thereby preserving long-range dependencies and narrative coherence. Moreover, by implicitly filtering out less relevant or potentially erroneous information from the history, our approach mitigates error accumulation. This allows MemFlow to achieve both robust memorization and computational efficiency, ensuring the generation of coherent long videos without the degradation of visual quality over time."
  },
  {
    "article": "\\ul\nTimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs\nAbstract\nThis paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.\n1 Introduction\nRecent multimodal large language models (MLLMs) have excelled at understanding ‚Äúwhat‚Äù happens in a video, yet they largely fail when asked ‚Äúwhen.‚Äù This limitation is central to the task of video temporal grounding (VTG). The challenge is twofold: 1) VTG necessitates a fundamental shift from coarse semantic aggregation to fine-grained time-aware perception; 2) Distinguishing queried events requires modeling long-term visual dynamics over appearance-centric features, which are notoriously difficult to annotate and learn. As MLLMs become integral to perception [yuan2021closerlookat, wu2025numberit, perception_test, vcr_bench] and reasoning systems [nagrani2025minerva, nagrani2024neptune, liu2025videomind, chain_of_frames, zhang2025vitcot, longvila_rl], equipping them with robust temporal awareness is no longer optional, but essential [vtg_survey, univtg, timechat, wang2025timer1, team2025vidi].\nThis work focuses on post-training MLLMs with leading temporal grounding ability. This investigation is a straightforward extension given the recent progress in pretrained foundation MLLMs [qwen2-5-vl, Qwen3-VL, internvideo2]. Different from heavily studied general understanding tasks, recipes for fine-grained grounding tasks are not yet to be established. This paper aims to systematically investigate core components of building time-aware MLLMs (Fig. 1) along two primary dimensions: data quality and algorithmic design.\nOur investigation starts by exposing critical flaws in evaluation benchmarks. We find that existing VTG benchmarks [charades-sta, activitynet-captions, qvhighlights] not only lack a clear comparison between leading proprietary and open-source models but are also rife with low-quality queries and erroneous timestamps. This noisy data may render current leaderboards misleading and misguide research efforts. To rectify this, we undertook a meticulous data overhaul. We first defined strict criteria for query and timestamp quality, in terms of uniqueness, existence, clarity, and accuracy. We then manually re-annotated three popular datasets (Charades-STA [charades-sta], ActivityNet Captions [activitynet-captions], QVHighlights [qvhighlights]) to create TimeLens-Bench, a rigorously cross-validated benchmark. As shown in Fig. 2(a), the necessity of this correction is confirmed by a dramatic re-ranking of models on TimeLens-Bench compared to their performance on legacy benchmarks, proving the unreliability of prior evaluation standards. Beyond evaluation, we also fix the noisy training data by automated re-annotation, yielding TimeLens-100K, a large-scale, high-quality training dataset.\nWith our curated data suite as a solid foundation, we conduct in-depth explorations on the algorithmic design principles from three key aspects. First, for timestamp representation, we discovered that a simple yet effective interleaved textual encoding strategy outperforms more complex alternatives. Second, we determined that VTG is fundamentally a perception-driven task, and thus employ a pure thinking-free reinforcement learning with verifiable rewards (RLVR) approach that outperforms other training paradigms in both efficiency and performance. Finally, our detailed analysis of RLVR training reveals two key recipes for both performance and training efficiency: (1) early stopping when reward metrics plateau, and (2) difficulty-based data sampling. By integrating these insights and design principles, we ultimately develop TimeLens models, a family of MLLMs with superior VTG capability. As shown in Fig. 2(b), our model achieves state-of-the-art performance among open-source models and even surpasses proprietary models such as GPT-5 and Gemini-2.5-Flash.\nThrough these efforts, we identified and addressed long-overlooked quality issues in existing datasets, and derived a series of insights and best practices in algorithmic design. We hope TimeLens can serve as a solid foundation in both data curation and algorithmic design principles, to facilitate future research on building MLLMs with strong VTG capabilities. Our code, data, and models will be open-sourced.\n[ADDRESS_REMOVED] been proposed, spanning diverse domains [qvhighlights, activitynet-captions, oncescu2021queryd, wang2024cosmo_cap, huang2024vtimellm-internvid-vtime, grauman2022ego4d, tacos]. Early works [charades-sta, 2d-tan, lu2019debug] trained and evaluated models on the training and test splits of a single benchmark [tacos, activitynet-captions] to assess their ability to fit single-domain data distribution. In recent works [liu2025videomind, timechat, guo2024trace], large diverse corpuses composed of multiple different source datasets [didemo, oncescu2021queryd, wang2024cosmo_cap, huang2024vtimellm-internvid-vtime, zala2023hirest, liu2024etbench] are aggregated for training, and a suite of distinct benchmarks [activitynet-captions, charades-sta, qvhighlights] are used to probe the models‚Äô real-world cross-domain generalizability.\nHowever, the critical issue of data quality has been overlooked. There lacks a systematic examination on whether existing datasets are reliable enough for training and evaluation. In this paper, we manually inspect existing datasets, identify and correct errors, and produce quality-improved training and evaluation suites for developing more practical VTG models.\nMLLMs for Temporal Grounding\nSubstantial works focus on algorithmic designs to improve MLLMs‚Äô VTG capability. One line of research explores model architectures, including token compression methods to reduce computation on long videos [timechat, zeng2024timesuite] and timestamp encoding strategies to align the timestamps of each frame with its corresponding features [chen2024timemarker, wu2025numberit, li2025unitime, ge2025arc-hunyuan-video, zeng2025distime]. Another line of works investigate training strategies: introducing various supervised fine-tuning tasks to improve VTG performance [zeng2024timesuite, cheng2025tempura], or designing verifiable rewards to improve performance via reinforcement learning [wang2025timer1, tvg-r1, li2025tempsamp_r1, yue2025tempo_r0].\nDespite the abundance of proposed designs, their inconsistent experimental settings make it difficult to fairly compare their relative merits and establish best practices. In this paper, we systematically analyze these design choices using our quality-assured training and evaluation suites, offering key insights for improving MLLMs‚Äô VTG capability.\n3 Towards Reliable, High-Quality VTG Data\n3.1 Annotation Criteria\nTask Formulation\nFor temporal grounding, a model takes as input a video and a text query , localizes the event described by , and outputs the corresponding temporal segment . In practice, a video is typically annotated with one or more query-segment pairs .\nInput Criteria\nThe input video and query should satisfy:\n-\n‚Ä¢\nQuery clarity and specificity. The query must be clear, precise, and unambiguous for accurate and definitive grounding (A counterexample like ‚Äúthe game continues‚Äù).\n-\n‚Ä¢\nEvent existence. The event described in the text query must genuinely exist within the video content.\n-\n‚Ä¢\nQuery uniqueness. All queries must be unique in a single video. The presence of multiple nearly identical queries describing the same event is equivalent to duplicating or weighting certain samples, leading to biased metrics. Indeed, this issue is severe in Charades-STA dataset.\n-\n‚Ä¢\nAvoid information leakage in queries. Queries like ‚Äúending credits‚Äù leak their temporal position, allowing the model to answer via shortcut, without truly ‚Äúgrounding‚Äù the query over the entire video. However, annotators tend to label such queries since they are easy to identify.\nOutput Criteria\nThe temporal segment should satisfy:\n-\n‚Ä¢\nAnnotation precision. The annotated event boundaries should be precise, excluding any subsegments that do not conform to the query‚Äôs description.\n-\n‚Ä¢\nAnnotation exhaustiveness. There should be no other time segments outside the annotated one that also satisfy the query‚Äôs description.\n3.2 Manual Auditing and Refinement\nWe introduce a rigorous and efficient pipeline for auditing and refining existing temporal grounding datasets.\nDiagnose-then-Refine\nOur pipeline follows a diagnose-then-refine workflow. Given a video-query pair from existing datasets, annotators first carefully review the video to identify potential errors against the criteria in Sec. 3.1. If an error is detected, they select the error category, then either revise the query or choose a new valid event to describe. Subsequently, the precise temporal segment is annotated. The core principle is that the same annotator performs both error detection and subsequent correction, which not only improves efficiency but also strengthens annotators‚Äô awareness of potential errors, thereby reducing the risk of introducing similar ones.\nError Identification\nDirectly applying the abstract criteria from Sec. 3.1 for error detection proves overly challenging for annotators. Therefore, as shown in Fig. 3, we derive from these criteria a set of concrete, easily identifiable error types with clear illustrations. Annotators check whether each error type is present and fill in the corresponding information. Additionally, we group all queries from the same video together to detect violations of ‚Äúquery uniqueness‚Äù and improve annotation efficiency. During the process, we do not provide original temporal segments to annotators.\nQuality Control\nUpon completion of each small data batch, every sample is assigned to a different annotator for cross-validation and error correction. If the error rate in a batch exceeds a threshold, the entire batch is rejected for re-annotation and then validated again. For annotator selection and training, we sampled a small subset of data for trial annotation with over a dozen vendors, then selected the vendor with the highest quality and consistency. Before formal annotation, we provided a detailed handbook and conducted several training sessions. The annotation interface and detailed manual are provided in Appendix E of the appendix.\n3.3 Empirical Analysis on TimeLens-Bench\nIn this section, we present our efforts and findings by applying the above annotation pipeline to existing datasets. We focus on three most widely-used temporal grounding benchmarks: Charades-STA [charades-sta], ActivityNet Captions [activitynet-captions], and QVHighlights [qvhighlights]. These datasets exhibit diversity across video domains, video durations, and query semantics. They are all manually annotated and generally considered the highest-quality VTG datasets available. Therefore, analyzing them offers a representative view of the quality and issues prevalent in existing data. Through diagnosis and refinement, we release TimeLens-Bench, comprising refined versions of the three aforementioned benchmarks: Charades-TimeLens, ActivityNet-TimeLens, and QVHighlights-TimeLens. Together, they form a comprehensive evaluation suite that combines diversity with high quality. Detailed statistics for these benchmarks are provided in Appendix A.\nError Statistics and Analysis\nAs shown in Fig. 4, we observe an alarmingly high proportion of errors across different categories in these benchmarks. The distribution of error composition varies across different datasets, yet all datasets exhibit consistently high overall error rates. For example, in Charades-STA, we find that 20.6% of samples violate query uniqueness, while 34.9% exhibit annotation accuracy issues. Such severe errors will lead to unreliable evaluation results and misguide research efforts.\nQualitative Examples of Errors and Fixes\nAs shown in Fig. 3, various error examples are identified in existing datasets, including multiple event occurrences, no event occurrence, duplicate queries for the same video, unclear query, and inaccurate annotation. Through our rigorous manual refinement, these detected errors have been properly corrected, significantly improving data quality. Our refined datasets provide more reliable evaluation results.\nCounter-intuitive Evaluation Results\nWe evaluate various frontier models on both the original and refined benchmarks, observing drastically contrasting performance trends. As illustrated in Fig. 2(a), on the original benchmarks, we observe a surprising phenomenon: frontier proprietary models like Gemini-2.5-Pro [comanici2025gemini-2-5] receive poor scores, whereas open-source models [wang2025timer1, qwen2-5-vl] attain significantly higher ones. Conversely, on our refined benchmarks, this trend reverses. The proprietary models exhibit much better results, though with room for improvement, while the open-source models suffer a substantial performance degradation, lagging far behind their proprietary counterparts. This reversal indicates that the original benchmarks produce misleading results due to inherent quality flaws, while our refined benchmarks yield results that align more closely with real-world user experience, providing reliable evaluation for developing better VTG models.\n3.4 Training Data Re-annotation\nBy applying our manual pipeline from Sec. 3.2 to a sampled subset of existing VTG training corpus [didemo, oncescu2021queryd, wang2024cosmo_cap, huang2024vtimellm-internvid-vtime, zala2023hirest], we found that the training data exhibits an even higher error rate compared to the evaluation benchmarks. This motivated us to refine training data based on scalable re-annotation. Given the vast scale of the training sets, we employ an automated pipeline to improve their quality based on advanced multimodal models. Owing to the poor quality of these training datasets, especially the high proportion of queries that fail to meet our criteria in Sec. 3.1, we re-annotate the videos rather than refining existing labels. Through this process, we curate TimeLens-100K, a large-scale, high-quality, and diverse VTG training set. Additional details are provided in Appendix F.\nAs presented in Fig. 2(b), models trained on TimeLens-100K demonstrate substantially improved performance on our refined evaluation benchmarks. This performance gain serves as a direct validation of the data‚Äôs enhanced quality. Notably, our automated re-annotation for training data is developed entirely independently of the manual benchmark refinement process, ensuring an unbiased evaluation.\n4 Benchmarking Grounding MLLMs\nIn this section, we benchmark the performance of various state-of-the-art proprietary and open-source models on TimeLens-Bench, including our TimeLens models derived from the exploration in Sec. 5.\nEvaluation Metrics\nWe evaluate VTG performance using the ‚ÄúR1@m‚Äù metric, which measures the proportion of test instances where the highest-ranked predicted segment achieves an IoU exceeding threshold m (where m takes values from 0.3, 0.5, 0.7). Additionally, we employ mIoU as a primary measure, computing the mean IoU across the entire test set for conciseness.\nEvaluation Results\nAs shown in Tab. 1, we observe a significant performance gap between existing open-source and proprietary models, and our TimeLens models substantially narrow this gap. TimeLens-7B delivers substantial improvements over its baseline, demonstrating the effectiveness of the insights and best practices obtained from our experiments in Sec. 5. It surpasses strong open-source competitors such as Time-R1-7B [wang2025timer1] and MiMo-VL-7B [mimo-vl], as well as proprietary models like GPT-4o [hurst2024gpt-4o] and GPT-5 [OpenAI2025_GPT5]. More remarkably, on the already stronger baseline Qwen3-VL-8B, our TimeLens-8B model still achieves substantial performance gains, establishing a new state-of-the-art among open-source models and even surpassing frontier proprietary models like Gemini-2.5-Flash [comanici2025gemini-2-5].\n5 Exploring Algorithmic Designs\nIn this section, we conduct a systematic study on the algorithmic designs for improving MLLMs‚Äô VTG performance, covering various aspects from model architectures to training strategies. Leveraging our high-quality training and evaluation suites as a reliable testbed, we derive several novel and valuable insights. As shown in Fig. 2(b), each of our findings contributes a non-trivial performance gain, ultimately culminating in our TimeLens model.\nExperimental Setup. Our experiments use Qwen2.5-VL-7B [qwen2-5-vl] as the baseline. For RLVR experiments, we employ GRPO [shao2024deepseekmath] as optimization method. We use TimeLens-Bench for evaluation and TimeLens-100K for training. To ensure rigor, all ablation studies are based on the final, best-performing model configuration, isolating the impact of a single design choice at a time. Due to limited computational resources, we adopt a lower per-frame resolution for our ablation experiments. More implementation details are provided in Appendix B of the appendix.\n5.1 Timestamp Encoding\nTo enable MLLMs to perform temporal grounding, a critical design decision is timestamp encoding (i.e., aligning the timestamp of each frame with its corresponding features). Effective timestamp encoding allows the model to accurately perceive the absolute temporal position of each frame and the relative order between frames, thereby producing precise localization results. As illustrated in Fig. 5, various timestamp encoding strategies have been proposed:\n-\n‚Ä¢\nPosition-embedding based. These methods adapt position embeddings in LLMs to represent the temporal position of each frame. For example, MRoPE [qwen2-5-vl, mimo-vl] and 3D RoPE [team2025kwai_keye] extend pure-text RoPE to multimodal scenarios, encoding the spatial and temporal dimensions of video frame tokens.\n-\n‚Ä¢\nVisual overlay. These methods [ge2025arc-hunyuan-video, wu2025numberit, cheng2025tempura] directly overlay timestamps or frame index onto each frame, enabling MLLMs to ‚Äúread‚Äù the temporal position through their OCR capabilities.\n-\n‚Ä¢\nTextual encoding. These methods convert timestamps into text tokens using the MLLM‚Äôs text tokenizer. There are two main variants: the Interleaved approach [guo2025seed1_5_vl, li2025unitime, yao2025genS, hong2024cogvlm2, chen2024timemarker] in Fig. 5(a) inserts timestamp tokens before the visual tokens of each frame. In contrast, the Non-interleaved approach [li2024videochat_flash, wang2025spacevllm, li2023videochat] adds an instruction like ‚ÄúThis video samples frames of a -second video at seconds.‚Äù into the prompt.\nWe conduct a comprehensive comparison of different timestamp encoding methods. For each method, we experiment with two timestamp formats: raw timestamps (e.g., ‚Äú10.2s‚Äù) or frame indices (e.g., ‚Äú1, 2, 3‚Äù), which are simpler but neglects the temporal interval between frames. As shown in Tab. 2, our results reveal: Position-embedding based methods yield unsatisfactory results. Given that they require fundamental modifications to the RoPE mechanism in LLMs, their practicality is limited without large-scale retraining. Instead, interleaved textual prefix with raw timestamps achieves the best performance among all approaches, while remaining simple and intuitive.\n5.2 Optimization Paradigms\nIn this section, we review different training paradigms and conduct systematic experiments to compare their effectiveness and efficiency for VTG, seeking insights into the optimal training paradigm.\nEarlier works [timechat, huang2024vtimellm-internvid-vtime, zeng2024timesuite, guo2024trace, guo2025vtg-llm] employ supervised fine-tuning (SFT) to improve MLLMs‚Äô VTG capability. Recently, some works [wang2025timer1, tvg-r1] utilize reinforcement learning with verifiable rewards (RLVR), following a ‚Äúthink-then-answer‚Äù approach [guo2025deepseek_r1] (details in Appendix B): during sampling, the model first generates an explicit thinking process and then produces the final answer. The task-specific VTG accuracy reward is computed only on the final answer. Despite these efforts, there lacks a systematic comparison of the respective merits of these methods, leaving some key questions unanswered:\n-\n‚Ä¢\nIs RLVR superior to SFT? While the pioneering work Time-R1 [wang2025timer1] demonstrates that RLVR outperforms SFT, they compare the two methods using the same amount of training data, despite RLVR requiring significantly more training time. A fair comparison under equal training budgets remains absent.\n-\n‚Ä¢\nIs explicit ‚Äúthinking‚Äù necessary for RLVR? Recent works suggest that the thinking process is not essential when applying RLVR to visual perception such as counting [li2025cls-rl, perception_test]. Whether this holds for VTG, a predominantly perception-oriented task, remains unanswered.\n-\n‚Ä¢\nDoes a preceding SFT phase benefit RLVR? An SFT phase prior to RLVR is typically employed to enhance the model‚Äôs capability and facilitate subsequent RLVR training [team2025kwai_keye, mimo-vl]. However, whether this preceding SFT phase actually improves final performance in the VTG scenario remains unexplored.\nIn Tab. 3, we compare the performance and efficiency of different training paradigms. Our results reveal that thinking-free RLVR surpasses both SFT and thinking-based RLVR in performance while being more efficient. Adding a preceding SFT phase before RLVR yields no significant performance gain compared to pure RLVR. Overall, a pure thinking-free RLVR approach maintains simplicity, superior performance, and high efficiency.\n5.3 Recipes for RLVR Training\nBuilding on the finding in Sec. 5.2 that thinking-free RLVR is the optimal training paradigm, in this section, we further explore effective recipes for RLVR training, focusing on two key questions: (i) How long should we train? (ii) How to effectively sample training data?\nHow long should we train? In SFT, the prevailing wisdom is ‚Äútrain longer, generalize better‚Äù [hoffer2017train_longer]. Given training data with sufficient scale and quality, we typically train MLLMs for at least one full epoch over the entire dataset, ensuring the model sees as much data as possible to enhance generalization. However, whether this strategy is optimal for RLVR remains to be explored.\nIn Fig. 6, we conduct RLVR training on 12K data from TimeLens-100K, tracking the reward and evaluating model checkpoints at different training steps on our evaluation benchmarks. When the temporal IoU reward and the within-group reward standard deviation plateau, model performance has reached its peak. Continued training beyond this point leads to performance degradation. Therefore, in RL training, even with sufficiently high data quality, training for a full epoch over all available data is suboptimal. A good practice is performing early stopping when reward metrics plateau, which not only saves computational cost but also prevents performance degradation.\nHow to sample training data? For RLVR training, it is crucial to select samples with appropriate difficulty relative to the model, and many works propose assessing training sample difficulty and employing difficulty-aware sampling [yuan2025vl_cogito, wang2025timer1, hong2025glm_4_5]. To evaluate the impact of sample difficulty on video temporal grounding, we conduct experiments on our TimeLens-100K high-quality training corpus. Following prior works [wang2025timer1, yuan2025vl_cogito], we use the model to be trained to perform offline inference on the training data, compute IoU metrics to estimate sample difficulty, and then perform Gaussian sampling based on sample difficulty (details in Appendix B). By varying the mean of the Gaussian distribution, we obtain training sets with different difficulty levels relative to the model, and conduct RLVR training on each set independently.\nAs shown in Fig. 7, model performance improves as the average sample difficulty increases, and eventually plateaus when difficulty becomes sufficiently high (over 0.75). This trend demonstrates that selecting training samples with sufficiently high difficulty relative to the model is crucial for achieving optimal performance.\n6 Conclusion\nIn this work, we presented TimeLens, a systematic investigation into building MLLMs with robust video temporal grounding capabilities. On the data front, we first exposed severe quality issues in existing VTG benchmarks. Through meticulous manual re-annotation guided by strict quality criteria, we created TimeLens-Bench, a reliable evaluation suite that dramatically reshapes model rankings and provides trustworthy evaluation for future research. We also developed an automated re-annotation pipeline for noisy training data, yielding TimeLens-100K, a large-scale, high-quality training dataset. On the algorithmic front, our comprehensive exploration yielded several key insights, which culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpasses leading proprietary models like GPT-5 and Gemini-2.5-Flash. By open-sourcing our code, data, and models, we hope TimeLens can serve as a strong foundation for building MLLMs with stronger temporal video grounding capability.\nSupplementary Material\nAppendix A Details of TimeLens-Bench\nIn this section, we provide details on our proposed TimeLens-Bench, a comprehensive, high-quality evaluation benchmark for video temporal grounding (VTG), comprising refined versions of three mainstream benchmarks: Charades-TimeLens, ActivityNet-TimeLens, and QVHighlights-TimeLens.\nSource datasets\nWe construct our TimeLens-Bench based on Charades-STA [charades-sta], ActivityNet Captions [activitynet-captions], and QVHighlights [qvhighlights]. For Charades-STA and ActivityNet Captions, we utilize their test splits, while for QVHighlights, we use the validation split, as its test split annotations are not publicly available and prior works [liu2025videomind, guo2025vtg-llm] also adopt the validation split. Since the test set of ActivityNet Captions is excessively large, resulting in prohibitively high evaluation cost, we uniformly partition videos based on their duration and sample an equal number of videos from each duration bin. This yields a subset with a video count comparable to Charades-STA and QVHighlights, while maintaining a balanced distribution of video durations. Although QVHighlights was originally annotated for both video temporal grounding and video highlight detection, our work focuses exclusively on the temporal grounding task.\nDetailed Statistics\nIn Tab. 4, we present detailed statistics of our TimeLens-Bench and its source dataset counterparts. For each source dataset, annotations with high-quality queries had their corresponding temporal segments refined. For the remaining annotations with low-quality queries, we either revised the queries or rewrote them entirely. A small fraction of queries that were deemed unfixable were subsequently discarded. Overall, TimeLens-Bench comprises a total of 4,279 videos with an average duration of 107.8 seconds, and 9,404 annotations.\nA.1 Evaluation Metrics\nWe evaluate model performance on TimeLens-Bench using four standard metrics: Recall@1 at IoU thresholds of 0.3, 0.5, and 0.7 (denoted as R1@0.3, R1@0.5, R1@0.7), and mean Intersection over Union (mIoU).\n-\n‚Ä¢\nmIoU is defined as the average of the temporal Intersection over Union (IoU) scores between the predicted and ground-truth segments across all test samples.\n-\n‚Ä¢\nR1@ measures the percentage of samples for which the temporal IoU of the prediction exceeds a given threshold .\nWhile TimeLens-Bench can be treated as a single unified benchmark for computing the aforementioned metrics, we compute and report metrics separately on its three constituent benchmarks to enable a more fine-grained analysis of model performance across different domains. We encourage future works to adopt this evaluation protocol.\nAppendix B More Implementation Details.\nB.1 Preliminaries for RL\nThinking-based vs. Thinking-free RLVR.\nWe formalize the distinction between thinking-based and thinking-free RLVR paradigms using GRPO [shao2024deepseekmath] as the reinforcement learning algorithm.\nIn the task of video temporal grounding, given a video and query , the model generates a response . For GRPO training, for each input pair in the training set , we sample a group of responses from the policy , compute their rewards , and optimize the policy to maximize the relative advantage within the group:\nwhere the advantage is computed as:\nThe key distinction between the two paradigms lies in the response structure and reward computation. In thinking-based RLVR, following the ‚Äúthink-then-answer‚Äù approach [guo2025deepseek_r1], the response consists of two parts:\nwhere represents the explicit reasoning process and contains the predicted temporal segment . The reward function combines accuracy and format compliance:\nwhere measures the temporal intersection-over-union with the ground truth segment , and ensures proper output formatting following the ‚Äúthink-then-answer‚Äù structure.\nIn contrast, our thinking-free RLVR directly generates the answer without explicit reasoning:\nwith a simplified reward based solely on grounding accuracy:\nAs shown in Tab. 3, the thinking-free paradigm eliminates the need for explicit reasoning generation and format reward engineering, leading to simpler mplementation, faster training and inference, and superior performance.\nDifficulty-aware Sampling\nTo formalize difficulty-aware sampling [yuan2025vl_cogito, wang2025timer1, hong2025glm_4_5], we first perform offline inference with the model to be trained on the training dataset . For each sample, we obtain the predicted segment and compute the difficulty estimate as:\nwhere higher values indicate more challenging samples for the current model.\nWe then compute sampling weights for each sample based on its difficulty. Following [yuan2025vl_cogito, wang2025timer1], we employ Gaussian sampling to construct a training subset where samples with difficulty around a target mean are more likely to be selected. Let denote the target Gaussian distribution:\nTo ensure that samples with difficulty are selected with probability proportional to , we compute the sampling weight for each sample as:\nwhere is the empirical density of samples with difficulty in the original dataset. This density correction ensures that the difficulty distribution of the sampled subset follows the target Gaussian distribution, rather than being biased by the original difficulty distribution in .\nBy varying the mean of the Gaussian distribution, we obtain training sets with different average difficulty levels and conduct RLVR training on each independently to evaluate the impact of sample difficulty on final model performance in Sec. 5.3.\nB.2 Experimental Setup\nUnless otherwise specified, all experiments are conducted using Qwen2.5-VL-7B [qwen2-5-vl] as the base model. Under the Qwen2.5-VL architecture, every two consecutive video frames are merged during the patch-embedding stage of the vision encoder.\nModel Configuration\nWe sample video frames at 2 FPS. For all ablation experiments, we set the minimum number of tokens per frame (i.e., every two merged frames) to , and the maximum number of tokens for the entire video to . Under this configuration, the model adaptively adjusts the spatial resolution based on the video‚Äôs duration and raw resolution. For the final TimeLens models presented in Tab. 1, we scale the resolution budget to and . Under this setting, for a 110-second video, the maximum resolution per frame is about pixels.\nTimestamp Encoding\nIn Tab. 2 and Sec. 5.1, we experiment with different timestamp encoding methods. For position-embedding methods, we directly adopt the original MRoPE implementation from Qwen2.5-VL [qwen2-5-vl]. For Visual Overlay, we render timestamps in red with a font size of pt, overlaid at the bottom-left corner of each frame. For Non-interleaved Textual Encoding, an instruction like ‚ÄúThis video samples frames of a -second video at seconds‚Äù is prepended to the prompt.\nFor Interleaved Textual Encoding, timestamps are converted to text with one decimal place retained (e.g., 10.2 seconds), then tokenized and prepended to the tokens of each frame. As described above, when processing videos, every two consecutive frames are merged during the patch-embedding stage of the vision encoder, while for images, each image is duplicated into two identical copies for merging. Since we insert textual timestamp tokens into the original video tokens to form an interleaved visual-text sequence, we treat each video frame as an independent image and duplicate it for processing. This approach allows us to bypass the original MRoPE mechanism entirely, enabling an isolated comparison between Interleaved Textual Encoding and MRoPE. Meanwhile, we adopt [ADDRESS_REMOVED] matches that of other temporal encoding methods using 2 FPS sampling.\nTraining Configuration\nFor all training procedures, we freeze the vision encoder while updating all other parameters, and train for one epoch. For supervised fine-tuning (SFT) experiments, we use a batch size of and a learning rate of . For reinforcement learning (RL) experiments, we perform difficulty-aware data sampling with a Gaussian distribution where and . The training batch size is , each prompt samples roll-outs, the learning rate is , and the KL coefficient is set to . We train until we observe that the reward plateaus and then perform early stopping. In practice, this corresponds to approximately 310 training steps (2.5K training examples) for Qwen2.5-VL models.\nThroughout the development of this work, our experiments were conducted based on Qwen2.5-VL. More recently, the more powerful Qwen3-VL [Qwen3-VL] models have been released, so we also validated the effectiveness of our data and recipe on Qwen3-VL. We observed that directly applying RL training on Qwen3-VL fails to yield improvements, likely because Qwen3-VL has undergone large-scale multi-task RL training that includes VTG data, preventing the model from generating rollouts with sufficient diversity on VTG task during our continual RL. Therefore, we first perform a small SFT stage to, in a sense, revert the model back to the ‚Äúbase model‚Äù state before RL. This is merely a workaround specific to Qwen3-VL, a model that has already acquired strong VTG capabilities through an RL stage similar to that proposed in this paper. In the common scenario, our recipes are designed to enhance the VTG capabilities of a ‚Äúbase MLLM‚Äù, where this trick is not required.\nAppendix C More Experimental Results\nResults across different model sizes\nIn Tab. 6, we demonstrate the effectiveness of our proposed design principles across various model sizes. Across base models of varying sizes, TimeLens consistently delivers significant performance gains. Remarkably, despite having fewer parameters, TimeLens-3B substantially surpasses even the larger Qwen-2.5-VL-7B model.\nComparison of results on TimeLens-Bench and original benchmarks\nIn Tab. 7, Tab. 8, and Tab. 9, we compare the evaluation results of various models on TimeLens-Bench and the original benchmarks. On the original benchmarks, due to data quality issues, open-source models deceptively surpass state-of-the-art proprietary models like Gemini-2.5-Pro. On our refined benchmarks, model capabilities are more reliably evaluated, with proprietary models maintaining a significant advantage over open-source models. Remarkably, our TimeLens model substantially narrows the performance gap between open-source and proprietary models.\nResults on general video understanding\nIn Tab. 10, we evaluate TimeLens-7B‚Äôs general video understanding capabilities on VideoMME [fu2025videomme], the most comprehensive and widely-adopted video understanding benchmark. The results demonstrate that TimeLens-7B maintains the strong general video understanding capability of its base model. This validates that our proposed design principles can effectively enhance video temporal grounding capabilities without sacrificing general-purpose video understanding abilities.\nAppendix D Discussion on Thinking-free vs. Thinking-based RLVR\nWe analyze the possible reasons why thinking-based RLVR underperforms thinking-free methods in our experiments, from both intuitive and empirical perspectives.\nIntuitive Analysis.\nWhen manually examining and refining existing grounding datasets, we observe that queries in the grounding task are relatively straightforward, primarily testing the model‚Äôs perception capability: whether the model can accurately localize the corresponding event in a long video containing massive information. From a human perspective, completing existing grounding tasks indeed relies mainly on intuition and instinct, rather than complex reasoning.\nEmpirical Observations.\nIn our experiments, when training with thinking-based RLVR, the model‚Äôs thinking length gradually decreases and converges to simple, non-reasoning thinking processes, as shown in Fig. 8. This suggests that the model learns to bypass explicit reasoning when it provides no benefit to the task.\nImplications and Future Work.\nWe believe that most samples in existing video temporal grounding data do not require complex reasoning capabilities, but rather demand sufficiently robust long-video perception and localization abilities. Due to the high cost and corresponding low quality of existing data annotation, as well as limitations in current algorithmic designs, existing MLLMs cannot yet achieve this perfectly. Therefore, in this work, we focus on addressing these two fundamental issues. Meanwhile, we believe that certain grounding tasks do require reasoning capabilities, and we leave the exploration of reasoning-intensive VTG scenarios to future work.\nAppendix E Annotation Interface and Manual\nAppendix F Details of Curating TimeLens-100K\nAs described in Appendix F, we perform automated re-annotation on existing training datasets, resulting in TimeLens-100K, a large-scale, high-quality VTG training set comprising approximately 20K videos and 100K VTG annotations.\nWe begin by sampling videos from numerous existing VTG datasets, including CosMo-Cap [wang2024cosmo_cap], InternVid-VTime [huang2024vtimellm-internvid-vtime], DiDeMo [didemo], QuerYD [oncescu2021queryd], HiREST [zala2023hirest], etc. These datasets already cover sufficiently diverse video domains. Additionally, we perform uniform sampling based on video duration to ensure sampled videos are approximately uniformly distributed within 0‚Äì240 seconds, with a small portion of longer videos included.\nGiven that most queries in the original annotations either lack clarity and specificity, or describe events that do not exist in the video, we directly use MLLMs for re-annotation. First, we prompt the MLLM to identify distinct events in the video and ensure these events are distributed across different time periods rather than being concentrated in a particular segment. Then, we have the model describe each event to generate queries and output the corresponding timestamps. Finally, we prompt the model to verify the quality of the queries and annotations.\nSpecifically, we use Gemini-2.5-Pro [comanici2025gemini-2-5], currently the best-performing VTG model, for re-annotation. The annotation prompt is provided in Fig. 10. Notably, although this prompt appears simple, it is the result of extensive prompt engineering and optimization. We find that a concise and intuitive prompt is sufficient, as the model possesses adequate common sense and reasoning capabilities to understand the task. Overly complex and detailed prompts are unnecessary and can actually degrade annotation quality. During its reasoning process, the model can automatically verify and ensure the uniqueness and uniform distribution of events throughout the video.\nAs shown in Tab. 5, our training data substantially improves model performance, validating the enhanced quality of our training set. Notably, the construction of our training data is independent of our manual benchmark refinement process, ensuring a fair comparison.\nAppendix G Implementation Details for Benchmarking Existing MLLMs\nIn this section, we present the implementation details for evaluating existing MLLMs on our TimeLens evaluation suite, yielding the results reported in Fig. 2(a) and Tab. 1.\nTimeLens Models\nThe prompt for training and evaluating TimeLens models is illustrated in Fig. 9. Implementation details are provided in Appendix B.\nGPT-5 [OpenAI2025_GPT5] and GPT-4o [hurst2024gpt-4o]\nSince GPT models only support multi-image sequences as input, we sample frames from videos at 1 FPS and prepend textual timestamps (i.e., ‚ÄúFrame at 2.5s:‚Äù) to each frame. As the Azure OpenAI API we use does not support more than [ADDRESS_REMOVED], we adopt different strategies for videos longer than 50 seconds: for videos lasting 50-80 seconds, we uniformly sample 50 frames; for videos longer than 80 seconds, we sample at 1 FPS and arrange every 4 consecutive frames into a grid within a single image, following previous works [ju2024miradata, zhang2024internlm-xcomposer-2-5, zhang2024longva]. For GPT-5, which is a thinking model, we use the default value for the reasoning.effort parameter. The evaluation prompt is shown in Fig. 14.\nGemini models [comanici2025gemini-2-5]\nAlthough Gemini models support audio input, we remove the audio from videos to ensure fair comparison with other vision-only models and maintain consistency with our benchmarks, which features vision-only, audio-free annotations. Following the best practices outlined in the official Gemini API documentation, we prompt the models to output timestamps in ‚ÄúMM:SS‚Äù format. For other hyperparameters, we use the default settings: 1 FPS sampling and default mediaResolution, which tokenizes each frame into 258 tokens. For thinking models, we do not impose any limit on the thinkingBudget parameter. The evaluation prompt is shown in Fig. 14.\nQwen3-VL [Qwen3-VL], Qwen2.5-VL [qwen2-5-vl] and MiMo-VL [mimo-vl]\nTimeLens models, Qwen2.5-VL-7B, and MiMo-VL share approximately the same model architecture and hyperparameter configurations. Therefore, when evaluating these models, we adopt the same settings to ensure fair comparisons in Tab. 1. Specifically, consistent with Sec. B.2, we sample video frames at 2 FPS and set the resolution budget to and . For MiMo-VL, we evaluate their best-performing model, MiMo-VL-7B-RL. The evaluation prompt is shown in Fig. 14.\nOther Open-source Models\nWhen evaluating Time-R1 [wang2025timer1], VideoChat-Flash [li2024videochat_flash] and VideoChat-R1 [li2025videochat_r1], we directly use their original codebases. Please refer to their papers and code repositories for details."
  },
  {
    "article": "figuret \\sidecaptionvpostablet\nSpherical Leech Quantization for Visual Tokenization and Generation\nAbstract\nNon-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization (-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.\n1 Introduction\nLearning discrete visual tokenization is fundamental to visual compression [daede2016daala, gersho2012vqsc], generation [esser2021vqgan, chang2022maskgit, tian2024var], and understanding [bao2022beit]. Although discrete token-based visual modeling [bai2024lvm, tian2024var] follows a recipe similar to that of language modeling, we observe a paradox: Visual information carries much more data than language in quantity and diversity111Human language conveys information at several tens of bits/s [shannon1951prediction, piantadosi2011word] while the brain receives visual input at bits/s [koch2006much].; However, the visual vocabulary size of vision models lags far behind that of Large Language Models (LLM)222The tokenizer has 199,997 elements in GPT-4o [hurst2024gpt4o] while 129,280 in Deepseek-R1 [guo2025deepseekr1]. Meanwhile, a typical visual codebook is around .. To bridge this gap, non-parametric quantization (NPQ) methods [yu2023magvit2, mentzer2023fsq, zhao2025bsq] have recently been proposed, demonstrating codebook scalability and parameter efficiency compared to vector quantization [gray1984vq, van2017vqvae]. However, existing NPQ methods have their own flaws and require ad hoc tweaks (e.g., regularization terms), which eventually boil down to the fact that most methods are heuristic and lack a principled design.\nIn this paper, we propose a simple and effective quantization method, called Spherical Leech Quantization (-SQ), which scales to a codebook of while keeping the training of both visual tokenizers and visual autoregressive models as simple as possible. -SQ is theoretically grounded in the intersection of vector quantization and lattice codes. We first provide a unified formulation of existing non-parametric quantization methods from the perspective of lattice coding and reinterpret entropy penalties as a lattice relocation problem. This motivates a family of densest hypersphere packing lattices, among which the Leech lattice in the first shell [leech1967notes] instantiates the codebook of -SQ.\nSpherical Leech Quantization features the following advantages. (i) Simplicity: Thanks to the densest sphere packing principle, -SQ enables the autoencoder to train with the simplest loss trio (i.e. , GAN, and LPIPS) without any regularization terms such as commitment loss and entropy penalties. (ii) Efficiency: Because of the fixed lattice vectors, -SQ is excluded from gradient updates, being both memory and runtime efficient. (iii) Effectiveness. -SQ effectively pushes the rate-distortion tradeoff frontier. Specifically, -SQ-based autoencoder improves rFID from 1.14 to 0.83 compared to BSQ with a slightly smaller effective bitrate ( vs. ). See Figure 1.\nBased on -SQ, we introduce improved techniques in training autoregressive visual generation models with a very large codebook. For the first time, we train a discrete visual autoregressive generation model with a codebook of , comparable to frontier language models, without bells and whistles (index subgrouping [yu2023magvit2, zhao2025bsq], multihead prediction [han2025infinity], bit flipping/self-correction [weber2024maskbit], etc.) and achieve a generation FID of 1.82 FID, close to the validation oracle (1.78 FID) on ImageNet-1k.\n2 Preliminaries\n2.1 Visual tokenization and quantization\nVisual tokenization. Visual tokenization transforms visual input into a set of discrete representations using an auto-encoder architecture and a bottleneck module based on vector quantization (VQ) [gray1984vq, van2017vqvae]. In this paper, we consider single images as input for simplicity. Given an image and an encoder (decoder) denoted by (), we have\nwhere is the spatial downsample factor and the quantizer assigns each to the closest entry in a learnable codebook , i.e. . The entire model , , and is end-to-end trainable using approximation methods such as straight-through estimator (STE) [bengio2013ste], Gumbel Softmax [jang2017gumbelsoftmax], or more recent tricks like Rotation Trick [fifty2025rotation]. One of the key challenges is to learn the codebook effectively, especially when the codebook size increases.\nImplicit codebooks. Yu et al. introduced a fixed implicit codebook [yu2023magvit2], where its best quantizer is binary quantization . Binary Spherical Quantization (BSQ) [zhao2025bsq] further projects the hypercube-shaped codebook onto a unit sphere, i.e. . Finite Scalar Quantization (FSQ) [mentzer2023fsq] extends the codebook to multiple values per dimension i.e. . We refer to these implicit codebook-based methods as non-parametric quantization (NPQ). Both LFQ and BSQ require an entropy regularization term [jansen2020coincidence] to encourage high code utilization:\nwhere is a soft quantization approximation [agustsson2017soft].\nEach of the NPQ variants has its own pros and cons. (1) LFQ is easiest to implement, but the computational cost for entropy increases exponentially. (2) BSQ provides an efficient approximation with a guaranteed bound, but still suffers from codebook collapse without proper entropy regularization. (3) FSQ does not need such complex regularization, but the way to obtain the number of levels per channel () is somewhat heuristic [mentzer2023fsq]. Although the geometric landscape of all these quantization methods varies (Figure 2 in [zhao2025bsq]), we show in the upcoming chapter that they can be interpreted from the same lattice coding perspective. This unified formulation further motivates us to develop a novel non-parametric quantization variant that is both theoretically sound and implementation-wise easy.\n2.2 Lattice-based codes\nLattice. A -dimensional lattice is defined by a discrete set of points in that constitutes a group. In particular, the set is translated such that it includes the origin. Mathematically, an -dimensional lattice is represented by\nwhere is called the generator matrix, comprising of generator vectors in columns, and .\nLattice-based codes. in Eq. 3 has infinite elements by definition. In practice, we include additional constraints so that the new set is enumerable:\nParticularly, spherical lattice codes [ericson2001codes] refers to a family of lattice-based codes with a constant squared norm, i.e. . We will see more examples in Section 3.3. Besides, we define the quantizer associated with a lattice by , which offers a bridge to vector quantization (Section 2.1).\n3 Method\n3.1 Non-parametric quantization as lattice coding\nFrom the perspective of the lattice-based codes defined in Eq. 4, we can describe all variants of non-parametric quantization methods [yu2023magvit2, mentzer2023fsq, zhao2025bsq] in the same language, despite the varying geometric landscapes.\n(i) Vanilla Lookup-Free Quantization [yu2023magvit2]:\nHere, is the standard basis vector, taking the value of at the -th index and elsewhere. The constraints in Eq. 6 are equivalent to saying that for all .\n(ii) Finite Scalar Quantization [mentzer2023fsq]: For simplicity, we consider the special case where any equals .\n(iii) Binary Spherical Quantization [zhao2025bsq]:\nAlthough it appears that Eq. 8 is simply a scaled version of Eq. 6, it is worth noting that the input range varies: in BSQ while in LFQ.\n(iv) Random-projection Quantization (RPQ) [chiu2022vqrp]. RPQ initializes the codebook using a standard normal distribution, followed by an normalization. Due to the codebook existence, strictly speaking, RPQ does not belong to lookup-free quantization by definition. Nevertheless, we can still include it in the same picture, where the generator matrix and contraints look like the following:\nwhere slightly abuses the definition, follows a projected normal distribution .\n3.2 Entropy regularization as lattice relocation\nWe review the entropy regularization term from the perspective of lattice coding. We give a geometric interpretation of the entropy regularization and show that the two subterms correspond to pushing the input point towards the lattice points and finding an optimal configuration of the lattice.\nRe-interpretating entropy regularization. The first term in Eq. 2 minimizes the entropy of the distribution that is assigned to one of the codes. This means that every input should be close to one of the centroids instead of the decision boundary333This principle is also known as the ‚Äúcluster assumption‚Äù [chapelle2005semi, grandvalet2004semi].. This becomes less of an issue since the codebook of interest is huge, exemplified by being greater than 1 as reported in [jansen2020coincidence, yu2023magvit2]. Ablation studies in BSQ [zhao2025bsq] also reveal that we can omit ‚Äì but not ‚Äì while achieving a similar performance.\nThe second term maximizes the entropy of the assignment probabilities averaged over the data, which favors ‚Äúclass balance‚Äù [krause2010discriminative]. Assuming that has a uniform distribution over its input range, we have , where is the Voronoi region for the codeword . is maximized when all have equal volumes.\nFSQ implicitly maximizes entropy. The interpretation explains why FSQ does not suffer from codebook collapse even without entropy penalties. Given an input , FSQ first applies a bounding function , and then rounds to the nearest integers,\nTherefore, the input range is and all integer points within this range are valid lattice points444When , this is the well-known ‚Äúsimple cubic‚Äù or ‚Äúprimitive cubic‚Äù lattice in crystallography; We will see this again in Section 3.3.. The codebook size is , often in the range of . The Voronoi cell for each lattice point is a unit-length hypercube , implicitly complying with the entropy maximization principle.\nLFQ requires explicit entropy maximization. Although the Voronoi cell for each point in LFQ is also identical, the range of input and quantized output is unbounded. This breaks the uniform distribution assumption, thus requiring explicit regularization.\nWhat‚Äôs left? BSQ is missing so far. Since its input lies on a hypersphere, BSQ has to be treated separately. We will discuss the lattice relocation problem for spherical lattice codes in Section 3.3, where BSQ is one such code.\n3.3 Spherical lattices and hypersphere packing\nSpherical lattices. The overall pipeline is written as follows:\nwhere is another way of ‚Äúbounding‚Äù, and the Voronoi regions now take arbitrary shapes on the hyperspherical shell. For simplicity, we study a surrogate problem that approximates the Voronoi regions by placing -dimensional balls with varied radii555This will leave some holes, but we believe that the total volume of holes is negligible compared to the balls., where is the cardinality of the lattice in which we are interested.\nEntropy maximization as dispersiveness pursuit. The entropy maximization term corresponds to finding the most dispersive configuration to relocate the balls. Sloane et al. formally state this problem of placing points on a -dimensional sphere to maximize the minimum distance (or angle) between any pair of points in [sloane2000spherical]. This problem generalizes the Tammes‚Äô problem [tammes1930origin] in dimensions greater than . Mathematically, we write this max-min problem as , where we denote for future empirical analysis.\nEntropy maximization as hypersphere packing. An alternative way is to assume equal radii for all hyperspheres and find the densest sphere packing [conway2013sphere]. The best known results in dimensions 1 to 8, 12, 16, and 24 are summarized in Table 2, where to and have been proved optimal among all lattices [conway2013sphere, cohn2017sphere].\nGiven these basics, we now propose a few candidates.\n(i) Random projection lattice follows RPQ in Section 3.1 where the projected normal distribution initializes points. We use it as a baseline to compare the dispersiveness of different candidate lattice codes in Figure 2, which turns out to be surprisingly strong in higher dimensions.\n(ii) Fibonacci lattice constructs points that ‚Äúare evenly distributed with each of them representing almost the same area‚Äù [gonzalez2010measurement] in a unit square . We can map this point distribution to a unit-length sphere using cylindrical equal-area projection. From Figure 2(a), achieved by this 3D spherical Fibonacci lattice is close to the known densest packing [sloane2000spherical] and much better than random projection. We explore its high-dimensional generalization with the hyperspherical coordinate system inspired by [stackexchange3297830], denoted by . Construction details are left in Appendix A.\n(iii) Densest sphere packing lattice has been introduced and summarized in Table 2. We pay particular attention to the Leech lattice [leech1967notes]. can be constructed in many ways [conway1982twenty] and we use the most convient way to calculate. The vectors in the first shell have a minimal norm and fall into three types; we summarize their shapes and numbers in Table 3 and provide more details in Appendix B. Normalizing these vectors in the first shell to unit length results in the Spherical Leech Quantization (-SQ) codes. We can easily get for and . From Figure 2(b), is much larger than all other candidates.\nBSQ are not the densest packing lattice codes. Before we conclude this chapter, we compare -SQ with BSQ, the prior art, in Table 4. Since the codebook size takes bits, we use BSQ with . -SQ increases by more than 80% (), indicating its superiority. Empirical results in Section 5 also support that -SQ enables a simple loss design such that the entropy regularization term can be omitted. Comparing Figures 2(a) and 2(b), we also conclude that the improvement in of BSQ over the random lattice baseline decreases when increases.\n3.4 Spherical Leech Quantization in practice\nInstantiation. -SQ follows the pipeline of Eq. 11 with the lattice being . Despite the huge codebook size, because the lattice vectors are fixed, we can use tiling and JIT-compiling techniques to reduce both memory and runtime costs compared to vanilla VQ.\nAccomodating smaller codebooks. In some cases with less data, a codebook size of may be too large. We can also take one type of or its subset so that the codebook size range .\n3.5 Integration with other quantization techniques\nMulti-scale residual quantization. Since -SQ is an in-place replacement of VQ, we can use it in combination with other techniques, such as multiscale quantization [juang1982msvq] and residual quantization [barnes1996rvq, lee2022rq]. In this paper, we integrate -SQ into the VAR tokenizer [tian2024var] for image generation, allowing for direct comparison with quantization methods such as VQ in VAR [tian2024var] and BSQ in Infinity [han2025infinity].\nAligning with vision foundation models. Better reconstruction does not always lead to better generation quality [yu2023magvit2, mentzer2023fsq, gupta2024walt, ramanujan2024worse]. VAVAE [van2017vqvae] proposes to address this reconstruction-generation dilemma by aligning latent embeddings with vision foundation models. We use the VF loss [yao2025vavae] between the latent embedding before quantization and the feature extracted from a pretrained DINOv2 [oquab2023dinov2].\n4 Autoregression with a Very Large Codebook\n4.1 Representing the codebook mapping\nPreliminaries. As NPQ scales up the effective size of the codebook, effectively representing the codebook mapping for the autoregressive models becomes a big issue. The most straightforward way is to represent each code by a unique index. It uses an embedding matrix to map each index to a vector and an unembedding matrix to get the final logits of dimension for a simple classification problem. Memory cost and training stability are the two biggest challenges.\nThere are two more solutions: (1) index subgroup [yu2023magvit2] and (2) bitwise operation [weber2024maskbit, han2025infinity]. The former is compatible with the autoregression framework, but the resulting sequence length grows linearly w.r.t. the number of groups. Han et al. model bitwise tokens with multiple BCE losses in parallel in [han2025infinity]. However, it only applies to LFQ/BSQ and relies on bitwise self-correction to mitigate the train-test discrepancy. In the following, we show our improvements to accommodate the family of spherical lattice codes.\nSimple classification with memory optimization. We adopt the cut cross entropy (CCE) [wijmans2025cce] to address memory consumption. Since the visual auto-regressive models are trained from scratch, we use Kahan summation [kahan1965pracniques] to maintain numerical stability, as suggested by [wijmans2025cce]. We leave the training techniques in Section 4.2.\nFactorized -itwise prediction. Densest sphere-packing lattices like and take integer values, but all possible values go beyond binary [zhao2025bsq]. We generalize the concept of bitwise prediction and propose a factorized -it666Short for base- unit, analogous to bit for and nat for . prediction. Assuming independence across channels, the joint log-probability of one lattice code is approximated by the sum of the log-probabilities of each dimension, i.e.\nwhere denotes the probability of the -th element of the -dim lattice codes. For -SQ, we use -way classification heads to include all possible values . -itwise self-correction is also possible by toggling any element with a certain probability, though we do not explore it in this paper.\n4.2 Training\nWe train a 16-layer Infinity model but observe a consistent increase in gradient norms and explosion of loss (the blue curve in Figure 3777The loss explosion occurs earlier when the model goes to 1B.). A natural hypothesis for this is about the large codebook888Also, we drop the entropy regularization that promotes class balance.: We plot the codebook usage on ImageNet-val, sorted by density, in Figure 4. For the standard VQ codebook, the largest frequency and smallest frequency are within the same order of magnitude (). For -SQ‚Äôs large codebook, the ratio between the most frequent index and the least frequent one surges to . The imbalance is more visible after the VF alignment (Sec. 3.5). We further hypothesize that it prevents prior visual auto-regressive models from utilizing a large visual codebook for generation, although the low utilization problem during reconstruction appears to be fixed [shi2025ibq, zhu2024vqganlc].\nDespite the difficulty, we recognize that this is not a unique issue in visual generation. An unbalanced, large codebook is common when training large language models [chowdhery2023palm, wortsman2024small, olmo2025olmo2]. Therefore, we borrow two simple and effective techniques, namely Z-loss [chowdhery2023palm] and improved optimization with orthonormalized matrix updates [jordan2024muon, liu2025muon].\nZ-loss [chowdhery2023palm] prevents the final output logits from exploding. Namely, , where we set to be as in [olmo2025olmo2].\nDistributed Orthonormalized Updates. We use Dion optimizer [ahn2025dion] for all weight tensors greater than 1D and Lion [chen2023lion] for 1D weight tensors and the [un]embedding layers. The unembedding layer is updated with a learning rate scaled by , where is its input dimension.\nFrom Figure 3, both techniques lead to smoother training dynamics, with lower variance and fewer spikes, and achieve a lower final loss value.\n4.3 Sampling\nThe sampling follows convention [tian2024var]. We apply classifier-free guidance (CFG), first proposed for diffusion models [ho2022cfg] and later adopted in AR-based models [mentzer2023fsq, sun2024llamagen]. At inference, each token‚Äôs logit is formed by , where is the conditional logit, is the unconditional logit, and is the CFG scale. We use layer-wise linearly scaling CFG, first introduced in Infinity [han2025infinity]. We observe that linearly scaling top- is also helpful. For the factorized -itwise configuration, we apply CFG on the normalized probability, i.e. , where according to Eq. 12. Nucleus sampling (top ) [holtzman2019nucleus] is also used.\n5 Experiments\n5.1 Experimental setup\nArchitectures. We train the image tokenizer with different quantization methods on ImageNet-1K [russakovsky2015imagenet]. The experiments cover two network architectures: (1) Vision Transformers (ViT) [dosovitskiy2021vit], which runs at a high throughput and yields high reconstruction fidelity, and (2) ConvNets, which are more commonly seen in image generation. We compare our method with VQ-based [rombach2022ldm, tian2024var] and BSQ-based methods [han2025infinity]. The training details are specified in the Appendix.\nTraining objectives. We use a weighted average of three losses, the mean absolute error (MAE, ), GAN, and perceptual loss, without any other regularization terms. The MAE, GAN, and perceptual loss optimize the PSNR, FID, and LPIPS score according to their respective definitions. Therefore, this trio can no longer be simplified.\nEvaluation. We evaluate image compression in the Kodak Lossless True Color Image Suite. It includes 24 24-bit lossless color PNG images. We report PSNR and MS-SSIM [wang2003msssim] at different levels of bits per pixel (BPP). We assess image reconstruction and generation on the ImageNet-1k validation set. Reconstruction is measured by FID, PSNR, SSIM, and LPIPS [zhang2018lpips]. Generation is measured by FID, Inception Score (IS) [salimans2016is], and improved precision and recall (IPR) [kynkaanniemi2019ipr], calculated by the ADM Tensorflow Evaluation Suite [dhariwal2021adm]. We use rFID/gFID to disambiguate.\n5.2 Main results: Comparison to state-of-the-art\nState-of-the-art image reconstruction. Table 5 compares the image reconstruction results on COCO 2017 and ImageNet-1k. A ViT-based auto-encoder with -SQ reduces rFID by and improves all other metrics.\nState-of-the-art image compression. We show the compression results on Kodak in Table 6. Since the resolution is or , we encode/decode them in tiles without overlapping or padding. We compare our method with traditional codecs, including JPEG2000 [openjpeg] and WebP [google2025webp], and tokenizer-based approaches, including MAGVITv2 [yu2023magvit2] and BSQViT [zhao2025bsq]. -SQ-ViT achieves higher PSNR and MS-SSIM scores while using a slightly smaller BPP. Note that the rate-distortion tradeoff can be further improved ( less bitrate) by training an unconditional AR model for arithmetic coding [deletang2024lmic, zhao2025bsq], which is not the primary focus of this paper.\nState-of-the-art image generation. We select the class-conditioned Infinity [han2025infinity] as a baseline. Infinity-CC employs a 7-level next-scale prediction backbone, saving tokens and running faster than VAR (10 levels) [tian2024var].\n(i) VAR tokenizer. We train a VAR tokenizer with the standard schedule (100 epochs) suggested in Infinity [han2025infinity]. We use -SQ as the bottleneck with two codebook sizes: (1) a complete codebook, whose bitrate is similar to BSQ (), and (2) a subset of 16,384 codes, whose bitrate is equivalent to BSQ (). Figure 5 clearly demonstrates the superiority of our method.\n(ii) VAR generation. Table 7 shows the generation results on ImageNet-1k. We also provide the oracle result computed from the validation set in the bottom row. Infinity-CC+-SQ works comparably with VAR- in terms of model size (1B) while being 30% more efficient. Note that our results achieve a higher recall and push the precision-recall tradeoff closer to the validation oracle. We attribute this to the larger codebook, which better captures visual diversity. When the parameters increase to 2.8B, -CC+-SQ achieves an FID of 1.82, which is comparable to both VAR- and the oracle result.\n5.3 Scientific investigations and ablative studies\nDispersiveness leads to better rate-distortion tradeoff. We start from the comparison in Figure 2 and ask if dispersiveness, quantified by higher , leads to better rate-distortion tradeoff for visual tokenization. We train a plain ViT-small encoder-decoder on ImageNet- while varying only the quantization bottleneck. We also test two vocabulary sizes, medium () and large (). With the codebook fixed, we find -SQ achieves the best rFID, LPIPS, SSIM, and PSNR. The random projection VQ and BSQ follow behind. We also test learnable codebooks given these as initialization. The conclusion still holds, and a learnable codebook does not greatly affect the final results.\nVF alignment helps discrete tokens, too. Figure 5 shows a worse reconstruction quality after aligning with the DINOv2 feature [oquab2023dinov2]. However, Figure 6 demonstrates that the VAR generation using a VF-aligned tokenizer converges faster and achieves better final results in gFID, IS, and especially recall. This extends the findings in VAVAE [yao2025vavae] about VF alignment from continuous latents to discrete ones.\nVAR generation with different heads. We test various prediction heads, namely BCE vs. -way CE for -CC+BSQ, 9-way CE vs. -way CE for -CC+-SQ in Table 9, showing that -SQ +CE achieves great results despite simplicity. The factorized -itwise prediction yields worse gFID and lower recall in both cases, implying that factorized approximation sacrifices diversity. We also find that the optimal sampling hyperparameters vary and conduct a small-scale grid search in Figure 8.\nScaling the codebook size does matter. The last but not least critical question is whether increasing the codebook size benefits the generation results. To answer this, we used the two VAR tokenizers with vs. ) with reconstruction results in Figure 5, and trained VAR models with varied sizes on top while keeping all the rest settings the same. To report the gFID, we search the sampling hyperparameters to find an optimal value. From Figure 7, we conclude that increasing the codebook size improves gFID when the model is large, e.g., 12-layer (0.24B) to 16-layer (0.49B). This echoes the finding in LLMs that larger models deserve larger vocabularies [tao2024scaling]. We look at the improved precision and recall metric in the right subplot of Figure 7. We use top-, CFG of , and vary top- to obtain the data points. We find that when the codebook size increases, the precision-recall Pareto frontier moves towards the oracle precision-recall derived from the val set.\n6 Related work\nVector quantization (VQ) [gray1984vq, van2017vqvae] lays the foundations of learning discrete visual tokens. However, VQ is notoriously difficult to train, which is attributed to the misalignment between the embedding distribution of the model and the codebook [huh2023straightening]. Optimization tricks are then introduced, e.g. Gumbel Softmax [jang2017gumbelsoftmax, baevski2020gumbelvq], Rotation trick [fifty2025rotation], and Index Backpropagation [shi2025ibq]. The line of lattice-based quantization methods covered in this paper [yu2023magvit2, mentzer2023fsq, zhao2025bsq] addresses this misalignment issue by keeping the codebook fixed. Our -SQ is an intuitive extension in this direction.\nScaling visual tokenizers has recently attracted attention and covers many directions, including increasing parameters [xiong2025gigatok], training data [hansen2025learnings], unifying generation and understanding [zhao2025qlip, ma2025unitok], and encoding multiple modalities [wang2024omnitokenizer, lu2025atoken]. Our paper focuses on scaling the vocabulary size. Despite several advances in reconstruction [zhu2024vqganlc, shi2025ibq], none have reported that an expanded vocabulary benefits generation yet. Our paper shows that a visual autoregressive model scales to a very large codebook () without tricks such as index subgrouping [yu2023magvit2], etc.\nAutoregressive visual generation applies an autoregressive model to visual generation [efros1999texture, van2016pixelcnn, chen2020igpt] similar to the LLM paradigm [bengio2003lm, radford2018gpt, brown2020gpt3]. Modern AR model employs a visual tokenizer for efficiency [esser2021vqgan]. Subsequent work explores what to auto-regress [yu2024titok, tian2024var] and auto-regressive order [yu2025rar, pang2025randar]. Most AR models are based on a medium-sized visual vocabulary (), limiting their potential [yu2023magvit2].\nLattice coding has wide applications in digital communication [zamir2014lattice] and cryptography [peikert2016decade]. In this paper, we borrow this concept to describe different non-parametric quantization methods and others in the same language. This further inspires us to devise new quantization methods based on the principle of densest hypersphere packing. The hyperspherical prior is also loosely related to some recent work about learning on a spherical manifold [davidson2018svae, loshchilov2025ngpt].\n[ADDRESS_REMOVED] introduced spherical Leech quantization (-SQ), a novel quantization method that scales the visual codebook to , and demonstrated its applications in visual compression, reconstruction, and generation on ImageNet. In the future, we are interested in verifying its effectiveness in larger-scale settings, e.g., text-conditioned visual generation.\nAcknowledgments. This material is based upon work in part supported by the National Science Foundation under Grant No. IIS-1845485. The authors acknowledge the IFML Center for Generative AI and the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing computational resources that have contributed to the research results reported within this paper. This work was supported by a Hoffman-Yee Research Grant from the Stanford Institute for Human-Centered Artificial Intelligence (HAI). This research was also supported in part by Lambda, Inc. YZ would like to thank Jeffrey Ouyang-Zhang and Mi Luo for their help in setting up environments on TACC; Yi Jiang and Bin Yan for their clarification on VAR and Infinity baselines.\nAppendix A Constructing Fibonacci Lattices\nFibonacci lattice constructs points that ‚Äúare evenly distributed with each of them representing almost the same area‚Äù [gonzalez2010measurement] in a unit square using the formula:\nwhere . We can map this point distribution to a unit-length sphere using cylindrical equal-area projection.\nA.1 Generalizing the Spherical Fibonacci Lattice to higher dimensions\nWe provide the details to generalize the 3D spherical Fibonacci lattice [gonzalez2010measurement] to higher dimensions [stackexchange3297830].\nGiven a -dimensional vector , we can also represent it in the hyperspherical coordinate system , where , , and specifically, for . The conversion to Cartesian coordinates is given as follows:\nWe examine the distribution over the angular coordinates .\nThe key observation is that the angles are independently distributed. To see this, if we fix , then parameterizes a ‚Äúsubsphere‚Äù isoporphic to with a rescaled radius . In other words, for any . As such, Equation 16 can be simplified to:\nThe absolute value of the Jacobian determinant for the change of variables is\nTherefore, Equation 16 reduces to\nWith Equation 17, we have . Then we get the normalization constants for and . Finally, we arrive at\nDenote the cumulative distribution function with another variable\nThe Fibonacci-like spiral is generated by the following formula:\nwhere refers to ‚Äôs decimal part, i.e. . satisfies .\nThe angles are given by taking the inverse:\nAppendix B Details of the Leech Lattice\nGenerator matrix. The generator matrix for the unconstrained is given in Table 10.\nAppendix C Experimental Details\nC.1 Architectures\nViT tokenizer. The architecture used to get the main results follows [zhao2025bsq]. To conduct ablative studies in Table 8, we use a smaller architecture (ViT-small) for fast iteration. Both the hidden dimension and the number of layers are halved.\nCNN-based VAR tokenizer. The architecture follows Infinity [han2025infinity]999https://github.com/FoundationVision/BitVAE. We use 7 scales: , which amounts to 521 tokens in total.\nInfinity-CC. The Infinity model used in the paper mostly follows the original Infinity paper [han2025infinity]. We make two key changes: (1) The text condition is converted to class condition, which is representation by a single <SOS> token. The vocabulary size is augmented to , where . The additional refers to the ‚Äúno-class‚Äù index, which is used to randomly replace the original class index with a probability of , to enable classifier-free guidance (CFG) at the sampling phase. (2) We revert the shared AdaLN to an unshared version following VAR [tian2024var]. Table 11 summarizes the model configurations and parameter size in this paper.\nC.2 Training specifications\nViT tokenizer. We train the image tokenizer with a batch size of 32 per GPU. We use AdamW optimizer with with weight decay. The base learning rate is (or a total learning rate of ) and follows a half-period cosine annealing schedule. The model is trained for 1M steps, which amounts to 200 epochs over the entire ImageNet-1k training set. We use an loss weight of , a perceptual loss weight of , and an adversarial loss weight of throughout the experiments.\nVAR tokenizer. We train the VAR tokenizer with a batch size of 8 per GPU. Two schedules are used: (1) the fast schedule trains the model for 500k iterations with 8 GPUs, which approximately sees the training data 25 epochs; (2) the standard schedule trains the model for 500k iterations with 32 GPUs, which is approximately 100 epochs.\nAppendix D More Results\nD.[ADDRESS_REMOVED], we retrain a VAR tokenizer with a fast schedule (25 epochs). We use -SQ as the bottleneck with two codebook sizes: (1) the full codebook, whose bitrate is similar to BSQ (), and (2) a subset of 16,384 codes, whose bitrate is equivalent to BSQ (). From the upper half of Table 12, -SQ outperforms BSQ in all metrics in both cases. Next, we train a VAR tokenizer with the standard schedule (100 epochs) suggested in Infinity [han2025infinity]. The full numbers are reported in the bottom half of Table 12, supplementing Figure 5.\n. rFID‚Üì LPIPS‚Üì SSIM‚Üë PSNR‚Üë (fast schedule) BSQ 16,384 1.82 0.1268 0.5626 19.989 -SQ‚Ä† 16,384 1.36 0.1170 0.5957 20.639 BSQ 262,144 1.29 0.1106 0.6006 20.683 -SQ 196,560 1.08 0.1005 0.6280 21.315 -SQ (vf) 196,560 1.18 0.1088 0.6006 20.734 (standard schedule) BSQ 262,144 1.07 0.1064 0.6035 20.430 -SQ 196,560 0.84 0.0954 0.6333 21.535 -SQ (vf) 196,560 0.92 0.1041 0.6118 21.188\nD.2 VAR generation\nGrid search of sampling parameters. We run a small-scale grid search of sampling hyperparameters for Infinity-CC with different prediction heads. We compare the gFID score on IN-1k by generating 10 samples per class (10k generated samples in total). From Figure 8, we conclude that the optimal top varies significantly across different prediction head settings.\nAdvanced sampling techniques. In Section 4.3, we introduced advanced sampling techniques, including layerwise linearly scaling CFG and linearly scaling top-. We show related ablation studies in Table 13. We use to denote the linear scaling strategy, which starts from and changes by per scale. We can see that both layerwise linear scaling CFG and top- bring a noticeable improvement.\nIt is also worth noting that, according to the bottom half of Table 13, the optimal decreases when the tokenizer is trained with the VF loss. This is most likely because the probability density is more skewed, as is illustrated in Figure 4.\n. Tokenizer rFID CFG top gFID -SQ (25 ep) 1.08 2 8.78 -SQ (100 ep) 0.84 2 7.46 -SQ (100 ep) 0.84 6.81 -SQ (100 ep) 0.84 7.33 -SQ (100 ep) 0.84 6.68 -SQ (vf) 1.18 5.79 -SQ (vf) 1.18 5.41 -SQ (vf) 1.18 5.30\nQualitative Results. Figure 9 shows more generation results sampled by Infinity-CC + -SQ (2B). We cherry-pick the images and emphasize the quality and diversity."
  },
  {
    "article": "\\newtoggle\ncomments\n\\toggletrue\ncomments"
  },
  {
    "article": "Universal Reasoning Model\nAbstract\nUniversal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art‚àó 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. 00footnotetext: ‚àóThis comparison focuses on pass@1 score of single small models trained from scratch under the same data setting as HRM and TRM, excluding test-time scaling, ensembling, and visual methods such as VARC [VARC]. Our code is avaliable at [URL_REMOVED]\n1 Introduction\nRecent advances in recurrent models [hrm1, hrm2, trm] have demonstrated the effectiveness of Universal Transformers (UTs) [ut] in addressing complex reasoning tasks, such as ARC-AGI and Sudoku [arc1, arc2]. UT-based small models, despite being trained from scratch on these tasks without internet-scale pre-training, consistently outperform most standard Transformer-based Large Language models (LLMs) by a significant margin [hrm1].\nWhile this contrast highlights the potential of UTs for depth-intensive iterative reasoning, the function and impact of gating mechanisms remain insufficiently explored beyond their initial intuition\nPrior studies often attribute improvements to high-level architectural innovations [hrm1, hrm2, trm], yet our analysis reveals that the core performance gain actually arises from the often-overlooked recurrent inductive bias intrinsic to the Universal Transformer. In particular, nonlinear depth-wise computation plays a much larger role than previously acknowledged, suggesting that architectural modifications that enhance recurrent processing can yield substantial downstream improvements. Motivated by this insight, we further investigate and strengthen this inductive bias via a simplified yet effective enhancement to the UT framework, enabling stronger abstraction capabilities while preserving parameter efficiency.\nOur main contributions are as follows:\n-\n‚Ä¢\nThrough extensive ablation studies, we show that the performance of models on ARC-AGI‚Äìstyle complex reasoning tasks primarily stems from their nonlinearity. Moreover, we reveal that the true source of reasoning capability beyond standard Transformers comes from the recurrent mechanism of Universal Transformers rather than overly elaborate design in prior work.\n-\n‚Ä¢\nBy introducing short convolutions and truncated backpropagation into the Universal Transformer, we achieve a state-of-the-art 53.8% pass@1 accuracy on ARC-AGI 1 and 16.0% on ARC-AGI 2.\n2 Preliminaries\n2.[ADDRESS_REMOVED] Transformer\nLet denote the vocabulary of size , and let be an input sequence of length . We define the token embedding function as , mapping discrete tokens to a -dimensional continuous representation. Conversely, the unembedding function (or language modeling head) is denoted by , which projects hidden states back to the vocabulary logit space.\nA single Transformer layer, parameterized by , is defined as a function . This function typically composes a Multi-Head Self-Attention (MHSA) module and a Position-wise Feed-Forward Network (FFN), each wrapped with residual connections and layer normalization:\nA standard, non-recursive Transformer model of depth is constructed by stacking layers with distinct parameters . The forward pass is the composition of these layers:\nHere, the operator denotes function composition. The computational cost and parameter count both scale linearly with , creating a rigid coupling between model capacity and inference compute.\n2.2 Universal Transformer\nThe Universal Transformer (UT) [ut] extends the standard Transformer [attn] by introducing recurrent computation over depth. Instead of stacking distinct layers, the UT applies a single transition block repeatedly to refine token representations. For an input sequence with embedding matrix , the UT updates states as\nfollowed by a shared position-wise transition function\nwhere is either a feed-forward network or separable convolution. To encode both position and refinement depth, UT adds 2-D sinusoidal embeddings at each step.\n2.2.1 Parameter Sharing\nA key design of UT is weight tying across depth. The attention and transition parameters\nare reused for all . Thus, the model performs iterative representation refinement with a flexible number of steps , enabling (i) depth adaptation at inference and (ii) higher theoretical expressivity than fixed-depth Transformers.\n2.2.2 Adaptive Computation Time (ACT)\nWith ACT [act], different tokens may halt at different recurrent steps. At step , each position predicts a halting probability\naccumulated until reaching threshold . The final token representation is a weighted mixture\nwhere is the truncated allocation. ACT allows UT to allocate more computation to complex tokens and less to simpler ones.\n3 Universal Reasoning Model\nThe base architecture of our Universal Reasoning Model (URM) closely follows that of the Universal Transformer [ut], with the difference being its decoder-only design. This aspect is consistent with previous works such as HRM [hrm1] and TRM [trm]. Our work differs from previous models [hrm1, trm] by introducing the following ConvSwiGLU module and a Truncated Backpropagation Through Loops mechanism.\n3.1 ConvSwiGLU\nTo strengthen the non-linearity of Universal Transformer, we introduce a ConvSwiGLU (motivation see Section 4.6), which augments the standard SwiGLU feed-forward block with a depthwise short convolution. Unlike the conventional point-wise SwiGLU [glu], which treats each token independently, our design explicitly injects local contextual interactions into the gating mechanism, introducing lightweight channel mixing in token space without increasing sequence-level complexity [plm, metaformer].\nGiven an input sequence , we first project it into an expanded intermediate representation:\nThe SwiGLU activation produces a gated representation:\nTo integrate short-range token interactions, we apply a depthwise 1D convolution over the gated features:\nwhere is a depthwise convolution kernel of size .\nFinally, the output is projected back to the hidden dimension:\n3.2 Truncated Backpropagation Through Loops\nWhen the number of recurrent reasoning loops becomes large, the gradients propagated from early loops may hinder optimization due to noise accumulation and instability (see empirical evidence in Section 4.5). To alleviate this issue, we employ Truncated Backpropagation Through Loops (TBPTL) and only compute gradients for the later loops.\nConsider a -layer Universal Reasoning Model unrolled for iterative loops during training. Let denote the hidden representation of layer at iteration . The recurrent transition is defined as:\nwhere denotes the parameterized transformation at layer with trainable parameters .\nInstead of backpropagating through all loops, we partition the rollout into forward-only and trainable segments. Specifically, for a truncation index :\nDuring training, we compute gradients only on the loss accumulated in the latter loops:\nwhere is cross-entropy loss function. The gradients with respect to are thus:\nExample. For a configuration with layers and inner loops, we choose forward-only loops. Thus, only the last loops (i.e., to ) contribute to gradient computation.\n4 Experiment\n4.1 Experiment Settings\nOur experimental setup largely follows HRM and TRM [hrm1, trm]. We use the same datasets and augmented data as in prior work, and apply an exponential moving average (EMA) to model parameters to improve training stability, following [trm]. All models are trained with the AdamAtan2 optimizer [adamatan2]. For ARC-AGI 1 and ARC-AGI 2, the main model learning rates are set to and , respectively, while the puzzle embedding uses a learning rate of ; for Sudoku, the puzzle embedding learning rate is . Weight decay is set to 0.1 for both the main model and puzzle embedding on ARC-AGI 1 and ARC-AGI 2, and to 1.0 for Sudoku, consistent with prior work. The model has 4 layers with hidden size 512 and 8 attention heads. The inner loop runs for 8 steps, with the first two steps being forward-only, while the outer loop employs Adaptive Computation Time (ACT) [act] with a maximum of 16 steps.\n4.2 Main Results\nAs shown in Table 1, the Universal Reasoning Model (URM) achieves substantial improvements over prior UT-based approaches across all benchmarks. On ARC-AGI 1, URM reaches 53.8% pass@1, outperforming TRM (40.0%) and HRM (34.4%) by large margins. On ARC-AGI 2, URM obtains 16.0% pass@1, nearly tripling HRM and more than doubling TRM. A similar advantage appears on Sudoku, where URM achieves 77.6% accuracy, surpassing both TRM and HRM.\nNotably, URM‚Äôs gains further widen under larger sampling budgets (e.g., pass@1000), indicating that iterative refinement enables richer candidate generation rather than brittle one-step predictions.\n4.3 Why Universal Transformer?\nTable 2 demonstrates that the performance gains of Universal Transformers (UTs) on ARC-AGI 1 arise from substantially higher parameter efficiency rather than increased model scale or computational budget. With only 4√ó parameters, a UT achieves a pass@1 score of 40.0, dramatically outperforming vanilla Transformers that employ up to 32√ó more parameters yet remain markedly weaker. Simply scaling depth or width in vanilla Transformers yields diminishing returns and can even lead to performance degradation, highlighting a fundamental inefficiency in how parameters are used to support multi-step reasoning.\nCrucially, this advantage persists even when computation is held constant. At 32√ó FLOPs, reallocating computation from deep, non-shared layers to recurrent refinement improves pass@1 from 23.75 for vanilla Transformers to 40.0 for UTs. This behavior is consistent with analyses of previous works [onthepower], which argue that many reasoning tasks benefit more from iterative computation than from increasing the number of independent layers. In standard Transformers, additional FLOPs are often spent on redundant refinement in higher layers, whereas recurrent computation converts the same budget into increased effective depth [importance, onthepower].\nThis superior efficiency is driven by the recurrent inductive bias introduced by parameter sharing across depth. Through repeated application of a shared transformation, UTs realize iterative refinement that better aligns with the structure of algorithmic reasoning, while avoiding any increase in parameter count. Consequently, under both fixed parameter and fixed FLOPs budgets, UTs consistently outperform vanilla Transformers on reasoning tasks, making them particularly well suited for reasoning-intensive settings such as ARC-AGI, where multi-step abstraction is more critical than sheer scale.\n4.4 Short Convolution\nTo strengthen the nonlinear inductive bias of the Universal Transformer, we introduce a depthwise short convolution module parameterized by (see Section 3.1 for details), which provides token-local mixing while preserving the per-step computational budget. Since ARC-AGI performance correlates strongly with nonlinear capacity (Section 4.6), we evaluate how inserting this module at different locations affects the recurrent transition.\nWe examine six insertion points: (a) after the SDPA output; (b) after the value projection; (c) after the key projection; (d) after the query projection; (e) between multi-head concatenation and the output projection; and (f) after the MLP expansion.\nAs shown in Figure 3, inserting the module inside the attention pathway, positions (a)‚Äì(d), does not yield improvements and often degrades performance, suggesting that local perturbations interfere with the geometric structure of attention‚Äôs linear projections. A mild gain appears at position (e), where the perturbation acts only on aggregated multi-head features.\nThe dominant effect arises at position (f), after the MLP expansion, indicating that short-range mixing is most beneficial when applied within an already nonlinear subspace. This supports a functional interpretation in which the MLP‚Äînot attention‚Äîconstitutes the model‚Äôs primary source of expressive nonlinearity; augmenting it with substantially enhances the model‚Äôs nonlinear representational capacity.\nAs shown in Fig. 4, the incorporation of short convolution into the MLP significantly enhances channel mixing. While the standard Universal Transformer exhibits relatively sparse and homogeneous attention patterns, the model with ConvSwiGLU produces attention matrices with more diverse and structured distributions. This suggests that short convolution facilitates more effective inter-channel information flow, thereby improving the expressiveness of the attention mechanism.\n4.5 Truncated Backpropagation Through Loops\nAs shown in Table 5, when the total number of inner loops is fixed to 8, truncating gradients for the first two loops‚Äîi.e., running the initial two inner-loop iterations in forward-only mode‚Äîachieves the best performance. Both pass@1 and pass@1000 peak at this truncation setting, while shorter or longer truncation horizons result in inferior outcomes.\nThis trend closely resembles truncated backpropagation through time (TBPTT) in recurrent neural networks, where the underlying motivation is largely the same. In full backpropagation through time, gradients are propagated through the entire sequence, which incurs high computational and memory costs and often yields ineffective long-range gradients due to vanishing or exploding behaviors. As a result, practical implementations typically restrict gradient propagation to a fixed recent window, e.g., by backpropagating errors only through the last time steps and updating the network parameters accordingly [difficulty, tbptt].\nSimilarly, in universal transformers, propagating gradients across all inner-loop iterations can lead to unstable optimization, while overly aggressive truncation limits the model‚Äôs ability to coordinate multi-step refinement. Moderately truncating gradient propagation therefore provides a favorable balance between optimization stability and effective long-horizon learning.\nWe note that all results in this experiment are obtained using a two-layer URM without the short convolution module, which differs from the full URM model reported earlier.\n4.6 Nonlinearity of Transformers\nAs shown in Table 4, the performance on ARC-AGI 1 decreases monotonically as nonlinear components are progressively removed from the model. Among these components, the activation function in the MLP plays a particularly critical role: replacing SwiGLU with simpler nonlinearities such as SiLU or ReLU leads to substantial degradation, while completely removing the attention softmax results in a dramatic collapse in performance. This clear monotonic trend highlights the importance of strong nonlinear transformations for solving complex abstract reasoning tasks.\nThese results suggest that the expressive power required for ARC-AGI primarily arises from rich nonlinear mappings. Weakening the nonlinearity may systematically limits the model‚Äôs ability to represent complex reasoning skills.\nWe note that the model still retains certain forms of nonlinearity that are not ablated in this study, such as the RMSNorm applied after each layer and the dot-product interaction between queries and keys in attention. However, these components are either difficult to remove without causing training instability or represent relatively weak nonlinear effects compared to explicit activation functions. As ablating them typically leads to training failure, they fall outside the scope of the present analysis.\n4.7 Muon Optimizer\nTo evaluate the training efficiency of the Universal Reasoning Model (URM), we compare the Muon (Momentum Updated Orthogonal Newton) optimizer[jordan2024muon] with a standard adaptive baseline, Adamatan2[adamatan2]. Muon approximates second-order curvature to apply orthogonal updates to better handle the complex loss landscapes[gong2025makesloopedtransformersperform] induced by deep recurrent structures. Both models are trained from scratch under identical experimental settings, including batch size, learning rate schedules, and data augmentation, ensuring that any observed differences arise solely from the choice of optimizer.\nAcross the ARC-AGI 1 and ARC-AGI 2 benchmarks, Muon demonstrates substantially faster convergence. On ARC-AGI 2, the Muon-optimized model reaches a pass@1 accuracy of 11.5% in approximately 600,000 training steps, whereas the Adamatan2 baseline requires over 1,300,000 steps to achieve the same performance, corresponding to nearly a twofold speedup in optimization. Despite this advantage in early training, both methods converge to similar final accuracies (approximately 53.8% on ARC-AGI 1 and 16.0% on ARC-AGI 2), indicating comparable asymptotic performance.\nThese results suggest a separation between optimization efficiency and architectural capacity in the URM. While Muon preconditions the challenging spectral properties of recurrent weight matrices[liu2025muonscalablellmtraining] and reduces training cost, it does not lead to improved final generalization.\n5 Related Work\n5.1 ARC-AGI\nPrior work on the ARC-AGI benchmark [arc1, arc2] spans vision-based formulations, large language model (LLM) adaptation, and recurrent reasoning architectures. Vision-centric approaches such as Vision ARC [VARC] reformulate ARC as an image-to-image transformation problem and show that standard visual inductive biases can achieve competitive performance, particularly with ensembling and test-time scaling. LLM-based methods explore fine-tuning and test-time training, demonstrating that transient parameter updates outperform static in-context learning on ARC-like tasks. Beyond language and vision models, recurrent architectures emphasize iterative computation as a core mechanism for abstraction. The Hierarchical Reasoning Model (HRM) [hrm1, hrm2] introduces multi-timescale recurrence and achieves strong ARC-AGI results, while subsequent analyses suggest that its gains may largely stem from recurrence rather than explicit hierarchy. The Tiny Recursive Model (TRM) [trm] further simplifies this paradigm, showing that a single lightweight network applied recursively can match or exceed more complex hierarchical designs.\n5.2 Universal Transformers (Looped Transformers)\nThe Universal Transformer (UT), also known as the Looped Transformer, was introduced by Dehghani et al. [ut] as an extension of the standard Transformer with recurrent computation and adaptive computation time. Subsequent work has shown that UTs exhibit significantly stronger multi-step reasoning abilities than vanilla Transformers, as the recurrent refinement mechanism helps overcome architectural limitations in multi-hop reasoning tasks [devil, grok]. In addition, UTs demonstrate improved algorithmic learning capabilities, enabling more effective modeling of iterative and rule-based computations [algo]. By reusing parameters across refinement steps, UTs also achieve higher parameter efficiency, allowing more expressive computation without increasing model size [onthepower].\n6 Conclusion\nWe systematically investigate the sources of performance gains in Universal Transformer models on complex reasoning tasks. Extensive ablation studies reveal that these gains stem primarily from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from overly complex architectural designs. Motivated by this insight, we propose the Universal Reasoning Model (URM), which enhances nonlinear depth-wise computation via short convolutional gating and improves optimization stability through truncated backpropagation through loops. URM achieves state-of-the-art performance on ARC-AGI 1 and 2."
  },
  {
    "article": "Native and Compact Structured Latents for 3D Generation‚Ä†‚Ä†thanks: Open-source project; see our project page for code, model, and data.\nAbstract\nRecent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper presents an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.\n1 Introduction\n3D generative modeling has progressed at an unprecedented rate recently, spurred by innovations in latent 3D representation design and the integration of large latent learning and generative models [zhang2024clay, xiang2025structured, chen2025dora, he2025sparseflex, wu2025direct3d]. These advancements have dramatically enhanced both reconstruction fidelity and generation realism, bringing 3D content creation closer to real-world deployment and industrial applications.\nDespite the progress, the field still lacks fundamental representations that can both faithfully capture the full-spectrum information of arbitrary 3D assets and effectively be processed into latents with neural networks. Recent large 3D generation models [zhang2024clay, xiang2025structured, chen20243dtopia, chen2025dora, wu2025direct3d, he2025sparseflex, li2025sparc3d] predominantly leverage iso-surface fields (e.g., signed distance function, Flexicubes [shen2023flexicubes]) to represent geometry, which have intrinsic limitations in handling open surfaces, non-manifold geometry, and enclosed interior structures. Moreover, most existing works focus on 3D shape generation while neglecting the appearance and material information inherent in 3D assets that are fundamentally correlated with shape. [xiang2025structured] introduces a structured 3D latent (SLAT) representation that jointly models geometry and appearance, but its reliance on multiview 2D image feature input and pure rendering-based supervision leads to deficiencies in capturing complex structures and materials.\nIn this work, we introduce a new ‚Äúfield-free‚Äù sparse voxel structure termed O-Voxel. O-Voxel is an omni-voxel representation that encodes both geometry and appearance, serving as a nexus between mesh assets and neural networks. The ‚Äúomnipotence‚Äù of this structure not only lies in its integrated approach to modeling geometry and appearance but also in its robust capacity to handle their inherent complexity. For geometry, it can handle arbitrary topology including open, non-manifold, and fully-enclosed surfaces, unlike existing field-based methods. By introducing a flexible dual grid structure corresponding to the primal sparse voxels, it can accommodate complex topology without lossy data preprocessing and preserve sharp edge features and normal discontinuities for accurate geometry reproduction. For appearance, it can capture arbitrary surface attributes beyond mere texture color. In this work, we implement physically-based rendering (PBR) parameters aligned with surface geometry to enable re-lighting capability. In particular, it incorporates material opacity, allowing it to handle translucent surfaces‚Äîa capability not present in previous methods.\nAnother notable advantage of O-Voxel is its instant bidirectional conversions to and from raw 3D assets. The processes are both optimization-free and rendering-free. Transforming a mesh into this structure takes only a few seconds on a single CPU, while reconstructing surfaces and materials from it completes within tens of milliseconds.\nGiven the O-Voxel representation, we design a sparse 3D variational autoencoder to learn a proper latent space. This leads to a native structured latent space compared to that in [xiang2025structured] which is built from multiview 2D information. Furthermore, we aim for a compact latent space to support efficient and high-resolution 3D generation. Utilizing a residual autoencoding design [chen2024deep] applied to sparse voxel structure, our VAE achieves a 16 spatial downsampling, a high ratio not seen in prior voxel-based methods. Our approach encodes a fully textured asset with resolution into only 9.6K latent tokens with negligible perceptual degradation upon reconstruction. Experiments show that our reconstruction quality surpasses prior methods by a wide margin, albeit using substantially fewer tokens. In addition to enabling high-resolution 3D generation, our compact latent space also facilitates the scaling of the generative model.\nWe train large flow-matching generative models in the learned latent space for image-to-3D generation. The models contain about 4 billion parameters in total and are trained on diverse public 3D asset datasets. Despite their scale, inference remains highly efficient: it takes only 3s for fully-textured assets generation, 17s for the resolution, and 60s for the resolution on a NVIDIA H100 GPU, which are significantly faster than existing large 3D generation models. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models, as demonstrated in our experiments. All our model, code, and dataset will be publicly released to facilitate reproduction and further research.\n2 Related Work\n3D Representations for Generation.\nEffectively representing geometry and appearance for neural network processing is a key challenge in 3D generation. Early works adopted implicit fields or their discretized structures to represent shape, such as occupancy fields [mescheder2019occupancy] and Signed Distance Functions (SDF) [park2019deepsdf, deng2021deformed, hui2022neural, zheng2023lasdiffusion]. NeRF [mildenhall2021nerf, tang2023volumediffusion, muller2023diffrf] integrates geometry and appearance in a radiance field, yielding realistic rendering but suffering from low geometry quality and heavy sampling costs. Unstructured 3D representations, including meshes [nash2020polygen, chen2024meshanything], point clouds [nichol2022point, luo2021diffusion, zhou20213d], and Gaussians [kerbl20233d, yu2024mip, zhang2024gaussiancube, he2024gvgen], offer explicit 3D representations but lack structural regularity, posing challenges for network processing and latent compression. Recent works have introduced structured representations tailored for 3D generation [he2025sparseflex, li2025sparc3d]. They combine field-based iso-surface modeling with sparse voxels to achieve high-resolution geometry. However, their reliance on field-based primitives limits their capability to represent open or non-manifold surfaces and they do not handle appearance, unlike our approach.\nLatent 3D Representations.\nRecent advances in 3D generation have increasingly shifted from using explicit geometric representations to compact latent spaces, analogous to those used in the 2D domain [rombach2022high]. Various latent formulations have been explored, including latent point clouds [lan2024ga, yang2024atlas, chen20243dtopia, vahdat2022lion], volumetric or hierarchical grids [ntavelis2023autodecoding, xiong2025octfusion, ren2024xcube, meng2025lt3sd], and triplane embeddings [wu2024direct3d, lan2024ln3diff, gupta20233dgen]. Among these, two latent paradigms have emerged as the dominant choices for recent large 3D generation models. The first is the unstructured latent, inspired by the Perceiver-style architectures [jaegle2021perceiver], where 3D data are encoded as unordered feature vectors. Representative works include [zhang20233dshape2vecset, zhao2024michelangelo] and the follow-up methods [jun2023shap, zhang2024clay, li2024craftsman, li2025step1x, hunyuan3d2025hunyuan3d]. These methods can achieve strong compression but are typically constrained by reconstruction fidelity. The second category is the structured latent built upon sparsity priors, exemplified by [xiang2025structured] and its extensions [li2025sparc3d, he2025sparseflex]. Such representations yield high geometric accuracy but require a larger number of latent tokens, which reduces compression efficiency. Some efforts [wu2025direct3d, chen2025ultra3d] attempted to mitigate this issue by optimizing network computation rather than improving latent compactness. In this work, our method learns a compact structured latent space directly from native 3D inputs, achieving higher spatial dowmsampling rate and fewer latent tokens.\nLarge 3D Asset Generation Models and Systems.\nWith the rapid expansion of large-scale 3D asset datasets [deitke2023objaverse, deitke2024objaverse, zhang2025texverse], there has been a surge in large models and systems capable of generating 3D assets with high-quality shapes and textures [zhang2024clay, xiang2025structured, chen20243dtopia, hunyuan3d2025hunyuan3d, yang2025pandora3d, li2025step1x, li2025triposg]. A common paradigm decomposes the generation into two stages: shape generation and multi-view texture synthesis [zhang2024clay, hunyuan3d2025hunyuan3d, yang2025pandora3d, li2025step1x, li2025step1x, li2025triposg].\nWhile this pipeline benefits from powerful 2D image diffusion backbones, it typically requires complex multi-view rendering, baking, and texture alignment, which hinder scalability and often introduce appearance inconsistencies. [xiang2025structured] tackles 3D generation with material information, yet still relies on multiview baking to merge the generated mesh and 3D Gaussians for asset extraction. In contrast, our approach performs native, end-to-end 3D asset generation, directly producing high-fidelity, fully textured 3D assets without any view-dependent postprocessing.\n3 Method\nOur objective is to generate high-resolution 3D assets with arbitrary shape topology and flexible material attributes. An overview of our approach is presented in Fig. 2.\n3.1 O-Voxel: A Native 3D Representation\nGiven a 3D asset, O-Voxel represents it as a collection of feature tuples associated with sparse voxels on a regular 3D grid of resolution :\nwhere encodes local geometric information, encodes material properties, and denotes the coordinate of the -th active voxel. Empty voxels that do not intersect with the asset are set inactive.\n3.1.1 Flexible Dual Grid for Shape\nThe O-Voxel can robustly represent surfaces with arbitrary topology, owing to its Flexible Dual Grid formulation. In this dual grid, one vertex is defined per primal cell and one quadrilateral face per primal edge, connecting dual vertices in adjacent primal cells. By constructing a dual grid corresponding to the primal regular voxel grid, our algorithm flexibly adjusts the positions of dual vertices and the existence of dual grid faces to accurately represent arbitrary input surface data (illustrated in the first row of Fig. 3).\nThis formulation is inspired by Dual Contouring (DC) [ju2002dual, chen2022neural], an algorithm to extract surfaces from a signed grid with edges tagged by Hermite data (i.e., intersection points and normals). The original DC was designed to process descritized scalar fields such as SDFs. Different from DC, we do not utilize any field representation. Our approach is straightforward: we directly use the asset‚Äôs mesh surface to determine edge intersection flags (rather than detecting sign changes as in DC) and to assign Hermite data. Each edge that intersects the mesh activates the corresponding dual face, and the associated Hermite data are then used to adjust the positions of the dual vertices. Given Hermite data , we compute the dual vertex in closed form using the following quadratic error function (QEF):\nThe original QEF in DC only contains the first component, which measures the squared distance from to the plane determined by , . We introduce an additional error term that penalizes the distance between and any boundary edges of the mesh intersecting the primal cell, , as well as a regularization term that encourages to stay close to the average of the intersecting points, . The former guides the dual vertex to align with boundary edges, improving the representation of open surfaces, while the latter encourages a smoother vertex distribution and stabilizes the QEF optimization against singularities.\nBased on the above algorithm description, for each active voxel, our geometric feature comprises:\n-\n‚Ä¢\nDual vertex , a vertex within the grid to represent local surface shape.\n-\n‚Ä¢\nEdge intersection flags , which determine the quad connections among neighboring dual vertices. We use the surface-edge intersections on 3 predefined voxel edges along the , , and axes, respectively (e.g., those that share the minimum-coordinate corner)111The flags for the other 9 voxel edges are stored in neighboring voxels..\n-\n‚Ä¢\nSplitting weights , controlling how quadrilateral faces are adaptively subdivided into triangles, following the flexible topology rule in [shen2023flexicubes].\nThe conversion algorithm between O-Voxel and mesh is summarized as follows (see also Fig. 3 for an illustration).\nMesh O-Voxel. Given a mesh, we first identify all voxel edges intersecting with the mesh surface and mark their neighboring voxels as active. The intersection points and their normals are computed analytically from mesh triangles, yielding Hermite data. The dual vertex for each active voxel is then calculated by solving the QEF in Eq. (2).\nO-Voxel Mesh. From an O-Voxel, we recover mesh surfaces by connecting dual vertices across intersected edges, forming quadrilateral faces among neighboring active voxels. Each quadrilateral can be adaptively subdivided into two triangles guided by the splitting weights, allowing the surface to better conform to local geometric features.\nThe Flexible Dual Grid offers several key advantages:\n-\n1.\nInstant bidirectional conversion ‚Äî it enables rapid mapping with meshes and supports high-resolution conversion with minimal computation overhead. The costly processes in prior works such as SDF evaluation, flood-fill procedure, and iterative optimization are not needed.\n-\n2.\nArability-topology modeling ‚Äî it is free from the watertight and manifold constraints, enabling robust handling of arbitrary geometry including self-intersecting surfaces and fully-enclosed interior structures.\n-\n3.\nHigh precision and sharp feature preservation ‚Äî the dual vertices are aligned with local geometric features by algorithm design [ju2002dual], allowing for the preservation of sharp features. Additionally, the dual vertex positions and splitting weights can receive learnable adjustments by neural network using other supervisions such as rendering loss (see Section 3.2.2), further enhancing the flexibility and precision of the geometry.\n3.1.2 Volumetric Attributes for Material\nThe O-Voxel can model arbitrary surface attributes aligned with surface geometry, including color and other material attributes. In this work, we implement physically-based rendering (PBR) parameters to capture the intrinsic light‚Äìsurface interaction characteristics of the materials. Specifically, our material feature for each active voxel consists of six channels:\nwhere denotes the base color, the metallic ratio, the roughness, and the opacity. This parameterization follows the standard PBR convention widely adopted in modern physically-based rendering pipelines. The conversion between O-Voxel data and mesh texture is simple and fast, as illustrated in Fig. 3.\nTexture O-Voxel. For an active voxel, we project its center onto each intersected triangle and sample each material attribute from the texture map using UV coordinates and appropriate mipmap levels. The sampled attributes are weighted-averaged based on the point-to-surface distances to obtain the final value. More details of this process can be found in the supplementary material.\nO-Voxel Texture. During reconstruction, for each query point ‚Äì either a mesh vertex position for vertex coloring or a 3D surface point corresponding to a texture map texel ‚Äì material attributes are obtained via trilinear interpolation of the neighboring voxel attributes. The reconstructed mesh is then ready for rendering without need for any additional post-processing.\n3.2 Sparse Compression VAE\nWe apply a VAE to learn a proper latent space from O-Voxel data. Our goal is to obtain a compact latent space that facilitates efficient, high-resolution 3D generation. We design a Sparse Compression VAE (SC-VAE) to achieve this.\n3.2.1 Network Architecture\nThe architecture of our SC-VAE is illustrated in Fig. 4. Unlike transformer-based designs in priors work [xiang2025structured, he2025sparseflex], our SC-VAE employs a fully sparse-convolutional network that is both computationally efficient at high resolutions and generalizes well across scales. Following a U-shaped VAE design [rombach2022high], our encoder hierarchically downsamples sparse voxel features through multiple residual blocks, and the decoder mirrors this process for reconstruction. We meticulously design the residual and (down/up)sampling blocks to enable high-compression encoding and faithful recovery.\nSparse Residual Autoencoding Layer.\nWe adapt the Residual Autoencoding principle from DC-AE [chen2024deep] to sparse voxel data by introducing non-parametric residual shortcuts within downsampling and upsampling blocks. These shortcuts mitigate optimization challenges under high spatial compression by rearranging information between space and channel dimensions in the sparse grid.\nSpecifically, for a downsampling factor of 2, we aggregate each voxel‚Äôs eight children into its channel dimension. Given input features and target coarse features (typically ), we have:\nwhere the operation averages grouped channels to produce a coarse residual estimate. Missing voxels contribute zero vectors due to sparsity.\nDuring upsampling, a symmetric channel-to-space shortcut distributes each coarse feature back to its neighborhood:\nwhere copies channels within each group to match the target dimension.\nEarly-pruning Upsampler.\nTo further enhance efficiency, we employ an early-pruning mechanism [ren2024xcube] for the upsampler. Before each upsampling step, the module predicts a binary mask specifying the active child voxels of each parent node. Inactive nodes are skipped subsequently, thus greatly reducing runtime and memory cost.\nOptimized Residual Block.\nSparse convolutions exhibit low effective computation and parameter efficiency for high sparsity data. In light of this, we redesign the residual block by reducing convolutional layers and incorporating point-wise MLPs for richer feature transformation. Specifically, we substitute the standard design of two conv layers with a single conv layer and fewer normalization and activation layers, following the ConvNeXt-style [liu2022convnet] simplification. The second conv is replaced by a wide point-wise MLP ‚Äì analogous to a Transformer FFN ‚Äì which expands channel dimensions for enhanced nonlinearity and representation. This modification does not affect efficiency but improves reconstruction quality, as demonstrated in our experiments.\n3.2.2 VAE Training\nSC-VAE is trained in two stages. In the first stage, we use low-resolution data to quickly stabilize learning with direct O-Voxel reconstruction loss and KL loss. For geometry features, Mean-Squared-Error (MSE) and Binary Cross Entropy (BCE) losses are applied on dual vertex positions and dual face flags , respectively. Material attributes and the pruning mask are supervised by the L1 and BCE loss, receptively:\nIn the second stage, we add rendering-based perceptual supervision at high resolution to enhance geometric and material fidelity. We render mask, depth, and normal maps and supervise them with L1 loss, augmented with SSIM and LPIPS terms on normals. The material attributes are rendered and supervised by these perceptual losses as well. The loss can be written as:\nWe randomly place cameras around with a shallow near plane to slice through the surface, encouraging the model to capture both external and internal structures. More details of the losses are provided in the supplementary material.\nTo facilitate a sequential generation scheme for shape and material (in particular, to enable the application of material generation for given shapes), we learn decoupled latent spaces with two SC-VAEs: one models shape, while the other models material conditioned on the shape VAE‚Äôs subdivision structures during upsampling.\n3.3 Generative Modeling\nBuilt upon the learned latent space, we construct a scalable generative framework following the overall design of [xiang2025structured]. We adopt full DiT-based architectures [peebles2023scalable] trained with the flow matching paradigm [lipman2023flow] and extend the pipeline of [xiang2025structured] to fully leverage the power of our new latents.\nModel and Generation Pipeline.\nThe complete generation process unfolds in three stages with three models: 1) sparse structure generation, which predicts the occupancy layout of the sparse voxel grid; 2) geometry generation, which produces geometry latents within active voxels; and 3) material generation, which synthesizes material latents aligned to the geometry structure. The first two stages largely follow the strategy of [xiang2025structured], forming the geometric backbone of the asset. The novel material generation stage models PBR materials directly in the native 3D space. A sparse DiT predicts material latents conditioned jointly on the input image and the generated geometry latents. This design unifies geometric and material generation in the same native 3D latent domain and ensures their spatially alignment under arbitrary topology.\nArchitectural and Training Details.\nAll our DiT modules employ the AdaLN-single modulation [chen2024pixart] and Rotary Position Embedding (RoPE) [su2024roformer] for better scalability and cross-resolution generalization. Image conditioning features are extracted from DINOv3-L [simeoni2025dinov3]. Benefiting from the high spatial compression achieved by SC-VAE, our sparse DiTs discard the convolutional packing and skip connection designs in [xiang2025structured], resulting in a vanilla-style DiT which reduces complexity and improves efficiency.\nWe first train the sparse structure generation with conditioning images to learn coarse occupancy priors and establish the global sparse layout. In the subsequent stages, training proceeds in a progressive manner, gradually increasing both the spatial and visual resolution. The geometry and material generators are scaled from outputs ( latent resolution) to outputs ( latent resolution), with the conditioning image resolution correspondingly increased to . This progressive strategy allows the learned priors to transfer smoothly across resolutions, enabling efficient training of large-scale sparse DiTs while maintaining fidelity in both geometry and material.\n4 Experiments\nImplementation details.\nOur SC-VAE is trained using the Trellis-500K setup [xiang2025structured] after filtering out assets without PBR materials, resulting in a curated collection from Objaverse-XL [deitke2024objaverse], ABO [collins2022abo], and HSSD [khanna2023hssd]. We use an optimized Triton [tillet2019triton] implementation for Submanifold convolution [graham2017submanifold] to further improve training speed. SC-VAEs are trained on 16 H100 GPUs with a batch size of 128.\nThe generative model is trained on an extended collection of about 800K assets, augmented with TexVerse [zhang2025texverse] to enrich PBR diversity and realism. For image prompts, we render 16 views per asset in Blender [blender] with randomized FoVs and lighting. All models are trained using AdamW [loshchilov2017decoupled] (learning rate , weight decay ) with classifier-free guidance (drop rate ). Each DiT in our framework contains approximately 1.3B parameters (width: 1536, blocks: 30, heads: 12, MLP width: 8192), trained on 32 H100 GPUs with a batch size of 256.\nFor reconstruction evaluation, we use the Toys4K benchmark [stojanov2021using] together with a curated test set containing 90 assets featuring complex PBR materials and detailed shapes from recent Sketchfab assets [sketchfab2025] released within the past two years. Both test sets are unseen during training. For generation quality comparison and user studies, 100 AI-generated image prompts [fortin2025nanobanana] are used to ensure training‚Äìtesting disjointness. Split-sum renderer from nvdiffrec [munkberg2022extracting] is used for PBR asset rendering. All runtime statistics are reported on an NVIDIA A100 GPU. Additional details are provided in the supplementary material.\n4.1 3D Asset Reconstruction\nShape Reconstruction.\nWe compare with four representative baselines: Dora [chen2025dora] based on Shape2Vecset; Trellis [xiang2025structured], Direct3D-s2 [wu2025direct3d], and SparseFlex [he2025sparseflex] based on sparse voxel structure. For evaluation, we employ multiple metrics: (i) Mesh Distance (MD) calculated as Bidirectional Point-to-Mesh Distance with F1-score to measure reconstruction fidelity of meshes including internal structures; (ii) Chamfer Distance with F1-score computed on point clouds sampled from visible surfaces, focusing only on external shapes; and (iii) surface quality metrics using PSNR and LPIPS of rendered normal maps.\nAs shown in Table 1, our method consistently outperforms all baselines by a substantial margin across every metric, despite using only a modest number of tokens and requiring significantly less runtime. This demonstrates not only its superior geometric fidelity, finer detail preservation, and more accurate modeling of internal structures, but also the high efficiency of our approach.\nMaterial Reconstruction.\nAs no suitable baseline exists for encoding only material properties given shapes, we report metrics solely for our method. We assess the fidelity of directly rendered PBR attribute maps and shaded images using PSNR and LPIPS. Our method achieves 38.89 dB / 0.033 on PBR attributes and 38.69 dB / 0.026 on shaded images, demonstrating faithful material reproduction and consistent geometry‚Äìappearance alignment.\n4.2 Image to 3D Generation\nWe next evaluate the generative capabilities of our framework by producing 3D assets conditioned on input images. Fig. 5 presents representative results, illustrating both geometric fidelity and material realism.\nLeveraging the compact latent space compressing O-Voxels, our method generates assets that faithfully preserve fine-scale structures, sharp surface features, and internally complex or non-manifold shapes‚Äîranging from detailed gears, enclosed cockpit and open leaves and flowers. It also reproduces vivid, realistic PBR textures with physically consistent shading under novel lighting, including challenging translucent or reflective materials such as glass and metal. Together, These results demonstrate that our native 3D latent space enables the generation of tightly aligned, high-fidelity geometry and photorealistic appearance, even for topologically complex assets.\nQualitative comparison.\nWe compare our approach against state-of-the-art 3D generation systems, including Trellis [xiang2025structured], Hi3DGen [ye2025hi3dgen], Direct3D-s2 [wu2025direct3d], Step1X-3D [li2025step1x], and Hunyuan3D 2.1 [hunyuan3d2025hunyuan3d]. Samples from Fig. 6 demonstrate that our method achieves superior generation quality‚Äîdelivering accurate and detailed geometry, physically plausible materials, and faithful prompt alignment.\nQuantitative comparison.\nWe conduct quantitative evaluation using AI-generated images. Visual alignment is measured with the CLIP score [radford2021learning], while multimodal models ULIP-2 [xue2024ulip] and Uni3D [zhou2023uni3d] are employed to assess the geometric similarity. Table [ADDRESS_REMOVED] alignment score across all metrics, demonstrating clear superiority in visual and geometric consistency.\nWe also conduct a user study with about 40 participants to evaluate perceptual quality. Using 100 AI-generated image prompts, we generate assets with each method under identical conditions without curation and collect preference votes. Table 2 shows our method is favored by participants, highlighting its clear superiority in visual realism, richness of geometric detail, and alignment with input prompts.\n4.3 Shape-Conditioned Texture Generation\nThe third stage of our generation pipeline can be independently used as a 3D PBR texture synthesis model given 3D mesh and reference image. To evaluate its performance, we compare against representative baselines: multi-view PBR generation and fusion methods Hunyuan3D-Paint [hunyuan3d2025hunyuan3d], and UV-based TEXGen [yu2024texgen].\nAs shown in Fig. 7, multi-view approaches often suffer from inconsistencies both between the shape and synthesized images, as well as across different views, resulting in ghosting or blurred textures. UV-based methods suffer from ambiguous UV charts and seam artifacts, resulting in degraded visual quality. In contrast, our framework performs appearance reasoning natively in 3D. This enables sharper textures, consistent shape‚Äìmaterial alignment, and synthesis of textures for internal surfaces, crucial for complex assets with occluded or non-manifold geometry.\n4.[ADDRESS_REMOVED] ablation study to analyze the architecture design of SC-VAE. All ablations are conducted on the curated sketchfab assets at a resolution of .\nSparse residual autoencoding.\nTo evaluate the effect of the sparse residual autoencoding layer, we compare SC-VAE with a baseline using average pooling and nearest-neighbor upsampling. As shown in Table 3, the baseline exhibits severe quality degradation: MD increases by 69% and PSNR decreases 0.5dB at compression, worsening to 526% and 1.6dB at . In contrast, the sparse residual design maintains high fidelity across compression ratios, confirming its robustness under strong spatial bottlenecks.\nOptimized residual block.\nTo assess the effect of the optimized residual block, we compare SC-VAE with a baseline using standard residual blocks. Table 3 shows that this leads to a clear drop in reconstruction quality, MD increases by 16% and PSNR decreases by 0.6dB, while runtime remains unchanged. This confirms that the hybrid sparse convolution and point-wise MLP design better preserves fine-scale details without sacrificing efficiency.\n4.[ADDRESS_REMOVED]-time Compute and Resolution Scaling\nOur framework enables flexible test-time scaling of both compute and resolution, facilitated by the efficiency of our compact latent space. Notably, our generation process requires significantly fewer tokens than prior approaches, thanks to SC-VAE‚Äôs spatial compression. This efficiency permits a cascaded application of the second-stage generator, allowing for the synthesis of shapes at resolutions exceeding the training scale efficiently. Specifically, after predicting the O-Voxel structure from the gemetry latent, we can downsample it into a higher-res sparse structure layout (e.g., max-pooling a generated O-Voxel to a sparse structure resolution). Subsequently, we re-apply the geometry generation stage to obtain a higher-res shape (a sparse structure to a O-Voxel). See Fig. 8 (left) for an exmaple.\nSimilar stratgies can be applied when operating within the trained resolution to improve generation quality. Rather than directly using the sparse structure generated by the first stage, one can obtain an alternative by downsampling a generated O-Voxel (e.g., downsampling a O-Voxel to a sparse structure). This can correct local errors and yield cleaner layouts for the subsequent high-resolution generation (e.g., a O-Voxel). Such a cascaded inference mechanism offers a controllable trade-off between computational efficiency and generation quality. As demonstrated in Fig. 8 (right), cascaded inference yielded finer details and enhanced structural stability.\n[ADDRESS_REMOVED] structured 3D latent representation for 3D generation. A key innovation is O-Voxel, an omni-voxel representation that is capable of encoding complex geometry and materials. We also introduce a Sparse Compression VAE that achieves a high spatial compression rate on O-Voxel to construct the latent space. Our large flow-matching models deliver substantially superior generation quality compared to existing methods while maintaining high efficiency. We believe our approach offers a significant advancement in 3D generative modeling, enhancing both the efficiency and realism of 3D content creation and opening avenues for broader applications across various domains.\nAppendix A More Implementation Details\nA.1 O-Voxel Conversion Algorithms\nThis section provides a detailed breakdown of the bidirectional conversion algorithms between standard 3D assets (meshes and PBR textures) and our O-Voxel representation. We present the process in four parts: converting a mesh to the O-Voxel shape representation, reconstructing a mesh from it, converting PBR textures to the O-Voxel material representation, and reconstructing textures from it.\nA.1.1 Shape Conversion\nA.1.2 Material Conversion\nA.2 Network Architectures\nSparse Compression VAE\nThe Sparse Compression VAE (SC-VAE) is a fully sparse-convolutional network designed to compress the O-Voxel representation into a compact latent space with a 16 spatial downsampling ratio. We employ a conventional U-Shaped VAE architecture, optimized with ConvNeXt-style [liu2022convnet] residual blocks and Residual AutoEncoding layers [chen2024deep] for (down/up)sampling. The detailed architecture for the SC-VAE encoder is presented in Table4. The decoder is constructed symmetrically using inverted block numbers and dimensions. The complete model comprises 800M parameters (354M for the encoder and 474M for the decoder). This configuration achieves near-lossless reconstruction fidelity while maintaining high computational efficiency.\nGenerative Models.\nOur generation framework consists of three Transformer-based models. These models adopt a standard encoder-only architecture, intentionally omitting complex designs such as token packing or skip connections to maintain a clean and scalable architecture (shown in Table 5). Conditional inputs are integrated using mechanisms tailored to the nature of the data:\n-\n‚Ä¢\nTimestep Injection: We use the AdaLN-single [chen2024pixart] scheme for the conditioning on diffusion timestep. This method modulates the activations within the network, allowing the model to effectively incorporate temporal information while drastically reducing required parameters comparing to the AdaLN baseline.\n-\n‚Ä¢\nImage Prompt Conditioning: Image prompts are integrated via cross-attention layers. This enables the model to align its generative process with the semantic content of the visual conditioning signal.\n-\n‚Ä¢\nShape Conditioning: For the material generation stage, shape information is provided as a condition by concatenating it channel-wise with the input tensor. This direct approach ensures that geometric constraints are explicitly provided, help improve material-shape alignment.\nTo enhance generalization across different input resolutions, we incorporate Rotary Position Embedding (RoPE) [su2024roformer]. Furthermore, we employ a QK-Norm scheme [esser2024scaling] to stabilize the attention mechanism. This involves applying Root Mean Square Normalization (RMSNorm) [zhang2019root] to the query and key tensors before the attention operation, which improves training stability.\nA.3 Training Details\nSparse Compression VAE.\nAs described in the main paper, the SC-VAE is trained using a two-stage strategy. The first stage focuses on stabilizing the training process by employing a direct O-Voxel feature regression loss at a resolution of . In the second stage, resolution is increased to while rendering-based perceptual loss is introduced to enhance visual quality, such as geometric sharpness and high-frequency material details, and to facilitate the model‚Äôs adaptation to higher resolutions. This rendering loss is implemented as follows:\nwhere and are the rendering losses for shape and material, respectively. The term denotes a perceptual distance metric combining the L1 norm with SSIM and LPIPS losses. In these equations, variables with a hat () represent model predictions, while variables without are the ground-truth targets. Specifically, is the silhouette mask, is the depth map, is the normal map, is the base color, and corresponds to the metallic-roughness-alpha map.\nFor inputs with resolutions exceeding , we directly apply the pre-trained SC-VAE models without modification. The fully sparse-convolutional design of the SC-VAE is inherently resolution-agnostic, a property that allows the models to generalize effectively to larger spatial resolutions without requiring fine-tuning.\nGenerative Models.\nWe employ the rectified flow formulation [liu2023flow] to train our generative models. This framework defines a forward process based on linear interpolation, , which constructs a straight path from a data sample to a random noise sample , indexed by timestep .\nThe corresponding reverse process is governed by a time-dependent vector field, , which guides samples from the noise distribution back toward the data distribution. This vector field is approximated by a neural network, denoted , which is trained by minimizing the Conditional Flow Matching (CFM) objective [lipman2023flow]:\nFollowing the approach of [xiang2025structured], we adopt an altered timestep sampling strategy, utilizing a distribution for better generation quality.\nAppendix B FlexGEMM: Our High-Performance Sparse Convolution Backend\nThe sparse convolutional networks in our model are accelerated by a custom high-performance backend developed for this work. This backend was engineered to address the performance and platform-dependency limitations of existing libraries, which are often tightly coupled to the NVIDIA CUDA ecosystem. By implementing our kernels in Triton [tillet2019triton], a high-level GPU programming language, we created a single, cross-platform codebase that delivers near-optimal performance on both NVIDIA and AMD hardware.\nOur final, optimized implementation employs a Masked Implicit GEMM strategy [spconv2022]. This approach moves beyond naive explicit matrix multiplication by fusing the feature gathering (im2col) and the matrix multiplication (GEMM) steps into a single, highly-optimized kernel. This fusion minimizes global memory I/O by keeping intermediate data in fast on-chip memory. To further enhance performance in sparse contexts, we introduce a masking mechanism that intelligently skips computation on empty neighbor slots. This is achieved by first reordering active voxels using Gray code ordering, a technique that groups voxels with similar neighborhood patterns together. This grouping significantly improves the SIMD efficiency of the GPU, reducing warp divergence and wasted computation. Finally, we incorporate a Split-K technique, which increases parallelism by dividing the accumulation dimension of the matrix multiplication into independent parallel tasks. This is particularly effective in common scenarios, such as those with a large number of channels or a small number of active voxels. The combination of these techniques results in a highly efficient backend, yielding up to a 2 speedup over widely-used sparse convolution libraries in our benchmarking (see Fig. 9).\nAppendix C Data Preparation Details\nThe data preparation pipeline is largely based on the setup proposed in Trellis [xiang2025structured]. We begin by curating a collection of 3D assets but exclude the 3D-FUTURE [fu20213d] dataset due to its lack of Physically-Based Rendering (PBR) materials. The remaining assets form the basis for training our SC-VAEs.\nAll assets in this curated collection are used to extract geometric data for training the shape SC-VAE. For the material SC-VAE, a more specific filtering process is required. We employ a custom Blender [blender] script to parse materials from the raw assets and retain only those that utilize a standard metallic-roughness PBR workflow. This filtering process yields a subset of approximately 350,000 assets suitable for training the material VAE.\nTo train the generative models, we further augment the dataset with TexVerse [zhang2025texverse] to increase the diversity of high-quality PBR materials. As a final quality control step, we filter the assets based on an aesthetic score. For simplicity, we leverage the thumbnail images provided on the Sketchfab [sketchfab2025] platform to estimate this score. Objects with an estimated aesthetic score below 4.5 are excluded from the training set. Detailed statistics of the final dataset are provided in Table 6.\nTo generate the image prompts required for training our image-conditioned model, we render a diverse set of views for each 3D asset using Blender. We apply a series of augmentations during this rendering process to ensure the model is robust against common ambiguities found in real-world inputs. Key augmentations include:\n-\n‚Ä¢\nField of View (FoV): The camera‚Äôs Field of View (FoV) is randomly sampled between and . This augmentation is designed to make the model robust to variations in camera intrinsics, which are often unknown in practice.\n-\n‚Ä¢\nLighting Conditions: The lighting environment is randomized by ramdomly placing and adjusting the intensity of light sources. This improves the model‚Äôs ability to predict intrinsic PBR attributes accurately, disentangling them from environmental illumination.\nAppendix D More Experiment Details\nD.1 Evaluation Protocol\nIn the main paper, we present quantitative comparisons and ablation studies using a series of numerical metrics. We provide the detailed protocols for their calculation below.\nD.1.[ADDRESS_REMOVED] set.\nTo ensure a robust evaluation of reconstruction quality, we prepared two distinct test sets.\n-\n‚Ä¢\nToys4k-PBR. Our first test set is derived from the Toys4k dataset. For a rigorous metric, we filtered the raw assets to include only those containing all three standard PBR maps (base color, metallic, and roughness). This process resulted in a refined test set of 473 instances.\n-\n‚Ä¢\nSketchfab Featured. Recognizing that the assets in Toys4k are relatively simple, we curated a second, more challenging test set from high-quality, recent assets on Sketchfab. Specifically, we selected models from the ‚ÄúStaff Picks‚Äù category, which features professionally curated content. We then applied a filter to retain only assets that utilize the metallic-roughness PBR workflow and were uploaded within the last two years. This process yielded a high-quality test set comprising 90 instances, designed to evaluate performance on complex, professional-grade assets.\nGeometry Accuracy.\nTo assess the overall geometric fidelity, we use Mesh Distance and the corresponding F-score. Unlike Chamfer Distance, which is sensitive to point cloud density, Mesh Distance provides a more stable measure of the discrepancy between two triangle meshes. This makes it particularly suitable for evaluating reconstruction accuracy across all surfaces, including those that are fully enclosed. For this evaluation, we sample 1 million points from the surface of each mesh. For the F-score calculation, we use a distance threshold of .\nFor evaluating the accuracy of visible surfaces, we compute Chamfer Distance (CD) and the corresponding F-score. The evaluation is performed on point clouds generated by sampling the outer shell of the meshes. Specifically, we render depth maps for each mesh from 100 uniformly sampled camera views. These depth maps are then unprojected to create a dense 3D point cloud, from which we randomly sample 1 million points. For the F-score calculation, we use a distance threshold of .\nTo evaluate the quality of fine surface details, we compute PSNR and LPIPS on rendered normal maps. For this, we render images from four fixed camera positions for all assets. The camera is placed on a sphere of radius 10 with a fixed pitch angle of and a narrow Field of View (FoV) of . The four views correspond to yaw angles of and .\nPrior to any metric calculation, all ground-truth and predicted meshes are normalized to fit within a unit cube. The definitions for the geometric metrics are as follows:\n-\n‚Ä¢\nMesh Distance (MD). MD is calculated as the bidirectional point-to-mesh surface distance, averaged over a dense sampling of points from both meshes. Given two meshes and , with sampled points and , MD is defined as:\n-\n‚Ä¢\nChamfer Distance (CD). Given two point clouds, and , the Chamfer Distance is defined as:\n-\n‚Ä¢\nF-score. The F-score evaluates shape correspondence by combining precision and recall, calculated based on a distance threshold . Given a ground-truth shape and a predicted shape , we sample point sets from and from . Precision and Recall are then defined as:\nwhere is the indicator function and is the minimum Euclidean distance from a point to the shape . The F-score is the harmonic mean of these values:\nThe distance function is defined differently depending on the context, described below.\n-\n‚Äì\nFor CD F-score: The shapes and are treated as discrete point clouds. The distance is the Euclidean distance from point to the nearest point within the point cloud .\n-\n‚Äì\nFor MD F-score: The shapes and are treated as continuous triangle meshes. The distance is the Euclidean distance from point to the closest point on the surface of the mesh .\n-\n‚Äì\nAppearance Fidelity.\nTo assess the quality of the reconstructed materials, we evaluate both the raw PBR attribute maps and the final shaded images. For both the ground-truth and the reconstructed 3D assets, we render two sets of images using the nvdiffrec renderer [munkberg2022extracting]. This rendering is performed using the same fixed-camera setup as the normal map evaluation, capturing four distinct views. The PSNR and LPIPS metrics are then calculated by comparing the rendered outputs from the reconstructed asset against those from the ground truth. The final reported scores are the average values across these four views.\nD.1.[ADDRESS_REMOVED] Set.\nFor quantitative evaluation of our image-to-3D generation capabilities, we conduct experiments on a challenging test set of 100 image prompts generated by the NanoBanana text-to-image model [fortin2025nanobanana]. This dataset was specifically chosen for its diversity and complexity. It features prompts that describe objects with intricate geometries, varied and dramatic lighting conditions, and a wide range of realistic materials, including metal, leather, rust, and translucent substances such as glass.\nEvaluation Metrics.\nWe employ a suite of metrics targeting different aspects of the output. The visual and semantic alignment between the input image prompt and rendered images of the asset is measured using the CLIP score [radford2021learning]. To evaluate how well the 3D geometry and appearance properties match the image prompt, we use the multimodal foundation models ULIP-2 [xue2024ulip] and Uni3D [zhou2023uni3d]. Details of the metrics are listed below:\n-\n‚Ä¢\nCLIP Score. The CLIP score measures the semantic similarity between two images. In our evaluation, we render the generated 3D asset from 4 predefined viewpoints (same yaw, pitch setup as previous metrics). We then compute the average cosine similarity between the CLIP embedding of the input image prompt and the rendered images (or normal map). A higher CLIP score indicates a better semantic alignment between the conditional input and the appearance (or geometry) of the generated asset.\n-\n‚Ä¢\nULIP-2 and Uni3D Scores. ULIP-2 and Uni3D are models designed to understand and align 3D content with text/image . To prepare the input for these models, we first convert our generated mesh into a colored point cloud. Specifically, we uniformly sample 10,[ADDRESS_REMOVED] Point Sampling. The color for each point is determined by querying its corresponding RGB value from the asset‚Äôs base color map. This colored point cloud is then fed into the ULIP-2 and Uni3D models to compute a similarity score against the image prompt. These scores provide a quantitative measure of how well the generated asset align with the condition from a native 3D perspective.\nD.2 User Study\nWhile quantitative metrics provide objective measurements of fidelity, they often fail to capture the nuanced perceptual qualities that define a high-quality 3D asset, such as aesthetic appeal and fine-detail plausibility. To provide a comprehensive evaluation that aligns with human perception, we conducted a rigorous user study to compare our method against others.\nStudy Design and Interface.\nOur study was designed to assess two critical aspects of 3D asset quality: overall quality (combining geometry and appearance) and shape quality (isolating geometric fidelity). Participants were presented with a series of choice questions through a custom web interface, as shown in Figure 10.\nFor each question, participants were shown a reference image and a set of turntable video renderings of the 3D models generated by different methods. The interface provided interactive controls, allowing users to play, pause, scrub through the animation timeline, and zoom in to inspect details closely. This ensured that participants could perform a thorough comparison. The positions of the generated models were randomized for each question to prevent positional bias.\nThe study consisted of two distinct types of questions:\n-\n‚Ä¢\nOverall Quality Evaluation: In this task, participants were shown fully textured and rendered 3D models. They were instructed to select the model with the ‚Äúhighest overall quality that best matches the object in the input image.‚Äù The evaluation criteria emphasized a holistic assessment, including accurate shape, high-definition textures, realistic material properties (reflection and transmission), and consistent appearance with the reference.\n-\n‚Ä¢\nShape Quality Evaluation: To specifically evaluate geometric accuracy without the confounding influence of materials, this task presented the models rendered with only a normal map. Participants were asked to select the model with the best shape, focusing on criteria such as ‚Äúa well-defined shape, high-definition detail, sharp edges, and clear boundaries.‚Äù\nDetailed Analysis.\nWe recruited about 40 participants in the evaluation. For each question, the model selected by a participant was recorded as a ‚Äúwin‚Äù over the other options presented. We aggregated these results from all participants and computed a global preference rate for each method. This percentage provides a clear ranking of perceptual quality. Detailed statistics of the user study are shown in Table 7.\nAppendix E More Results\nE.1 3D Asset Reconstruction\nAdditional Reconstruction Results.\nWe present additional reconstruction results of our SC-VAE in Figure 11. The figure showcases the model‚Äôs ability to achieve high-fidelity reconstruction across a diverse range of 3D assets. Our method successfully captures hard-surface mechanical objects (a combat mech), intricate thin structures (a shopping cart, a ferris wheel), open surfaces (a plant), words (a fridge), and complex material properties (a crystal). Despite the highly compact nature of the learned latent space, the model faithfully recovers both complex geometries, visualized via normal maps, and detailed PBR materials, shown in the final renders.\nAdditional Qualitative Comparisons.\nFigure [ADDRESS_REMOVED] several state-of-the-art methods. The comparison includes normal map renderings, magnified insets to highlight fine details, and corresponding error maps that visualize the deviation from the ground truth. Across all examples, our method consistently demonstrates a superior ability to preserve high-frequency geometric details. For instance, our model more accurately reconstructs the intricate chainmail links of the helmet and the sharp ornamental patterns on the decorative vessel, where other methods often produce overly smooth or blurry surfaces. Notably, as demonstrated in the final column, our method also excels at recovering enclosed internal structures, which pose a significant challenge for many surface reconstruction techniques. This high fidelity is further corroborated by the error maps, which show visibly lower reconstruction errors for our method across all examples when compared to the baselines.\nE.2 Image to 3D Asset Generation\nAdditional Generation Results.\nWe present additional qualitative results from our image-to-3D generation method in Figure 13. The figure demonstrates the model‚Äôs versatility and robustness across a wide range of categories, including organic structures (a garden trellis with ivy), complex hard-surface machinery (a sci-fi pod, a bulldozer), and detailed characters (a dwarf blacksmith, a soldier). For each generated asset, we display the final physically-based render, the corresponding normal map to illustrate geometric detail, and a breakdown of the constituent PBR attribute maps along with relighting results. This comprehensive visualization highlights our method‚Äôs ability to jointly generate not only high-fidelity geometry but also plausible PBR materials that respond correctly to novel lighting conditions.\nAdditional Qualitative Comparisons.\nIn Figure 14, we provide further qualitative comparisons for the image-to-3D generation task against several recent state-of-the-art methods. A primary advantage of our method is its ability to generate high-quality PBR materials, a capability not present in several baselines such as Step1X-3D, TRELLIS, Direct3D-S2, and Hi3DGen. When comparing geometric fidelity via the normal maps, our results consistently exhibit sharper and more coherent details. For example, our method more accurately captures the fine mechanical joints of the crab and the face of the character, whereas competing methods often produce results that are overly smoothed or contain noticeable artifacts. Furthermore, for methods that do produce PBR materials (Hunyuan3D 2.1), our approach generates textures that are visually more plausible and better aligned with the input prompts.\nAppendix F Limitation Discussion and Future Work\nDespite the promising results, our method has several limitations that open avenues for future research.\nFirst, similar to other voxel-based methods, O-Voxel‚Äôs representation power is bounded by its spatial resolution. For detailed geometric features smaller than the voxel size, the Flexible Dual Grid formulation could produce aliasing artifacts. For example, when two parallel surfaces that are very close to each other intersect the same voxel, the QEF solver, by design, will place the dual vertex at a position that minimizes the error to both surfaces, often resulting in a vertex located between them rather than accurately on one. Similarly, the volumetric material attributes in such a voxel will be an average of the properties of both surfaces, leading to blurred appearance.\nSecond, we observe that the reconstructed and generated results sometimes contain small holes, though they can mostly be rectified with standard mesh post-processing techniques (e.g., hole filling). We attribute this issue to challenges in the sparse nature of our decoder, where ensuring a perfectly closed, manifold surface from the high-resolution sparse structure predicted by our decoder can be difficult. Improving the inherent stability of decoding process is an important area for improvement.\nFinally, our O-Voxel is currently focused on geometry and material and it does not explicitly encode higher-level structural or semantic information. A significant direction for future research is to extend our representation to incorporate part-level segmentation and a graph-based topological structure. Such a structured representation would unlock an even wider range of downstream applications."
  },
  {
    "article": "[*]Equal Contribution \\contribution[‚Ä†]Done during a visit to the University of California, Los Angeles. 1]University of Wisconsin‚ÄìMadison 2]University of California, Los Angeles 3]Michigan State University 4]University of Illinois Urbana‚ÄìChampaign 5]University of Adelaide 6]Salesforce AI Research 7]Microsoft 8]Adobe Research\nMMGR: Multi-Modal Generative Reasoning\nAbstract\nVideo foundation models have made striking progress in synthesizing visually compelling and temporally coherent content, yet their viability as world simulators hinges on whether they internalize the physical, logical, and spatial constraints that govern reality. Existing evaluation metrics‚Äîsuch as Fr√©chet Video Distance (FVD)‚Äîlargely emphasize perceptual fidelity, leaving critical reasoning failures undetected, including hallucinations that violate causal structure, physical laws, and global consistency. To address this gap, we propose a principled evaluation framework grounded in five core reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal reasoning. Building on this framework, we introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a comprehensive benchmark suite designed to assess generative reasoning across three complementary domains: Abstract Reasoning (e.g., ARC-AGI, Sudoku), Embodied Navigation (e.g., real-world 3D navigation and localization), and Physical Commonsense (e.g., sports and compositional physical interactions). MMGR evaluates both video and image generative models using fine-grained, domain-specific metrics that require holistic correctness rather than partial success. We benchmark state-of-the-art video generation models‚Äîincluding Veo-3, Sora-2, and Wan-2.2‚Äîalongside leading image generation models such as Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image, revealing a pronounced performance asymmetry across modalities. While current models achieve moderate success on Physical Commonsense tasks, they fail catastrophically on Abstract Reasoning (achieving accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Through detailed quantitative analysis and human evaluation, we identify key limitations in existing training paradigms: a severe imbalance favoring perceptual data over symbolic reasoning, architectural weaknesses in maintaining global state consistency, and optimization objectives that reward visual plausibility over causal correctness. By unifying abstract logic, embodied interaction, and intuitive physics under a single evaluation framework, MMGR provides a diagnostic lens into the reasoning deficits of modern generative models and outlines a concrete roadmap toward physically grounded, logically consistent, and reasoning-aware world models.\n[Repo][URL_REMOVED] \\metadata[Contact][EMAIL_REMOVED], [EMAIL_REMOVED], [EMAIL_REMOVED], [EMAIL_REMOVED]\n1 Introduction\nThe field of generative artificial intelligence has achieved a paradigm shift with the advent of large-scale text-to-video models (OpenAI, 2024b; Ho et al., 2022a; Singer et al., 2022; Blattmann et al., 2023). These systems can now synthesize photorealistic, diverse, and temporally rich scenes from simple natural-language prompts. This capacity to generate dynamic visual narratives promises to revolutionize filmmaking, scientific visualization, embodied simulation, and robotics. However, as generative models scale, evaluation remains a critical bottleneck. Conventional metrics‚Äîsuch as Fr√©chet Video Distance (FVD) (Unterthiner et al., 2018a), Inception Score (IS) (Salimans et al., 2016), and CLIP-based similarity (Radford et al., 2021)‚Äîprioritize perceptual fidelity: assessing whether a video looks realistic or aligns semantically with a caption. Yet, these metrics remain blind to world consistency and physical plausibility. Consequently, a model might render a visually stunning billiards shot where balls pass through one another, or a navigation sequence where an agent teleports through walls‚Äîhallucinations that satisfy texture-based metrics while violating fundamental laws of reality.\nWe argue that for video generation to evolve from mere image animation to genuine world modeling (Ha & Schmidhuber, 2018; LeCun, 2022), models must acquire foundational reasoning capabilities akin to human intuitive physics and cognition. Moving beyond superficial fidelity (Huang et al., 2024; Liu et al., 2024b), we propose a formal evaluation framework asking: Can a video model reason about the physical and logical constraints of the content it generates? Drawing on theories of core knowledge and cognitive development (Spelke & Kinzler, 2007; Lake et al., 2017), we posit that robust world simulation rests on five complementary pillars of reasoning:\n-\n1.\nPhysical Reasoning: Understanding intuitive physics, such as object permanence, gravity, collisions, and material properties. This capability aligns with theories of ‚Äúcore knowledge‚Äù in human cognition (Spelke & Kinzler, 2007; Baillargeon, 1987; Ullman et al., 2017; Piloto et al., 2022) and is a prerequisite for robust simulation and interaction (Battaglia et al., 2013; Yi et al., 2020; Wu et al., 2015; Bear et al., 2021; Riochet et al., 2021; Bakhtin et al., 2019; Allen et al., 2020).\n-\n2.\nLogical Reasoning: Manipulating abstract concepts, following rules, and performing logical operations (e.g., ‚Äúif A happens, then B follows‚Äù). This mirrors the symbolic processing required for System 2 reasoning (Kahneman, 2011; Marcus, 2001; Lake et al., 2017), enabling generalization beyond simple pattern matching (Chollet, 2019; Johnson et al., 2017; Xu et al., 2024; Barrett et al., 2018; Zhang et al., 2021; Webb et al., 2023).\n-\n3.\n3D Spatial Reasoning: Understanding 3D spatial relationships, navigating environments, and grasping topology. This involves building an internal ‚Äúcognitive map‚Äù of the world (Tolman, 1948; Gibson, 1979; Epstein et al., 1999) to ensure geometric consistency across camera viewpoints (Hudson & Manning, 2019; Zhong et al., 2020; Wu et al., 2022).\n-\n4.\n2D Spatial Reasoning: The accurate interpretation of visual layouts, shapes, and relative positions in the projected image plane. This relies on compositional image understanding (Biederman, 1987; Kosslyn, 1980) to correctly ground spatial prepositions in complex prompts (Johnson et al., 2017; Chollet, 2019; Hudson & Manning, 2019).\n-\n5.\nTemporal Reasoning: Modeling causality, the order of events, and long-range dependencies. This captures the human perceptual ability to segment continuous streams into discrete causal events (Michotte, 1963; Zacks & Tversky, 2001), which is essential for maintaining narrative coherence (Xiao et al., 2020; Piergiovanni et al., 2020; Zhou et al., 2022; Yi et al., 2020).\nWe explicitly distinguish 2D from 3D spatial reasoning because they rely on fundamentally different perceptual and computational mechanisms. While 2D reasoning operates on planar relationships‚Äîsuch as adjacency and relative positioning‚Äî3D reasoning necessitates depth estimation, viewpoint transformation, and occlusion handling. This separation mirrors human cognition, which processes flat representations (e.g., maps) differently than volumetric environments111Our benchmark targets this dichotomy to enable fine-grained diagnosis of model capabilities: abstract tasks like Sudoku and ARC-AGI probe 2D grid-based logic, whereas embodied navigation tasks demand coherent 3D spatial understanding..\nBuilding upon this five-ability framework, we introduce MMGR (Multi-Modal Generative Reasoning), a benchmark suite designed to systematically assess generative reasoning across diverse settings. MMGR encompasses three complementary domains‚Äîranging from abstract logic to embodied interaction‚Äîthat each necessitate the coordination of multiple reasoning abilities (see TableÀú1, FigureÀú1, FigureÀú2):\n-\n1.\nAbstract Reasoning: Evaluates Logical, 2D Spatial, and Temporal reasoning in non-photorealistic environments. Key tasks include synthetic Maze environments (Ivanitskiy et al., 2023), Sudoku (Seely et al., 2025) (applying rule-based 2D spatial logic), Math (visualizing symbolic solution paths), and ARC-AGI (Chollet, 2019; Xu et al., 2024) (performing spatial‚Äìlogical transformations).\n-\n2.\nEmbodied Navigation: Assesses the synthesis of Physical, 2D/3D Spatial, and Temporal reasoning from an agent-centric perspective. Models are tasked with generating successful trajectories within diverse settings, specifically complex real-world navigation scenes utilizing both egocentric and top-down views (Chang et al., 2017; Ramakrishnan et al., 2021b; Savva et al., 2019; Anderson et al., 2018b; Zhu et al., 2017; Ramakrishnan et al., 2021a; Chaplot et al., 2020a; Deitke et al., 2020).\n-\n3.\nPhysical Commonsense: Probes the understanding of intuitive physics (Battaglia et al., 2013; Yi et al., 2020; Bear et al., 2021; Bakhtin et al., 2019) and object dynamics. The scope extends from fundamental concepts (leveraging the VideoPhy (Bansal et al., 2024) ontology) to compositional sports scenarios that require modeling physically plausible interactions consistent with real-world constraints.\nBy evaluating state-of-the-art image generative models (i.e., Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image) and video generative models (i.e., Veo-3, Sora-2, Wan-2.2) (DeepMind, 2025b; OpenAI, 2024a; Qwen, 2024; DeepMind, 2025a; OpenAI, 2025; Wan, 2025) on MMGR, we deliver the first comprehensive characterization of their generative reasoning capabilities. Our results reveal a consistent trend: while models demonstrate encouraging performance on Physical Commonsense tasks (e.g., Sports: 60%), they struggle markedly with Abstract Reasoning challenges such as ARC-AGI (10%) (Chollet, 2019; Barrett et al., 2018) and with long-horizon, multi-step planning in the Embodied Navigation domain (e.g., S.L.A.G.: 3.64% holistic success (Ivanitskiy et al., 2023; Savva et al., 2019)).\nThese performance patterns illuminate several critical deficiencies in current training recipes, offering a guide for future model development:\n-\n‚Ä¢\nTraining Data Imbalance: While current video corpora abound in naturalistic physical interactions (e.g., sports, everyday dynamics)‚Äîexplaining strong Physical Commonsense performance‚Äîthey severely lack structured, symbolic reasoning data. This deficit leads to near-random performance on logic-heavy tasks like Sudoku () and ARC-AGI. Furthermore, the stark disparity between Final Correctness () and Intermediate Correctness () on Math tasks (e.g., GSM8K) suggests that models are merely memorizing answer patterns rather than learning genuine multi-step reasoning.\n-\n‚Ä¢\nArchitectural Limitations: The pronounced divergence between primary success metrics and holistic Overall scores (e.g., vs. on 3D Real-World Navigation) indicates that current architectures sacrifice global consistency for local plausibility. With Scene Consistency dropping to and Destination Integrity to , models struggle to enforce long-range spatial and temporal coherence. This highlights an urgent need for mechanisms‚Äîsuch as external memory, world-state representations, or structured latent spaces‚Äîto sustain context across extended generation horizons.\n-\n‚Ä¢\nOptimization Objective Gaps: Current objectives prioritize perceptual fidelity (via reconstruction loss or adversarial objectives) over reasoning correctness. Consequently, models optimize for appearance rather than logical validity‚Äîrendering visually convincing mazes or equations without actually solving them. Future work must integrate auxiliary objectives that reward rule adherence and causal consistency, potentially leveraging reinforcement learning from structured feedback or neuro-symbolic supervision.\nUltimately, MMGR provides a unified framework for diagnosing these limitations, charting a path toward video generation systems that are physically grounded, logically consistent, and truly reasoning-aware.\n2 Related Work\nVideo Generation Models.\nThe field of video generation has witnessed a paradigm shift, evolving from early GAN-based approaches (Vondrick et al., 2016; Tulyakov et al., 2018) to diffusion-based systems (Ho et al., 2022b; Singer et al., 2022) and large-scale transformer architectures (Yan et al., 2021; Hong et al., 2022). Contemporary state-of-the-art models, including Sora (OpenAI, 2024b), Veo (DeepMind, 2024), and Kling (Kuaishou, 2024), demonstrate exceptional capacity for synthesizing high-fidelity, photorealistic video with complex temporal dynamics. However, while these models excel at surface-level perceptual quality, the extent to which they internalize the underlying physical laws and logical constraints of the world remains an active area of inquiry.\nEvaluation of Generative Models.\nTraditional evaluation metrics have largely prioritized appearance quality over semantic consistency. Metrics such as FVD (Unterthiner et al., 2018b) and Inception Score (IS) (Salimans et al., 2016) capture perceptual fidelity, while more recent benchmarks (Huang et al., 2024; Liu et al., 2024a) focus on text‚Äìvideo alignment and basic temporal consistency. These tools, however, are insufficient for probing world modeling capabilities. They fall short of evaluating whether a model possesses the reasoning skills necessary to generate content that is not only visually plausible but also logically coherent and physically robust over long horizons.\nFrom Visual Understanding to Generative Reasoning.\nPrior benchmarks in video understanding (Girdhar & Ramanan, 2020; Goyal et al., 2017; Chollet, 2019) primarily assess discriminative models‚Äîtesting their ability to recognize interactions or perform symbolic reasoning on existing inputs. Similarly, embodied AI benchmarks (Savva et al., 2019) rely on rigid simulators to test perception. Our work shifts this paradigm from understanding to generation: requiring models to not merely interpret a video, but to manifest reasoning processes through synthesis. Recent studies have begun to explore this frontier. Wiedemer et al. (2025) identify emergent ‚ÄúChain-of-Frames‚Äù (CoF) reasoning in models like Veo-3, while Guo et al. (2025) utilize MME-CoF to expose failures in geometric consistency. Tong et al. (2025) further demonstrate competitive performance by Sora-2 across vision tasks. We build upon these insights by formalizing a five-ability reasoning framework. Unlike prior works that focus on specific failure modes or emergent properties, we provide a holistic assessment spanning Abstract Reasoning, Embodied Navigation, and Physical Commonsense, creating generative adaptations of rigorous tasks such as ARC-AGI to test the limits of current world models.\n3 Benchmark Overview\nOur three evaluation domains are grounded in the principle that world modeling necessitates both internal and external simulation capabilities (Ha & Schmidhuber, 2018; LeCun, 2022). Abstract Reasoning targets internal simulation‚Äîthe capacity to manipulate symbolic representations, adhere to logical rules, and execute mental transformations independent of physical reality. Conversely, Embodied Navigation and Physical Commonsense evaluate external simulation‚Äîthe ability to model interactions within the physical world. Specifically, Embodied Navigation tests a model‚Äôs capacity to simulate agent-environment dynamics for spatial planning, whereas Physical Commonsense assesses an understanding of the intuitive physics governing real-world objects.\nThese domains are strategically complementary; together, they exercise the five core reasoning abilities outlined in SectionÀú1. Abstract Reasoning prioritizes Logical and 2D Spatial reasoning, while the external domains integrate 3D Spatial, Temporal, and Physical reasoning through dynamic scenarios (see TableÀú1 for the complete mapping). This design ensures a comprehensive evaluation of the reasoning competencies essential for robust world modeling. FigureÀú2 shows several examples from MMGR.\n3.[ADDRESS_REMOVED] Reasoning\nMaze.\nDesigned to assess 2D spatial, logical, and temporal reasoning, this task requires models to navigate a valid path from a start cell (green) to a goal cell (red) while avoiding obstacles. We employ DFS and Wilson‚Äôs algorithms to generate 240 mazes across three difficulty levels‚ÄîEasy (‚Äì), Medium (‚Äì), and Hard (‚Äì)‚Äîusing four distinct start-goal configurations (e.g., corner-to-corner, random-to-random) (Ivanitskiy et al., 2023).\nSudoku.\nThis task evaluates constraint satisfaction and logical deduction (Seely et al., 2025). Models must complete grids such that every row, column, and subgrid contains unique digits. The dataset comprises 300 puzzles across two grid sizes ( and ) and three difficulty levels (Easy, Medium, and Hard), where complexity is modulated by the sparsity of initial clues.\nARC-AGI.\nTo evaluate abstract reasoning and few-shot rule induction, we utilize the ARC-AGI benchmark (Chollet, 2019). Models must infer latent transformation rules from input-output demonstration examples and apply them to unseen test cases. Our benchmark comprises 456 tasks from v1 (381 tasks) and v2 (75 tasks), classified by shape consistency (Match and Mismatch) and quantitative difficulty (Easy, Medium, and Hard).\nVisual Math.\nWe assess mathematical reasoning across diverse domains using five benchmarks: GSM8K (Cobbe et al., 2021) (grade school), MATH500 (Hendrycks et al., 2021) (high school), AIME 2024/2025 (Mathematical Association of America, 2024, 2025) (invitational competitions), and Omni-MATH (Gao et al., 2024) (Olympiad-level). The resulting dataset contains 327 problems requiring logical deduction and spatial understanding.\n3.2 Embodied Navigation\n3D Real-World Navigation.\nUtilizing cutaway ‚Äúdollhouse‚Äù renderings from Matterport3D (Chang et al., 2017) and HM3D (Ramakrishnan et al., 2021b), this task assesses multi-room and multi-level spatial reasoning. Models operate from a fixed third-person perspective to generate navigation trajectories, requiring them to interpret full 3D scene structures, including verticality and complex room connectivity.\nLast-Mile Navigation (Ego-centric).\nThis setting presents a 360‚àò panoramic environment via a proximal ‚Äúover-the-shoulder‚Äù view. Models must synthesize wide-field visual context to execute short-range navigation, necessitating the interpretation of agent-centric layouts to generate goal-directed trajectories.\nTop-down View Navigation.\nAdopting a fixed bird‚Äôs-eye perspective, this task targets global spatial planning and long-horizon prediction. Models generate trajectories on 2D overhead maps, emphasizing the ability to reason about global geometry and multi-step pathfinding.\nSimultaneous Localization and Generation (SLAG).\nSLAG integrates both 3D and top-down views, challenging models to jointly localize the agent while generating the surrounding scene layout. This requires maintaining geometric coherence and performing cross-view spatial alignment across distinct observation modalities.\nDataset Configuration.\nWe evaluate each of the four tasks on 120 samples. The dataset spans 24 configurations stratified by environmental complexity (single vs. multi-floor), view fidelity (quality 3‚Äì5), trajectory distance (short vs. long), and goal specification (visual marker vs. linguistic description).\n3.3 Physical Commonsense\nPhysical Concept.\nLeveraging the VideoPhy ontology (Bansal et al., 2024), this task assesses intuitive understanding of fundamental physical interactions. We evaluate three core categories: Solid-Solid (143 captions), Solid-Fluid (146 captions), and Fluid-Fluid (55 captions). The dataset spans broad physical domains including statics, dynamics, kinematics, and hydrodynamics. Additionally, we incorporate VideoPhy v2 to expand the evaluation scope with 600 supplementary captions covering 197 unique physical actions. From the larger source corpus, we randomly sample 25 examples and ensure diversity across these interaction categories and physical domains to create a balanced evaluation set.\nSports. This task evaluates compositional physical reasoning within complex scenarios characterized by the intersection of multiple physical laws. The source corpus encompasses diverse activities‚Äîspecifically Ballet (12), Skiing (13), Diving (12), and Swimming (13)‚Äîchallenging models to analyze phenomena such as momentum conservation, balance control, projectile motion, and fluid dynamics in goal-oriented contexts. To construct a balanced evaluation set, we randomly sampled 25 diverse examples from this larger collection.\n4 Experimental Setup\nTo systematically evaluate zero-shot reasoning capabilities (Wiedemer et al., 2025; Guo et al., 2025; Tong et al., 2025), we benchmark state-of-the-art generative models across the ten tasks outlined in SectionÀú3. Our analysis aims to quantify model performance and disentangle granular strengths and limitations within the five core reasoning dimensions.\n4.1 Data\nTableÀú2 provides a comprehensive statistical overview of the benchmark, which aggregates 1,853 testing samples across three domains and ten tasks. To facilitate fine-grained capability analysis, we employ rigorous difficulty stratifications and human verification. For Abstract Reasoning, complexity is modulated by grid dimensions (Maze, Sudoku), shape consistency (ARC-AGI), and mathematical scope (Math). Similarly, Embodied Navigation tasks are organized into [ADDRESS_REMOVED] configurations defined by environmental complexity, visual fidelity, trajectory distance, and goal specification.\n4.[ADDRESS_REMOVED] performance estimation and account for stochastic variability, we generate 5 samples per prompt for every model. We strictly adhere to the default API parameters (for closed-source models such as Sora-2, Veo-3, GPT-4o-image, Nano-banana, and Nano-banana Pro) and recommended configurations (for open-weights models such as Wan-2.2 and Qwen-image) to guarantee a fair, zero-shot comparison without task-specific fine-tuning.\n4.3 Evaluation Protocol\nVLM-based Evaluation.\nFollowing established video benchmarking protocols (Huang et al., 2024; Liu et al., 2024b; Wiedemer et al., 2025), we employ Gemini 2.5-Pro (Comanici et al., 2025) as a unified automated evaluator. The model assesses generation quality using task-specific rubrics that evaluate both the plausibility of the reasoning process and the correctness of the final result.\nMetrics Aggregation.\nWe begin by reporting diverse task-specific fine-grained metrics to dissect model performance across individual reasoning dimensions. Building on these components, we define a strict primary metric for each task that necessitates the simultaneous satisfaction of all sub-metrics (detailed in TableÀú2). We prioritize this holistic measure to address the disparity between partial success and complete correctness‚Äîa gap that typically inflates performance estimates by 1.2‚Äì4 when ignored.\n4.4 Models for Evaluation\nOur study evaluates a diverse selection of state-of-the-art multimodal generative models, spanning both video and image modalities. We include representative closed-source and open-weights models from major research laboratories, as detailed in TableÀú3.\n4.5 Human Evaluation\nTo establish ground-truth performance and validate the reliability of VLM-based automatic evaluation, we conducted systematic human annotation on generated outputs. This human evaluation serves as a critical complement to AutoEval, particularly for tasks requiring nuanced judgment of temporal consistency, spatial reasoning, and physical plausibility.\nAnnotation Interface.\nWe developed a web-based annotation platform (FigureÀú3) featuring full video playback controls including frame-by-frame navigation and adjustable playback speed. The interface displays the original task prompt alongside the generated video and provides structured evaluation forms tailored to each task type. Annotators assess multiple dimensions including task completion, process correctness, and failure modes with associated confidence ratings.\nEvaluation Protocol.\nWe recruited 6 annotators with bachelor education background. The training process included a 4-hour instruction session, a 50-video practice phase, and calibration meetings.\n5 Maze\n5.1 Task Description\nWe introduce the 2D Maze task to evaluate a model‚Äôs foundational reasoning capabilities. This task is a direct probe for 2D Spatial Reasoning, as the model must understand the topology of the maze (i.e., the white path vs. the black walls). Furthermore, it challenges Logical Reasoning by requiring the model to generate a valid plan (the solution path) from a start state (green square) to a goal state (red square). Finally, it tests Temporal Reasoning by requiring the model to execute this plan sequentially over time, moving the agent along the path without deviation.\n5.[ADDRESS_REMOVED]-Level Control\nTo ensure a diverse and controllable set of evaluation cases, we leverage the open-source Python library maze-dataset (Ivanitskiy et al., 2023) to programmatically generate mazes of varying structure and difficulty. We vary task parameters along three axes:\n-\n‚Ä¢\nGenerators (2 types): We employ two maze-generation algorithms‚ÄîDepth-First Search (DFS) and Wilson‚Äôs Algorithm‚Äîto produce topologically diverse maze layouts.\n-\n‚Ä¢\nGrid Sizes (10 levels): Maze difficulty is scaled across ten grid sizes, ranging from 33 to 1313.\n-\n‚Ä¢\nStart‚ÄìGoal Placement (4 schemes): Each maze is instantiated under four placement schemes‚Äîcorner-to-corner, corner-to-random, random-to-corner, and random-to-random‚Äîto prevent models from overfitting to a single trajectory pattern. A minimum start‚Äìgoal distance is enforced to rule out trivial solutions.\nFor each generator, we produce 120 mazes, comprising 40 Easy (33-55), 40 Medium (66-99), and [ADDRESS_REMOVED] (1010-1313) instances. Overall, this yields 240 mazes across the two generators. With our generation algorithm, each maze only has one solution path. FigureÀú4 presents representative examples across difficulty levels and start‚Äìend configurations, along with their corresponding solutions.\n5.3 Evaluation and Metrics\nWe evaluate generated videos and images using a Vision‚ÄìLanguage Model (VLM)‚Äìbased evaluator, Gemini-2.5-Pro (Comanici et al., 2025). The evaluator receives three inputs: (i) the model-generated video or image, (ii) the ground-truth maze solution image, and (iii) a structured evaluation prompt (with modality-specific variants for video vs. image). Given these inputs, the VLM judges whether the model solved the task and provides fine-grained feedback across multiple failure modes. The evaluation prompt asks:\n-\n‚Ä¢\nDoes the green square (start) reach and stop on the red square (end)?\n-\n‚Ä¢\nDoes the green square ever touch or cross a black wall?\n-\n‚Ä¢\nDoes the layout of the black walls or the position of the red square change at any time?\nUsing the VLM‚Äôs responses, we compute one primary metric and four fine-grained metrics:\n-\n‚Ä¢\nMaze Changed (Failure Mode): (i) Video: 1 if the maze layout changes in any frame, 0 if unchanged throughout. (ii) Image: 1 if the maze structure differs from the solution reference, 0 otherwise.\n-\n‚Ä¢\nCross Wall (Failure Mode): (i) Video: 1 if the green square crosses a black wall in any frame, 0 only if it stays on white paths at all times. (ii) Image: 1 if the blue path touches or crosses black walls, 0 if fully contained within the white corridors.\n-\n‚Ä¢\nAction Reflection: (i) Video: 1 if the video shows exploratory behavior (e.g., backtracking or trying multiple paths), [ADDRESS_REMOVED] route. (ii) Image: 1 if the rendered blue trajectory depicts multiple attempted paths, [ADDRESS_REMOVED] path.\n-\n‚Ä¢\nTarget Achievement: (i) Video: 1 if the green square reaches and stops on the red square in any frame, 0 otherwise. (ii) Image: 1 if a continuous, valid blue path connects start and end.\n-\n‚Ä¢\nOverall Score: 1 only if Maze Changed=0 AND Cross Wall=1 AND Task Completion=1; 0 otherwise.\n5.4 Case Study\nImage Generation.\nFigureÀú5(a) illustrates Nano-Banana‚Äôs behaviors, showing not only perfectly solved outputs but also common failure modes unique to image-based generation. These include wall-crossing artifacts in the final predicted trajectory, slight distortions of maze geometry, and ‚Äúaction-reflection‚Äù artifacts‚Äîcases where the rendered trajectory contains redundant loops or implausible detours even though no temporal dynamics are involved. These artifacts reflect the model‚Äôs uncertainty when inferring long-range paths from a single static instruction, resulting in inconsistent or physically implausible solution traces.\nVideo Generation.\nFigureÀú5(b) shows Veo-3‚Äôs successful generations. Frame-by-frame annotations reveal that Veo-3 can preserve the maze‚Äôs topology, keeps the green square strictly on valid white paths, and maintains consistent wall boundaries from start to finish. The model occasionally performs mild exploratory behaviors‚Äîsuch as brief backtracking or short directional adjustments between early frames‚Äîbefore ultimately converging on the correct route. Notably, these explorations remain structurally valid: the agent never crosses walls or distorts the environment, and the maze remains unchanged throughout the entire sequence. In contrast, FigureÀú5(c) highlights Veo-3‚Äôs failure cases. Here, the model sometimes introduces structural inconsistencies‚Äîremoving or altering wall segments, inserting open passages, or shifting the geometry of the target region. In other cases, the green square traverses invalid regions (e.g., sliding across black walls during transitions) or produces contradictory intermediate frames despite ending at the correct goal cell. The examples also show how subtle frame-to-frame drifts, such as disappearing wall pixels or morphing corridors, can accumulate into integrity violations not captured by coarse success metrics. Collectively, these case studies show that both models may successfully reach the red goal but still differ dramatically in path fidelity, wall adherence, and temporal consistency. The frame-level evidence‚Äîranging from clean, stable trajectories to structurally inconsistent or wall-violating behaviors‚Äîunderscores the necessity of a fine-grained maze-evaluation framework capable of capturing these nuanced, multimodal failure modes that simple goal-achievement metrics overlook.\nFine-grained Metrics Primary Metric Model Maze Changed Cross Wall Action Reflection Target Achievement Overall Generator: Depth-First Search Level: Easy (33‚Äì55) Video Models Veo-3 15.50% 25.50% 1.00% 60.50% 42.00% Sora-2 67.50% 7.50% 77.50% 12.50% 2.50% Wan-2.2 35.00% 79.17% 7.50% 10.00% 1.67% Image Models Nano-banana 5.00% 30.00% 15.50% 85.00% 15.50% Nano-banana Pro 5.00% 42.50% 10.00% 90.00% 17.50% GPT-4o-image 95.00% 5.00% 7.50% 72.50% 0.00% Qwen-image 5.00% 23.33% 15.00% 65.00% 11.67% Level: Medium (66‚Äì99) Video Models Veo-3 0.50% 25.63% 0.00% 50.75% 38.69% Sora-2 47.50% 12.50% 60.00% 10.00% 7.50% Wan-2.2 10.83% 90.83% 28.33% 23.33% 1.67% Image Models Nano-banana 0.63% 30.63% 11.88% 71.25% 4.38% Nano-banana Pro 0.00% 25.00% 30.00% 82.50% 2.50% GPT-4o-image 72.50% 10.00% 0.00% 82.50% 5.00% Qwen-image 2.50% 28.33% 12.50% 44.17% 0.00% Level: Hard (1010‚Äì1313) Video Models Veo-3 0.00% 18.50% 1.50% 60.00% 51.50% Sora-2 57.50% 7.50% 60.00% 25.00% 10.00% Wan-2.2 6.67% 80.83% 35.00% 20.00% 5.00% Image Models Nano-banana 0.00% 24.17% 18.33% 60.00% 0.83% Nano-banana Pro 0.00% 12.50% 20.00% 80.00% 5.00% GPT-4o-image 62.50% 5.00% 5.00% 77.50% 0.00% Qwen-image 7.50% 11.67% 11.67% 42.50% 1.67% Generator: Wilson‚Äôs Algorithm Level: Easy (33‚Äì55) Video Models Veo-3 3.50% 21.50% 2.50% 61.50% 46.50% Sora-2 67.50% 10.00% 40.00% 15.00% 5.00% Wan-2.2 32.50% 84.17% 8.33% 15.00% 1.67% Image Models Nano-banana 10.50% 47.50% 16.50% 81.00% 6.50% Nano-banana Pro 10.00% 32.50% 30.00% 85.00% 12.50% GPT-4o-image 82.50% 15.00% 0.00% 90.00% 2.50% Qwen-image 5.83% 12.50% 9.17% 67.50% 20.00% Level: Medium (66‚Äì99) Video Models Veo-3 1.25% 15.63% 1.25% 55.63% 47.50% Sora-2 62.50% 12.50% 47.50% 20.00% 10.00% Wan-2.2 14.17% 85.83% 15.83% 14.17% 1.67% Image Models Nano-banana 0.00% 36.88% 13.75% 70.00% 1.25% Nano-banana Pro 2.50% 17.50% 15.00% 80.00% 2.50% GPT-4o-image 75.00% 5.00% 5.00% 80.00% 5.00% Qwen-image 4.17% 25.00% 14.17% 47.50% 0.00% Level: Hard (1010‚Äì1313) Video Models Veo-3 1.25% 18.75% 0.63% 58.75% 45.63% Sora-2 45.00% 10.00% 52.50% 10.00% 2.50% Wan-2.2 10.00% 89.17% 28.33% 13.33% 0.83% Image Models Nano-banana 1.25% 27.50% 23.13% 60.63% 0.00% Nano-banana Pro 0.00% 22.50% 17.50% 75.00% 5.00% GPT-4o-image 60.00% 7.50% 5.00% 77.50% 7.50% Qwen-image 2.50% 27.50% 20.00% 38.33% 0.00%\n5.5 Evaluation Results\n5.5.1 VLM-Based Evaluation\nTableÀú4 isolates the reasoning capabilities of video and image generative models by correlating path planning strategies with environmental consistency. Veo-[ADDRESS_REMOVED] Execution‚Äù reasoning style: its near-zero Action Reflection (0.00%‚Äì3.00%) confirms it generates a single, non-exploratory route without backtracking. While this allows for high Target Achievement (up to 62.00%), the model‚Äôs reliance on one-shot generation comes at the cost of physical precision, evidenced by significant Cross Wall rates (18.00%‚Äì25.00%). This suggests Veo-3 solves mazes via pattern-matching (predicting the solution trajectory directly) rather than active search, often fudging‚Äù collisions to maintain momentum. In contrast, Sora-2 exhibits ‚ÄúPerformative Reasoning‚Äù: it achieves high Action Reflection scores (40.00%‚Äì78.00%) by generating visible exploratory behaviors like backtracking and trying multiple paths. However, this exploration is fundamentally disconnected from valid state maintenance‚Äîthe model frequently hallucinates new maze structures (Maze Changed 45.00%‚Äì68.00%) while searching, and rarely achieves the target (10.00%‚Äì25.00%). Wan-2.2 and the Nano-banana family demonstrate a failure of physical constraint satisfaction: their high Cross Wall rates (up to 91.00%) indicate they treat walls as visual suggestions rather than impermeable boundaries, allowing them to traverse the maze without solving the topological puzzle. Finally, GPT-4o-image bypasses the reasoning task entirely, altering the problem definition (Maze Changed up to 95.00%) to fabricate a successful outcome.\n5.5.2 Human Evaluation\nTo establish ground-truth performance estimates and validate the reliability of our VLM-based evaluator, we conducted a human evaluation on a subset of Veo-3‚Äôs generated videos. TableÀú5 presents a side-by-side comparison of Auto-Eval versus Human-Eval across all maze generators and difficulty levels. The results highlight a substantial divergence between automated and human assessments, with human evaluators consistently uncovering failure modes that the VLM overlooks.\nAuto-Eval Human-Eval Setting MC CW AR TA Overall MC CW AR TA Overall Generator: Depth-First Search Easy (33‚Äì55) 15.50% 25.50% 1.00% 60.50% 42.00% 10.00% 80.00% 40.00% 70.00% 10.00% Medium (66‚Äì99) 0.50% 25.63% 0.00% 50.75% 38.69% 20.00% 90.00% 90.00% 20.00% 0.00% Hard (1010‚Äì1313) 0.00% 18.50% 1.50% 60.00% 51.50% 20.00% 100.00% 40.00% 70.00% 0.00% Generator: Wilson‚Äôs Algorithm Easy (33‚Äì55) 3.50% 21.50% 2.50% 61.50% 46.50% 40.00% 70.00% 60.00% 40.00% 20.00% Medium (66‚Äì99) 1.25% 15.63% 1.25% 55.63% 47.50% 20.00% 100.00% 60.00% 40.00% 0.00% Hard (1010‚Äì1313) 1.25% 18.75% 0.63% 58.75% 45.63% 40.00% 100.00% 70.00% 40.00% 0.00%\nNote: MC = Maze Changed, CW = Cross Wall, AR = Action Reflection, TA = Target Achievement.\nThe most critical discrepancy appears in the Cross Wall (CW) metric. While Auto-Eval reports moderate wall-crossing rates (16.00%‚Äì26.00%), Human-Eval detects significantly higher violation rates (70.00%‚Äì100.00%)‚Äîa 3‚Äì5 increase. Notably, human evaluators identified wall-crossings in 100.00% of samples for DFS Hard, Wilson‚Äôs Medium, and Wilson‚Äôs Hard mazes, whereas Auto-Eval reported rates of only 16.00%‚Äì19.00%. This confirms that the VLM evaluator systematically fails to capture transient wall-crossing events, likely due to insufficient temporal resolution or frame-dropping when processing fast-moving agents. These Cross Wall failures propagate directly to the Overall Score, which mandates zero wall-crossings for success. Consequently, while Auto-Eval suggests moderate competence (39.00%‚Äì52.00%), Human-Eval reveals that true performance is near zero (0.00%‚Äì20.00%). In four of the six configurations, Veo-3 achieved zero successful completions according to human review. This indicates that the automated metrics overestimate the model‚Äôs maze-solving success by a factor of 2‚Äì5.\nConversely, the Action Reflection (AR) metric exhibits an inverse pattern. Human evaluators assigned substantially higher scores (40.00%‚Äì90.00%) than the VLM (0.00%‚Äì3.00%). This suggests that humans are sensitive to subtle exploratory behaviors‚Äîsuch as hesitations, micro-adjustments, or partial backtracking‚Äîthat the VLM fails to classify as meaningful reflection. While these behaviors do not constitute full multi-path exploration, they indicate that the model engages in implicit trajectory reasoning that automated metrics miss.\nFinally, the Maze Changed (MC) metric shows a narrower gap, with Human-Eval reporting slightly higher rates (10.00%‚Äì40.00%) than Auto-Eval (0.00%‚Äì16.00%). This suggests that structural changes to maze geometry are more visually salient to VLMs than the fleeting motion artifacts involved in wall-crossings.\nIn summary, these findings expose a critical limitation in VLM-based evaluation for temporally dense tasks: automated evaluators systematically overestimate success by missing transient but fatal errors. Human calibration reveals that while Veo-3 demonstrates emerging spatial understanding, its operational reliability is substantially lower than automated metrics imply.\n5.5.3 Limitations and Insights from VLM-Based Evaluation\nOur evaluation pipeline relies on a VLM-based automated evaluator (AutoEval), instantiated as Gemini-2.5-Pro. While AutoEval delivers consistent ratings for Easy and Medium tasks, its reliability degrades significantly at the Hard difficulty level. In these complex scenarios, high-velocity agent movements challenge the VLM‚Äôs temporal resolution. The evaluator frequently ‚Äúdrops frames,‚Äù missing transient but critical violations‚Äîspecifically Cross Wall and Maze Changed errors‚Äîthat occur within single frames. Consequently, AutoEval systematically overestimates performance on harder tasks. To calibrate this blind spot, Human Evaluation (HumanEval) serves as an essential baseline; we argue that presenting HumanEval scores alongside AutoEval is strictly necessary for Hard settings to provide a truthful performance picture.\nBeyond validation reliability, our analysis highlights the critical concept of evaluability. Models that explicitly visualize reasoning‚Äîsuch as Nano-Banana, which renders its intended trajectory as a static blue path‚Äîfundamentally transform the evaluation task. This ‚Äúplan visualization‚Äù converts a challenging temporal verification problem (tracking frame-by-frame collisions) into a straightforward spatial comparison (checking static path validity), thereby enhancing transparency and trustworthiness.\nHowever, this benefit relies on structural faithfulness. When prompted to generate similar trajectory visualizations, Veo-3 frequently produced erratic, hallucinated blue curves covering area the agent never visited. Far from aiding interpretation, these hallucinations obscured the model‚Äôs actual reasoning and actively confused the Gemini-2.5-Pro evaluator, further degrading AutoEval accuracy. This underscores a key insight: while explicit visual reasoning can improve evaluability, it is only beneficial when the visualized artifacts remain grounded in the physical reality of the environment.\n6 Sudoku\nWe introduce the Sudoku task to evaluate a model‚Äôs core abilities in Constraint Satisfaction and Logical Reasoning. This task directly probes a model‚Äôs capacity for logical inference-its ability to derive valid conclusions under a structured set of logical rules. In Sudoku, the model must internalize the underlying constraints that govern valid solutions, ensuring that each symbol appears exactly once in every row, column, and subgrid. Moreover, the task explicitly tests deductive reasoning: the step-by-step application of these constraints to infer the only logically consistent values for each empty cell. Successful completion thus requires the model to integrate global structural understanding with local deductive consistency, producing a fully valid and complete grid (Seely et al., 2025).\n6.[ADDRESS_REMOVED]-Level Control\nTo build a diverse and systematically controlled evaluation set, we use an open-source Sudoku generation library (Seely et al., 2025) to create puzzles spanning a range of structural and reasoning complexities. We vary task parameters along two main axes:\n-\n‚Ä¢\nGrid Size (2 levels): We generate puzzles in two standard configurations‚Äî44 (digits 1‚Äì4) and 99 (digits 1‚Äì9). The larger grid substantially increases combinatorial difficulty and requires deeper multi-step logical inference.\n-\n‚Ä¢\nPuzzle Difficulty (3 levels): For each grid size, we control difficulty by varying the number of initial clues (pre-filled digits). The three levels‚ÄîEasy (many clues), Medium, and Hard (few clues while ensuring a unique solution)‚Äîmodulate the search space and constraint-satisfaction complexity.\nFor every difficulty level (Easy, Medium, Hard), we generate 100 puzzles: 50 for the 44 grid and 50 for the 99 grid. This results in a balanced evaluation set of 300 Sudoku puzzles spanning a controlled spectrum of structural sizes and reasoning challenges. FigureÀú6 provides representative puzzles and solutions for each difficulty level and grid size, with each puzzle displayed directly above its corresponding solution.\n6.2 Evaluation and Metrics\nWe evaluate both video and image outputs using a Vision-Language Model (VLM)‚Äìbased evaluator, Gemini-2.5-Pro (Comanici et al., 2025). The evaluator receives three inputs: (i) the generated video or image, (ii) the ground-truth solved Sudoku grid, and (iii) a structured evaluation prompt (with modality-specific variants). Using these inputs, the VLM assesses whether the model produces a valid Sudoku solution and identifies failure modes related to rule consistency, clue preservation, and reasoning behavior.\nFrom the evaluator‚Äôs structured outputs, we derive one primary success metric and four fine-grained metrics:\n-\n‚Ä¢\nClues Changed (Failure Mode): (i) Video: 1 if any original digits (‚Äúclues‚Äù) are modified, removed, or displaced in any frame; [ADDRESS_REMOVED] across the entire sequence. (ii) Image: 1 if any given clue differs from the original puzzle image; 0 otherwise.\n-\n‚Ä¢\nConstraints Violation (Failure Mode): The fraction of Sudoku constraints correctly satisfied in the final output (rows, columns, and subgrids). A value of 1 indicates full rule compliance; 0 indicates violation.\n-\n‚Ä¢\nCompletion Accuracy: The fraction of correctly filled originally empty cells, computed by comparing the model‚Äôs final output against the ground-truth solution.\n-\n‚Ä¢\nAction Reflection: (i) Video: 1 if the sequence shows interpretable step-by-step reasoning (e.g., gradual cell updates without overwriting earlier entries); 0 if digits appear simultaneously or in an erratic order. (ii) Image: Not applicable.\n-\n‚Ä¢\nOverall Score: 1 only if Clues Changed=0 AND Constraints Violation=0 AND Completion Accuracy=1; 0 otherwise.\n6.3 Case Study\nFigureÀú7 and FigureÀú8 illustrate several solution trajectories generated by Nano-Banana (image generative model) and Veo-3 (video generative model) for the same 44 easy Sudoku puzzle. In the successful case, Nano-Banana solves the puzzle perfectly‚Äîpreserving all initial clues, following valid reasoning steps, and reaching the correct final configuration without any constraint violations. However, when the model fails, the error patterns align closely with our diagnostic evaluation metrics: missing digits, repeated numbers within a row or box, constraint violations across frames, and changes to the original clues. These failures often arise despite our detailed, instruction-focused generation prompts, suggesting that Nano-Banana still lacks the deeper abstract reasoning capacity required for consistent multi-step logical problem solving. Although Veo-[ADDRESS_REMOVED] solutions, its video outputs reveal a set of systematic and interpretable reasoning behaviors: (1) Veo-3 demonstrates a strong positional bias: it almost always initiates its reasoning from the top-left box of the grid. Humans, in contrast, typically adapt their solving order based on puzzle structure, difficulty, or available constraints‚Äîsuggesting that Veo-3 relies more on a fixed procedural heuristic than on dynamic reasoning. (2) Veo-3 frequently engages in self-reflective edits during the generation process. These edits occasionally help the model avoid violations (e.g., at second 3, where it changes the (1,1) cell from ‚Äú4‚Äù to ‚Äú2‚Äù to resolve a conflict), but they can also be detrimental. At second 2, for example, Veo-[ADDRESS_REMOVED] digit, changing the (0,0) entry from ‚Äú2‚Äù to ‚Äú3,‚Äù instantly invalidating the entire solution. This behavior illustrates a broader challenge in generative reasoning models: self-reflection without grounded logical consistency can introduce more instability than benefit. (3) Veo-3 exhibits a characteristic temporal drift pattern. Its early frames often remain stable and respect the given clues, but as generation progresses, the model increasingly modifies clues, introduces inconsistencies, or oscillates between alternative partial solutions. This suggests that the model lacks a persistent internal representation of constraints, leading to reasoning degradation over time, even when the prompt explicitly prohibits altering clues.\nOverall, these case studies highlight the gap between current generative models and true abstract reasoning ability. Both Nano-Banana and Veo-3 demonstrate surface-level competence‚Äîfilling cells, correcting mistakes, or imitating stepwise processes‚Äîbut neither maintains a robust, constraint-aware reasoning trajectory throughout the entire solution. These observations reinforce the importance of MMGR‚Äôs diagnostic metrics: beyond assessing whether the final answer is correct, evaluating how the model reasons is essential for understanding the strengths and limitations of generative reasoning models.\nFine-grained Metrics Primary Metric Model Clues Changed Constraints Violation Completion Accuracy Action Reflection Overall Grid Size: 44 Level: Easy Video Models Veo-3 27.60% 42.00% 37.21% 92.00% 11.38% Sora-2 100.00% 44.22% 25.78% 32.65% 0.00% Wan2.2 100.00% 35.67% 17.22% 4.67% 2.00% Image Models Nano-banana 18.40% 17.20% 67.48% N/A 66.25% Nano-banana Pro 35.25% 1.33% 93.38% N/A 56.12% GPT-4o-image 23.83% 19.65% 73.14% N/A 61.22% Qwen-image 83.50% 61.50% 46.27% N/A 6.67% Level: Medium Video Models Veo-3 24.40% 43.53% 37.61% 95.60% 9.70% Sora-2 100.00% 45.24% 21.71% 61.90% 0.00% Wan2.2 100.00% 36.67% 14.36% 6.00% 1.33% Image Models Nano-banana 0.63% 21.30% 52.14% N/A 51.78% Nano-banana Pro 34.81% 0.50% 91.86% N/A 56.75% GPT-4o-image 31.68% 24.88% 56.39% N/A 46.28% Qwen-image 83.50% 61.50% 46.27% N/A 6.67% Level: Hard Video Models Veo-3 30.12% 44.88% 30.50% 95.18% 8.71% Sora-2 100.00% 41.85% 20.03% 60.87% 0.00% Wan2.2 99.33% 38.17% 14.79% 6.00% 0.00% Image Models Nano-banana 24.40% 25.80% 46.21% N/A 42.45% Nano-banana Pro 40.53% 0.83% 89.50% N/A 57.38% GPT-4o-image 36.40% 24.37% 49.00% N/A 39.08% Qwen-image 83.50% 61.50% 46.27% N/A 6.67% Grid Size: 99 Level: Easy Video Models Veo-3 31.60% 39.60% 15.47% 70.00% 3.18% Sora-2 95.74% 54.85% 8.47% 34.04% 4.26% Wan2.2 87.00% 44.59% 18.03% 8.00% 0.00% Image Models Nano-banana 10.50% 31.66% 19.87% N/A 28.80% Nano-banana Pro 19.09% 33.67% 31.52% N/A 39.28% GPT-4o-image 70.72% 59.67% 19.83% N/A 12.44% Qwen-image 28.33% 7.88% 73.12% N/A 18.08% Level: Medium Video Models Veo-3 34.00% 40.59% 13.66% 72.00% 2.77% Sora-2 100.00% 54.96% 9.43% 30.00% 0.00% Wan2.2 85.33% 46.44% 16.13% 2.00% 0.00% Image Models Nano-banana 2.00% 35.54% 36.88% N/A 24.15% Nano-banana Pro 14.91% 28.40% 26.19% N/A 33.99% GPT-4o-image 71.37% 58.53% 17.92% N/A 11.43% Qwen-image 28.33% 7.88% 73.12% N/A 18.08% Level: Hard Video Models Veo-3 31.20% 43.01% 13.45% 70.40% 2.57% Sora-2 92.86% 59.79% 8.65% 35.71% 7.14% Wan2.2 91.00% 48.07% 16.86% 3.00% 1.00% Image Models Nano-banana 2.40% 38.16% 17.03% N/A 19.94% Nano-banana Pro 14.75% 41.36% 22.52% N/A 30.86% GPT-4o-image 71.58% 57.68% 15.74% N/A 10.00% Qwen-image 28.33% 7.88% 73.12% N/A 18.08%\n6.4 Evaluation Results\n6.4.1 VLM-Based Evaluation\nTableÀú6 reveals a clear and persistent performance divide between image and video generative models across grid sizes and difficulty levels. Image models (Nano-Banana, Nano-Banana Pro, GPT-4o-image, Qwen-image) consistently achieve higher completion accuracy and overall scores, reflecting stronger symbolic consistency when directly producing completed Sudoku grids. In contrast, video models (Veo-3, Sora-2, Wan-2.2) frequently generate visually coherent step-by-step edits‚Äîhigh action reflection‚Äîyet still suffer from widespread clue changes, constraint violations, and low end-to-end correctness.\n44 Performance.\nEven in the simplest setting, video models struggle. On Easy puzzles, the best video performer, Veo-3, reaches an overall accuracy of only 11.38%, while Sora-2 and Wan-2.2 effectively fail (0% and 2%). Image models perform dramatically better: Nano-Banana achieves 66.25%, GPT-4o-image 61.22%, and Nano-Banana Pro 56.12%. This gap widens at Medium and Hard levels, where video models rarely produce valid solutions (overall 9.70%). Although Veo-3 maintains high action reflection (92%‚Äì96%), its completion accuracy remains low (30%‚Äì38%), and clue-change rates (24%‚Äì30%) indicate persistent temporal drift.\n99 Performance.\nOn full-scale Sudoku, where global structural consistency is essential, video models break down almost entirely. Overall scores remain below 5% for most models. Sora-2‚Äôs occasional peak at 7.14% on Hard puzzles stems from anomalously low clue-change instances rather than genuine logical success. High constraint-violation rates (39%‚Äì60%) and low completion accuracy (8%‚Äì18%) further highlight compounding temporal errors that worsen with puzzle size.\nImage models, by contrast, remain substantially more stable. Nano-Banana Pro delivers the strongest overall results across difficulties‚Äî39.28%, 33.99%, and 30.86%‚Äîwhile standard Nano-Banana remains competitive (19.94%‚Äì28.80%). Qwen-image is a mixed case: it frequently alters clues on 44 puzzles but achieves surprisingly strong completion accuracy on 99 (73.12%). However, its solutions often contain subtle structural inconsistencies, limiting overall correctness under our strict validator.\nAcross all conditions, video models exhibit a fundamental weakness: despite producing locally plausible temporal edits, they lack the global, long-horizon reasoning required to satisfy Sudoku‚Äôs symbolic constraints. Their sequential generation process introduces temporal instability‚Äîdigit drift, unintended clue overwrites, and cumulative constraint violations‚Äîthat image models avoid by generating a single, static grid. The persistent performance gap across both grid sizes and all difficulty levels underscores a deeper architectural mismatch between current video-generation paradigms and tasks requiring coherent, symbolic logical reasoning.\n6.4.2 Human Evaluation\nTo establish ground-truth performance estimates and validate the reliability of our VLM-based evaluator, we conducted a human evaluation on a subset of Veo-3‚Äôs generated videos. TableÀú7 presents a side-by-side comparison of Auto-Eval versus Human-Eval across all grid sizes and difficulty levels.\nFine-grained Metrics Primary Metric Grid Eval Type Clues Changed Constraints Violation Completion Accuracy Action Reflection Overall Level: Easy 44 Auto-Eval 27.60% 42.00% 37.21% 92.00% 11.38% Human-Eval 10.00% 10.00% 17.50% 90.00% 0.00% 99 Auto-Eval 31.60% 39.60% 15.47% 70.00% 3.18% Human-Eval 30.00% 7.50% 0.00% 100.00% 0.00% Level: Medium 44 Auto-Eval 24.40% 43.53% 37.61% 95.60% 9.70% Human-Eval 30.00% 5.00% 12.50% 100.00% 0.00% 99 Auto-Eval 34.00% 40.59% 13.66% 72.00% 2.77% Human-Eval 20.00% 0.00% 2.50% 80.00% 0.00% Level: Hard 44 Auto-Eval 30.12% 44.88% 30.50% 95.18% 8.71% Human-Eval 50.00% 10.00% 17.50% 100.00% 0.00% 99 Auto-Eval 31.20% 43.01% 13.45% 70.40% 2.57% Human-Eval 10.00% 20.00% 0.00% 100.00% 0.00%\nClues Changed.\nThe clues-changed metric exhibits a mixed pattern across conditions. For 44 Easy puzzles, Auto-Eval reports a higher rate (28%) than Human-Eval (10%), yet this reverses on Hard puzzles where Human-Eval detects more clue modifications (50% vs. 30%). On 99 grids, the discrepancy is smaller but inconsistent: Human-Eval reports lower clue-change rates on Medium (20% vs. 34%) and Hard (10% vs. 31%) levels, but comparable rates on Easy (30% vs. 32%). These divergent patterns suggest that VLM and human annotators apply different detection criteria‚Äîthe VLM may flag subtle visual perturbations as clue changes, while humans focus on semantically meaningful digit alterations.\nConstraints Violation.\nThe most striking discrepancy emerges in constraint violation detection. Human evaluators consistently assign substantially lower constraint violation scores across all conditions: 10% vs. 42% on 44 Easy, 5% vs. 44% on 44 Medium, 10% vs. 45% on [ADDRESS_REMOVED]. For 99 puzzles, the gap widens further‚ÄîHuman-Eval yields 8% (Easy), 0% (Medium), and 20% (Hard), compared to Auto-Eval‚Äôs 40%, 41%, and 43% respectively. Notably, human annotators report zero constraint violations on 99 Medium puzzles, whereas Auto-Eval detects violations in over 40% of outputs. This suggests the VLM-based evaluator may over-detect violations by misinterpreting visual artifacts, blurry digits, or frame inconsistencies as rule breaches.\nCompletion Accuracy.\nCompletion accuracy is substantially lower under human evaluation across all conditions. On 44 grids, Auto-Eval reports accuracy of 37% (Easy), 38% (Medium), and 31% (Hard), while Human-Eval yields only 18%, 13%, and 18% respectively‚Äîroughly half the VLM‚Äôs estimates. The disparity is even more pronounced on 99 puzzles: Auto-Eval reports 15% (Easy), 14% (Medium), and 13% (Hard), but Human-Eval drops to 0% on both Easy and Hard levels, with only 3% on Medium. This near-zero completion accuracy under human judgment indicates that human annotators apply stricter criteria for recognizing correctly filled digits, likely requiring clearer visual rendering and unambiguous digit shapes that Veo-3 fails to consistently produce at the 99 scale.\nAction Reflection.\nIn contrast to other metrics, action reflection scores show reasonable agreement between evaluation methods. Both evaluators recognize that Veo-3 exhibits step-by-step reasoning behavior, with Auto-Eval reporting 92%‚Äì96% on 44 and 70%‚Äì72% on 99, while Human-Eval assigns 90%‚Äì100% on 44 and 80%‚Äì100% on 99. Human annotators even rate action reflection slightly higher on several conditions, reaching perfect scores (100%) on 44 Medium, [ADDRESS_REMOVED], 99 Easy, and [ADDRESS_REMOVED]. This consensus confirms that Veo-3 does produce visually interpretable sequential edits‚Äîthe model‚Äôs failure lies not in lacking a reasoning process, but in lacking reasoning correctness.\nOverall Score.\nMost strikingly, no Veo-3 output achieved an overall success score under human evaluation‚Äîall conditions yield 0%, compared to Auto-Eval‚Äôs modest but non-zero scores ranging from 3% (99 Easy) to 11% (44 Easy). This complete failure under human judgment, despite the model producing visually coherent step-by-step animations, underscores that even outputs deemed partially successful by the VLM fail to meet the stringent correctness standards required for valid Sudoku solutions.\nKey Insights.\nThese findings highlight two key insights: (1) VLM-based evaluation tends to be more lenient, potentially overestimating model performance on structured reasoning tasks‚Äîparticularly by over-detecting constraint violations while being more generous on completion accuracy; and (2) human evaluation remains essential for validating generative model outputs in tasks requiring strict logical correctness, as the metrics that matter most (completion accuracy and overall success) show the largest divergence between evaluation methods. The consistent zero overall scores under human evaluation reinforce our main conclusion: current video generative models lack the robust constraint-satisfaction capabilities needed for reliable Sudoku solving.\n7 ARC-AGI\nWe evaluate models on the ARC-AGI task (Abstraction and Reasoning Corpus for Artificial General Intelligence), a benchmark designed to measure Abstract Reasoning, Pattern Recognition, and Rule Induction (Chollet, 2019). ARC-AGI probes a model‚Äôs ability to infer latent transformation rules from a small set of input‚Äìoutput examples and to apply those rules to unseen test cases‚Äîan ability central to fluid intelligence. Unlike tasks with explicit instructions or predefined rule sets, ARC-AGI demands that the model autonomously identify and generalize the underlying visual and structural principles governing each puzzle. As a result, the task jointly tests 2D spatial reasoning (interpreting grid-based patterns) and logical reasoning (deducing and executing abstract transformations).\n7.[ADDRESS_REMOVED]-Level Control\nTo construct a diverse and controllable evaluation suite, we build on the open-source ARC-AGI benchmark (Chollet, 2019). Our final dataset comprises 456 tasks drawn from two benchmark versions:\n-\n‚Ä¢\nARC-AGI v1 (381 tasks): The publicly released training set from the original benchmark, encompassing a wide variety of pattern-transformation and abstract reasoning problems.\n-\n‚Ä¢\nARC-AGI v2 (75 tasks): Newly added tasks that introduce novel pattern families and higher structural complexity, expanding the benchmark‚Äôs coverage.\nThis combined set provides broad coverage of transformation types while introducing sufficient novelty and difficulty for rigorous model evaluation. All tasks undergo manual curation to ensure clean pattern design, consistent formatting, and unambiguous transformation rules. Collectively, the benchmark spans a wide spectrum of transformation categories‚Äîincluding symmetry, rotation, scaling, color manipulation, and object-level reasoning‚Äîcapturing the core abstractions and reasoning skills fundamental to ARC-style tasks.222Additional examples and visualizations are available in the official repositories: [URL_REMOVED] , [URL_REMOVED] , and the automated generation toolkit [URL_REMOVED]\n7.1.1 Two-Level Classification System\nTo enable fine-grained analysis of model capabilities, we classify all cases along two dimensions:\nLevel 1: Shape Consistency Classification. We categorize each case based on whether the input and output grids maintain the same spatial structure:\n-\n‚Ä¢\nMatch (316 cases): The output grid has the same dimensions as the input grid. Transformations occur ‚Äúin-place‚Äù through color changes, pattern filling, or local modifications. Examples include color replacement, symmetry completion, and pattern extension within fixed boundaries.\n-\n‚Ä¢\nMismatch (140 cases): The output grid dimensions differ from the input. These cases require spatial restructuring, such as cropping, extraction of sub-patterns, grid concatenation, or shape reconstruction. These are generally more challenging as they demand explicit understanding of spatial relationships beyond simple transformations.\nLevel 2: Quantitative Difficulty Classification. Within each shape-consistency category, we further assign each case to one of three difficulty levels‚ÄîEasy, Medium, or Hard‚Äîbased on five quantitative grid-level features:\n-\n1.\nGrid Size (minimum side length): cells score 0; - cells score 1; cells score 2.\n-\n2.\nColor Count (distinct colors in input): score 0; - score 1; score 2.\n-\n3.\nObject Count (distinct connected components): score 0; - score 1; score 2.\n-\n4.\nOccupancy Ratio (non-background cells / total cells): score 0; score 1; score 2.\n-\n5.\n(grid change ratio, for Match cases only): score 0; score 1; score 2. For Mismatch cases, this feature is not applicable.\nA case‚Äôs overall difficulty is determined by summing the applicable feature scores.For Match cases (5 features): Easy , Medium ‚Äì, Hard . For Mismatch cases (4 features): Easy , Medium ‚Äì, Hard . TableÀú8 reports the full distribution across categories. FigureÀú9 and FigureÀú10 present some selected examples.\nVersion Shape Consistency Easy Medium Hard Total V1 Match 102 (38.8%) 124 (47.1%) 37 (14.1%) 263 Mismatch 34 (28.8%) 57 (48.3%) 27 (22.9%) 118 Total 136 181 64 381 V2 Match 1 (1.9%) 27 (50.9%) 25 (47.2%) 53 Mismatch / 7 (31.8%) 15 (68.2%) 22 Total 1 34 40 75 Overall Match 103 (31.0%) 151 (45.4%) 62 (23.6%) 316 Mismatch 34 (25.0%) 64 (45.7%) 42 (29.3%) 140 Total 133 (29.2%) 205 (45.0%) 118 (25.9%) 456\n7.2 Evaluation and Metrics\nWe evaluate both video and image outputs using a Vision-Language Model (VLM)‚Äìbased evaluator, Gemini-2.5 Pro (Comanici et al., 2025). The evaluator takes four inputs: (i) the demonstration examples, (ii) the test input, (iii) the ground-truth output, and (iv) the generated video or image. Using these inputs, the VLM determines whether the predicted transformation is correct and identifies errors in pattern recognition, structural consistency, and rule application.\nFrom the evaluator‚Äôs structured responses, we compute one primary correctness metric and three fine-grained diagnostic metrics:\n-\n‚Ä¢\nPattern Recognition: 1 if the evaluator confirms that the model successfully identifies the transformation pattern from the demonstrations; 0 otherwise.\n-\n‚Ä¢\nGrid Integrity: [ADDRESS_REMOVED] grid dimensions and structural layout; 0 if the grid is distorted or misaligned.\n-\n‚Ä¢\nColor Accuracy: 1 if all colors are applied correctly according to the transformation rule; 0 otherwise.\n-\n‚Ä¢\nValid Solution (Primary Metric): 1 only if the generated output exactly matches the ground-truth solution; 0 otherwise.\n7.3 Case Study\nThe case studies presented in FigureÀú11(a) and FigureÀú11(b) highlight a significant failure mode in video generation models applied to abstract reasoning tasks: the inability to maintain temporal consistency for static information. In both instances, the model fails to distinguish between the invariant problem context (the ‚ÄúEXAMPLES‚Äù E1-E4) and the dynamic solution generation (the ‚ÄúTEST‚Äù T1). As the video progresses from Frame 1 to Frame 4, the demonstration examples‚Äîwhich should remain strictly fixed to define the logic of the task‚Äîsuffer from severe hallucinations, including unintended color shifts, pattern deformations, and progressive structural degradation (such as the complete erasure of grid contents in Figure 11). This ‚Äúdrift‚Äù suggests that the model treats the entire visual field as a mutable video sequence rather than respecting the logical constraints of the prompt, ultimately undermining the reasoning process by destabilizing the very ground truth required to solve the puzzle.\n7.4 Evaluation Results\nFine-grained Metrics Primary Metric Model Pattern Recog. Grid Integrity Color Accuracy Overall Video Models Veo-3 17.32% 32.98% 8.22% 5.16% Sora-2 71.99% 94.58% 36.75% 20.18% Wan-2.2 0.61% 13.04% 0.17% 0.17% Image Models Nano-banana 28.42% 55.79% 12.63% 9.21% Nano-banana Pro 61.98% 84.73% 40.42% 30.54% GPT-4o-image 1.05% 10.24% 0.52% 0.00% Qwen-image 1.31% 4.46% 0.52% 0.52%\nFine-grained Metrics Primary Metric Model Pattern Recog. Grid Integrity Color Accuracy Overall Video Models Veo-3 17.78% 31.11% 6.22% 4.00% Sora-2 4.00% 16.00% 1.33% 1.33% Wan-2.2 0.00% 5.78% 0.00% 0.00% Image Models Nano-banana 18.67% 42.67% 8.00% 2.67% Nano-banana Pro 62.50% 83.93% 44.64% 30.36% GPT-4o-image 1.33% 2.67% 1.33% 0.00% Qwen-image 1.33% 5.33% 1.33% 1.33%\n7.4.1 VLM-Based Evaluation\nOverall Performance Patterns.\nThe quantitative results (TableÀú9 and TableÀú10) establish a clear performance hierarchy across the 456 ARC-AGI cases. Nano-banana Pro dominates with 30.54% overall accuracy on v1 and 30.36% on v2, demonstrating both superior reasoning capability and remarkable robustness to distribution shift. Among video models, Sora-2 emerges as the clear leader with 20.18% on v1‚Äînotably surpassing the base Nano-banana image model (9.21%)‚Äîyet collapses to 1.33% on the harder v2 dataset, exposing critical brittleness. Veo-[ADDRESS_REMOVED] but stable performance (5.16% v1, 4.00% v2), while Wan-2.2 effectively fails (0.17% v1, 0.00% v2). At the bottom tier, GPT-4o-image achieves 0.00% across both versions despite non-zero Grid Integrity (10.24% on v1), and Qwen-image barely registers (0.52% v1, 1.33% v2).\nFine-grained Metrics Primary Metric Model Pattern Recog. Grid Integrity Color Accuracy Overall Version: v1 & v2 Combined Level: Easy Video Models Veo-3 18.98% 39.66% 10.46% 5.60% Sora-2 76.07% 89.74% 41.88% 22.22% Wan-2.2 0.00% 16.79% 0.00% 0.00% Image Models Nano-banana 32.85% 64.23% 13.87% 10.22% Nano-banana Pro 62.60% 86.18% 43.90% 30.89% GPT-4o-image 0.73% 10.95% 1.46% 0.00% Qwen-image 1.46% 5.11% 0.73% 0.73% Level: Medium Video Models Veo-3 17.36% 32.40% 7.29% 5.12% Sora-2 58.67% 84.18% 29.08% 16.33% Wan-2.2 0.78% 10.54% 0.31% 0.31% Image Models Nano-banana 24.77% 51.87% 10.75% 7.01% Nano-banana Pro 62.98% 86.74% 40.33% 30.39% GPT-4o-image 0.47% 9.77% 0.00% 0.00% Qwen-image 1.40% 4.65% 0.47% 0.47% Level: Hard Video Models Veo-3 15.38% 24.04% 5.77% 3.85% Sora-2 40.43% 59.57% 18.09% 10.64% Wan-2.2 0.64% 8.01% 0.00% 0.00% Image Models Nano-banana 23.08% 43.27% 11.54% 7.69% Nano-banana Pro 59.30% 77.91% 38.37% 30.23% GPT-4o-image 2.88% 4.81% 0.96% 0.00% Qwen-image 0.96% 3.85% 0.96% 0.96%\nFine-grained Metrics Primary Metric Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Video Models Veo-3 Match 17.74% 35.74% 9.51% 5.70% Mismatch 16.38% 26.84% 5.37% 3.95% Sora-2 Match 67.29% 93.46% 35.05% 17.76% Mismatch 80.51% 96.61% 39.83% 24.58% Wan-2.2 Match 0.51% 13.31% 0.00% 0.00% Mismatch 0.85% 12.43% 0.56% 0.56% Image Models Nano-banana Match 24.05% 53.82% 11.07% 8.40% Mismatch 38.14% 60.17% 16.10% 11.02% Nano-banana Pro Match 62.24% 89.63% 42.74% 31.54% Mismatch 61.29% 72.04% 34.41% 27.96% GPT-4o-image Match 0.38% 9.51% 0.76% 0.00% Mismatch 2.54% 11.86% 0.00% 0.00% Qwen-image Match 0.76% 4.56% 0.76% 0.76% Mismatch 2.54% 4.24% 0.00% 0.00%\nFine-grained Metrics Primary Metric Model / Difficulty Pattern Recog. Grid Integrity Color Accuracy Overall Veo-3: Match Easy 18.95% 40.52% 11.76% 5.88% Medium 17.74% 35.48% 8.06% 5.65% Hard 14.41% 23.42% 8.11% 5.41% Veo-3: Mismatch Easy 19.61% 37.25% 6.86% 4.90% Medium 14.04% 23.39% 5.26% 3.51% Hard 17.28% 20.99% 3.70% 3.70% Sora-2: Match Easy 73.17% 90.24% 42.68% 19.51% Medium 63.81% 95.24% 33.33% 19.05% Hard 62.96% 96.30% 18.52% 7.41% Sora-2: Mismatch Easy 85.29% 91.18% 41.18% 29.41% Medium 80.70% 100.00% 36.84% 19.30% Hard 74.07% 96.30% 44.44% 29.63% Wan-2.2: Match Easy 0.00% 16.34% 0.00% 0.00% Medium 0.54% 12.37% 0.00% 0.00% Hard 1.80% 8.11% 0.00% 0.00% Wan-2.2: Mismatch Easy 0.00% 17.65% 0.00% 0.00% Medium 1.75% 9.36% 1.17% 1.17% Hard 0.00% 12.35% 0.00% 0.00% Nano-banana: Match Easy 27.45% 56.86% 13.73% 8.82% Medium 20.33% 50.41% 8.13% 7.32% Hard 27.03% 56.76% 13.51% 10.81% Nano-banana: Mismatch Easy 47.06% 85.29% 11.76% 11.76% Medium 35.09% 52.63% 19.30% 10.53% Hard 33.33% 44.44% 14.81% 11.11% Nano-banana Pro: Match Easy 62.77% 90.43% 45.74% 31.91% Medium 60.71% 89.29% 37.50% 29.46% Hard 65.71% 88.57% 51.43% 37.14% Nano-banana Pro: Mismatch Easy 60.71% 71.43% 35.71% 25.00% Medium 65.12% 74.42% 37.21% 30.23% Hard 54.55% 68.18% 27.27% 27.27% GPT-4o-image: Match Easy 0.98% 10.78% 1.96% 0.00% Medium 0.00% 11.29% 0.00% 0.00% Hard 0.00% 0.00% 0.00% 0.00% GPT-4o-image: Mismatch Easy 0.00% 11.76% 0.00% 0.00% Medium 1.75% 10.53% 0.00% 0.00% Hard 7.41% 14.81% 0.00% 0.00% Qwen-image: Match Easy 0.98% 5.88% 0.98% 0.98% Medium 0.00% 3.23% 0.00% 0.00% Hard 2.70% 5.41% 2.70% 2.70% Qwen-image: Mismatch Easy 2.94% 2.94% 0.00% 0.00% Medium 3.51% 7.02% 0.00% 0.00% Hard 0.00% 0.00% 0.00% 0.00%\nFine-grained Metrics Primary Metric Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Video Models Veo-3 Match 17.61% 32.08% 6.92% 3.77% Mismatch 18.18% 28.79% 4.55% 4.55% Sora-2 Match 5.66% 18.87% 1.89% 1.89% Mismatch 0.00% 9.09% 0.00% 0.00% Wan-2.2 Match 0.00% 4.40% 0.00% 0.00% Mismatch 0.00% 9.09% 0.00% 0.00% Image Models Nano-banana Match 18.87% 45.28% 11.32% 3.77% Mismatch 18.18% 36.36% 0.00% 0.00% Nano-banana Pro Match 64.29% 88.10% 50.00% 33.33% Mismatch 57.14% 71.43% 28.57% 21.43% GPT-4o-image Match 0.00% 1.89% 0.00% 0.00% Mismatch 4.55% 4.55% 4.55% 0.00% Qwen-image Match 1.89% 5.66% 1.89% 1.89% Mismatch 0.00% 4.55% 0.00% 0.00%\nFine-grained Metrics Primary Metric Model / Difficulty Pattern Recog. Grid Integrity Color Accuracy Overall Veo-3: Match Easy 0.00% 33.33% 0.00% 0.00% Medium 20.99% 34.57% 8.64% 6.17% Hard 14.67% 29.33% 5.33% 1.33% Veo-3: Mismatch Medium 23.81% 42.86% 4.76% 4.76% Hard 15.56% 22.22% 4.44% 4.44% Sora-2: Match Easy 0.00% 0.00% 0.00% 0.00% Medium 7.41% 22.22% 3.70% 3.70% Hard 4.00% 16.00% 0.00% 0.00% Sora-2: Mismatch Medium 0.00% 28.57% 0.00% 0.00% Hard 0.00% 0.00% 0.00% 0.00% Wan-2.2: Match Easy 0.00% 33.33% 0.00% 0.00% Medium 0.00% 4.94% 0.00% 0.00% Hard 0.00% 2.67% 0.00% 0.00% Wan-2.2: Mismatch Medium 0.00% 9.52% 0.00% 0.00% Hard 0.00% 8.89% 0.00% 0.00% Nano-banana: Match Easy 100.00% 100.00% 100.00% 100.00%* Medium 18.52% 55.56% 7.41% 0.00% Hard 16.00% 32.00% 12.00% 4.00% Nano-banana: Mismatch Medium 42.86% 57.14% 0.00% 0.00% Hard 6.67% 26.67% 0.00% 0.00% Nano-banana Pro: Match Easy 100.00% 100.00% 100.00% 100.00%* Medium 66.67% 95.24% 61.90% 38.10% Hard 60.00% 80.00% 35.00% 25.00% Nano-banana Pro: Mismatch Medium 80.00% 100.00% 40.00% 20.00% Hard 44.44% 55.56% 22.22% 22.22% GPT-4o-image: Match Easy 0.00% 0.00% 0.00% 0.00% Medium 0.00% 3.70% 0.00% 0.00% Hard 0.00% 0.00% 0.00% 0.00% GPT-4o-image: Mismatch Medium 0.00% 0.00% 0.00% 0.00% Hard 6.67% 6.67% 6.67% 0.00% Qwen-image: Match Easy 0.00% 0.00% 0.00% 0.00% Medium 3.70% 7.41% 3.70% 3.70% Hard 0.00% 4.00% 0.00% 0.00% Qwen-image: Mismatch Medium 0.00% 0.00% 0.00% 0.00% Hard 0.00% 6.67% 0.00% 0.00%\nThe Modality Gap.\nThe results reveal a fundamental divide between image and video generation paradigms (TableÀú9 and TableÀú10). Image models demonstrate superior abstract reasoning: Nano-banana Pro achieves 61.98% Pattern Recognition and 84.73% Grid Integrity, substantially outperforming the best video model Sora-2 (71.99% Pattern Recognition, 94.58% Grid Integrity). However, a striking anomaly emerges: Sora-2 achieves higher fine-grained scores but lower overall accuracy than Nano-banana Pro. This suggests that video models can perceive patterns and maintain structure but fail at the final integration step‚Äîtranslating partial understanding into correct solutions. The gap amplifies on v2: Nano-banana Pro maintains stable performance (30.36%), while Sora-2 collapses from 20.18% to 1.33%‚Äîa 93% relative decline‚Äîrevealing that video models rely heavily on memorized patterns rather than generalizable reasoning.\nDataset Difficulty Progression.\nPerformance stratification by difficulty level (TableÀú11) reveals distinct scaling behaviors across model types:\n-\n‚Ä¢\nNano-banana Pro (Robust Stability): Maintains remarkable consistency across Easy (30.89%), Medium (30.39%), and Hard (30.23%) cases. Grid Integrity shows only modest degradation (86.18% 86.74% 77.91%), confirming that this model has internalized abstract transformation rules rather than relying on surface-level pattern matching.\n-\n‚Ä¢\nSora-2 (Difficulty-Sensitive Collapse): Exhibits pronounced degradation from Easy (22.22%) to Medium (16.33%) to Hard (10.64%). Notably, Pattern Recognition drops from 76.07% to 40.43%, indicating that video models lose the ability to identify transformation rules as complexity increases.\n-\n‚Ä¢\nVeo-3 (Consistent Underperformance): Remains stagnant around 5% across all difficulty levels, with Grid Integrity showing steady decline (39.66% 32.40% 24.04%). This suggests that Veo-3‚Äôs failures are systematic rather than difficulty-dependent.\nShape Consistency Analysis.\nComparing Match cases (same input/output dimensions) against Mismatch cases (different dimensions) across ARC-AGI v1 and v2 reveals distinct model behaviors (TableÀú12 and TableÀú14):\n-\n‚Ä¢\nThe Inverse Pattern (v1 vs. v2): In v1, both Sora-2 and the base Nano-banana model perform better on Mismatch cases than Match cases (Sora-2: 24.58% vs 17.76%; Nano-banana: 11.02% vs 8.40%). This suggests these models may leverage visual restructuring effectively when dimensions shift. However, this advantage disappears in the more complex v2, where Sora-2 collapses to 0.00% on Mismatch cases.\n-\n‚Ä¢\nNano-banana Pro Stability: Demonstrates demonstrates the most robust handling of transformation types. In v1, it maintains high stability (31.54% Match vs 27.96% Mismatch). In v2, while it remains the top performer, it exhibits a clearer preference for Match cases (33.33%) over Mismatch (21.43%), indicating that increased task complexity reintroduces the expected difficulty penalty for dimension changes.\n-\n‚Ä¢\nModel-Specific Difficulty Curves: Other models show varied sensitivity to grid consistency. Veo-3 displays mixed behavior‚Äîdropping slightly in v1 (5.70% to 3.95%) but surprisingly improving on Mismatch in v2 (3.77% to 4.55%), potentially due to low sample variance. Conversely, image models like Qwen-image consistently struggle with Mismatch tasks, collapsing to near-zero performance across both datasets.\nDimensionality-Difficulty Interaction.\nA granular intersectional analysis of difficulty and grid consistency (TableÀú13 and TableÀú15) exposes a paradox in video model behavior distinct from standard scaling laws:\n-\n‚Ä¢\nThe Sora-2 Anomaly (Hard Case Inversion): On v1 Hard cases, Sora-2 displays a highly anomalous inverted profile, achieving nearly 4 higher accuracy on Mismatch tasks (29.63%) compared to Match tasks (7.41%). Fine-grained metrics reveal this is not a structural failure‚ÄîGrid Integrity remains identical (96.30%) across both categories. Instead, the collapse is driven by Color Accuracy, which plummets to 18.52% on Match compared to 44.44% on Mismatch. This implies video models struggle to execute complex pixel-level color transformations when constrained to a fixed canvas, but recover capability when generating new spatial structures.\n-\n‚Ä¢\nNano-banana Pro (Logical Scaling): In contrast, the leading image model exhibits expected difficulty scaling. It consistently performs better on Match (fixed-dimension) tasks than Mismatch (dynamic-dimension) tasks across all difficulty levels (e.g., v1 Hard Match 37.14% vs. Hard Mismatch 27.27%), confirming a stable, dimension-agnostic reasoning core.\n-\n‚Ä¢\nBrittleness of the Generative Advantage: Sora-2‚Äôs ‚Äúgenerative advantage‚Äù on Mismatch tasks proves brittle. On the v2 benchmark, the anomaly vanishes entirely: Sora-2 collapses to 0.00% on Hard Mismatch cases, while Nano-banana Pro maintains robust capability (22.22%). This reinforces that while video models may occasionally exploit spatial restructuring, they lack the robust generalization of their image-based counterparts.\nMetric Cascade and Bottlenecks.\nThe four evaluation metrics form a consistent funnel where success becomes progressively more restrictive. For Nano-banana on v1: Grid Integrity (55.79%) Pattern Recognition (28.42%) Color Accuracy (12.63%) Overall (9.21%). This cascade reveals that approximately 51% of structurally correct outputs achieve pattern recognition, 44% of those achieve color accuracy, and 73% of color-accurate outputs reach full correctness. Color Accuracy emerges as the critical bottleneck, representing only 20‚Äì25% of Grid Integrity performance across all models. This bottleneck is most severe for GPT-4o-image: despite achieving 10.24% Grid Integrity, it manages only 0.52% Color Accuracy, resulting in 0.00% overall success. The cascade pattern suggests that models can learn structural rules but struggle with precise color-based execution.\nVersion Comparison.\nThe transition from v1 to v2 exposes model robustness to novel pattern families:\n-\n‚Ä¢\nDramatic Video Model Collapse: Sora-2 drops from 20.18% to 1.33% (93% decline), while Nano-banana drops from 9.21% to 2.67% (71% decline). Nano-banana Pro remains stable (30.54% 30.36%).\n-\n‚Ä¢\nDataset Composition Effects: v2 skews harder, containing only 1 Easy Match case versus 102 in v1. The difficulty distribution shift disproportionately penalizes models that rely on memorized patterns.\n-\n‚Ä¢\nGrid Integrity Preservation: Interestingly, Veo-3 maintains similar Grid Integrity across versions (32.98% v1, 31.11% v2) despite failing on the primary metric, suggesting it can preserve structure without understanding transformation semantics.\n7.4.2 Human Evaluation\nTo establish ground-truth performance estimates and validate the reliability of our VLM-based evaluator, we conducted a human evaluation on a subset of 98 Veo-3 generated videos (60 from v1, 38 from v2). Human annotators assess the same four metrics: Pattern Recognition, Grid Integrity, Color Accuracy, and Valid Solution. TableÀú16 and TableÀú17 present side-by-side comparisons of Auto-Eval versus Human-Eval across Match and Mismatch subsets.\nFine-grained Metrics Primary Metric Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Evaluation Methods Human-Eval Match 23.33% 26.67% 30.00% 0.00% Mismatch 13.33% 20.00% 16.67% 0.00% Total 18.33% 23.33% 23.33% 0.00% Auto-Eval Match 19.01% 40.30% 9.51% 5.70% Mismatch 11.86% 25.42% 3.39% 2.54% Total 16.80% 35.70% 7.61% 4.72%\nFine-grained Metrics Primary Metric Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Evaluation methods Human-Eval Match 38.10% 47.62% 38.10% 0.00% Mismatch 5.88% 11.76% 11.76% 0.00% Total 23.68% 31.58% 26.32% 0.00% Auto-Eval Match 20.75% 41.51% 7.55% 1.89% Mismatch 27.27% 40.91% 4.55% 4.55% Total 22.67% 41.33% 6.67% 2.67%\nPattern Recognition.\nHuman and VLM evaluators show reasonable agreement on pattern recognition ability. On v1, humans report 18.33% versus Auto-Eval‚Äôs 16.80%‚Äîa modest 1.5 percentage point difference. On v2, the gap is slightly larger: 23.68% (Human) versus 22.67% (Auto). This consensus suggests that both evaluation methods can reliably detect whether a model has identified the underlying transformation rule, even when execution fails.\nGrid Integrity.\nA substantial divergence emerges in structural assessment. Humans consistently rate Grid Integrity lower than the VLM evaluator: 23.33% versus 35.70% on v1 overall (12.4 pp gap), and 31.58% versus 41.33% on v2 overall (9.8 pp gap). This systematic discrepancy suggests that the VLM evaluator may be more tolerant of minor structural deformations‚Äîsuch as slight grid misalignments or cell boundary artifacts‚Äîthat human annotators penalize. The gap is particularly pronounced for Match cases on v1 (26.67% Human vs 40.30% Auto), indicating that humans apply stricter standards when evaluating in-place transformations where structural preservation is expected.\nColor Accuracy.\nInterestingly, the pattern reverses for color assessment. Humans rate Color Accuracy substantially higher than the VLM: 23.33% versus 7.61% on v1 (3.1 higher), and 26.32% versus 6.67% on v2 (3.9 higher). This suggests the VLM evaluator may be overly sensitive to minor color deviations‚Äîperhaps detecting subtle RGB differences that humans perceive as acceptable matches. The implication is significant: VLM-based Color Accuracy scores may systematically underestimate model performance on this metric.\nValid Solution Rate.\nThe most critical finding is the complete failure under human evaluation: 0.00% Valid Solution rate across all 98 cases, compared to 4.72% (v1) and 2.67% (v2) from Auto-Eval. Despite achieving non-trivial scores on fine-grained metrics‚Äîwith Pattern Recognition reaching 38.10% and Grid Integrity reaching 47.62% on v2 Match cases‚Äîno single generated video produces an exact match to the ground-truth solution. This reveals a fundamental gap between partial understanding and precise execution: video models can demonstrate awareness of transformation patterns and maintain structural consistency in nearly half of cases, yet consistently fail to translate this partial competence into pixel-perfect outputs.\nMatch versus Mismatch Performance.\nHuman evaluation confirms and amplifies the performance gap observed in automatic evaluation. On v2, Match cases achieve 38.10% Pattern Recognition compared to only 5.88% for Mismatch cases‚Äîa 6.5 difference. Grid Integrity shows an even more pronounced gap: 47.62% for Match versus 11.76% for Mismatch (4.0). On v1, the gaps are smaller but consistent: Match cases achieve 23.33% Pattern Recognition versus 13.33% for Mismatch (1.75). These results confirm that video generation models fundamentally struggle with shape-changing transformations, where producing outputs with different dimensions from inputs poses a substantially greater challenge than in-place modifications.\nKey Insights.\nThe human evaluation reveals two critical findings: (1) VLM-based evaluation tends to overestimate overall success while being overly strict on Color Accuracy and lenient on Grid Integrity‚Äîa pattern that may mask true model capabilities and limitations; and (2) Human evaluation remains essential for validating generative model outputs in tasks requiring strict logical correctness, as the complete 0.00% Valid Solution rate under human judgment starkly contrasts with the non-zero Auto-Eval scores. These findings underscore that current video generation models lack the robust constraint-satisfaction capabilities needed for reliable abstract reasoning.\n7.4.3 Key Findings and Implications\nThree critical insights emerge from the ARC-AGI evaluation, revealing fundamental limitations in how current generative models approach abstract visual reasoning.\n1. The Temporal Consistency Barrier.\nA defining failure mode of video generation on ARC-AGI is the inability to maintain static context. As illustrated in FigureÀú11(a) and FigureÀú11(b), video models fail to distinguish between invariant problem context (the demonstration examples E1‚ÄìE4) and the dynamic solution generation (test output T1). Demonstration examples‚Äîwhich should remain strictly fixed to define the transformation logic‚Äîundergo progressive hallucination: color shifts, pattern deformations, and structural degradation across frames. This ‚Äúcontext drift‚Äù effectively corrupts the problem definition itself, undermining any chance of correct solution generation. The implication is clear: current video architectures treat the entire visual field as a mutable sequence rather than respecting logical invariants, making them fundamentally unsuited for reasoning tasks that require stable reference points.\n2. The Perception-Execution Gap.\nThe metric cascade analysis reveals a systematic disconnect between pattern perception and precise execution. Models frequently achieve reasonable Pattern Recognition (up to 71.99% for Sora-2) and Grid Integrity (up to 94.58%) scores, yet fail catastrophically on Overall accuracy. This suggests that current generative models can see the transformation pattern but cannot execute it correctly. The bottleneck consistently occurs at Color Accuracy, where performance drops to 20‚Äì25% of Grid Integrity levels. This perception-execution gap implies that models may learn to recognize visual patterns without internalizing the precise symbolic rules governing color assignment‚Äîa fundamental limitation for tasks requiring exact constraint satisfaction.\n3. The Robustness-Memorization Trade-off.\nThe dramatic performance collapse from v1 to v2‚Äîparticularly Sora-2‚Äôs 93% relative decline‚Äîexposes a critical reliance on memorized patterns rather than generalizable reasoning. In contrast, Nano-banana Pro‚Äôs stability across versions (30.54% 30.36%) demonstrates that image-based generation can achieve robust abstract reasoning. This dichotomy suggests that video models may be overfitting to temporal motion patterns in training data rather than learning the underlying transformation logic. For ARC-AGI specifically, where novel pattern families are explicitly designed to defeat memorization, this limitation is fatal. The finding has broader implications: video generation architectures may require fundamentally different training objectives or inductive biases to support genuine rule-based reasoning rather than sophisticated pattern matching.\n[ADDRESS_REMOVED] the Visual Math task to evaluate Logical Reasoning, Logical Deduction, and 2D Spatial Reasoning. This task probes a model‚Äôs ability to understand complex mathematical problems presented visually (e.g., geometry, diagrams) and challenges its Temporal Reasoning by requiring it to generate a video animating the step-by-step deduction from premises to solution (Huang et al., 2025).\n8.[ADDRESS_REMOVED]-Level Control\nWe evaluate models across five benchmarks that span a wide spectrum of difficulty: GSM8K (Cobbe et al., 2021) (grade school), MATH (Hendrycks et al., 2021) (high school competition), the AIME 2024 (Mathematical Association of America, 2024) and AIME 2025 (Mathematical Association of America, 2025) invitational examinations, and Omni-MATH (Gao et al., 2024) (Olympiad-level). For Omni-MATH, we further classify problems by difficulty (five levels: T0‚ÄìT4) and category (eight types: Algebra, Applied Math, Calculus, Discrete Math, Geometry, Precalculus, Number Theory, and Other). TableÀú18 summarizes the evaluation benchmarks and sample counts, while TableÀú19 details the Omni-MATH sample distribution across difficulty levels and categories. FigureÀú12 shows some selected examples.\nDataset Level Sample Count GSM8K Grade School 50 MATH500 High School Competition 50 AIME 2024 Invitational Competition 30 AIME 2025 Invitational Competition 30 Omni-MATH Multi-level (T0-T4) 167 Total ‚Äì 327\n-\n‚Ä¢\nNote: Omni-MATH provides a fine-grained ontology for mathematical problem classification. Difficulty levels range from T0 (easiest, suitable for middle school) to T4 (hardest, olympiad-level problems). Categories cover the major branches of mathematics commonly tested in competitions and standardized assessments. These results represent video generation evaluations only; image generation was not performed for this dataset.\n8.2 Evaluation and Metrics\nWe use Gemini-2.5-Pro (Comanici et al., 2025) as our evaluator to assess the correctness and quality of generated solutions. The evaluator analyzes intermediate reasoning steps, final answers, and reflective behavior (for video generations). We define the following metrics:\n-\n‚Ä¢\nFinal Correctness: 1 if the final solution matches the ground truth answer, 0 otherwise. This metric verifies whether the model arrives at the correct conclusion.\n-\n‚Ä¢\nIntermediate Correctness: 1 if all reasoning steps leading to the solution are logically valid and mathematically sound, 0 otherwise. This evaluates the quality of the step-by-step deduction process.\n-\n‚Ä¢\nAction Reflection (videos only): 1 if the generated video exhibits self-correction behavior (e.g., revising incorrect steps, reconsidering approaches), 0 if the solution proceeds without reflection. This metric is not applicable to static image generations.\n-\n‚Ä¢\nOverall Score: 1 if and only if both Final Correctness=1 AND Intermediate Correctness=1, 0 otherwise. This is the primary metric representing complete solution correctness.\n8.3 Evaluation Results\n8.3.1 Overall Performance Patterns\nThe Modality Gap. The quantitative results (TableÀú20) establish a stark performance hierarchy where image generative models consistently outperform video generative models. The gap is most visible when comparing the state-of-the-art in both modalities: on the entry-level GSM8K benchmark, Nano-banana Pro achieves a near-perfect Overall Success Rate of 97.83%, whereas the best-performing video model, Sora-2, reaches only 30.00%‚Äîa greater than performance disparity. This gap widens on competition-level mathematics; on AIME 25, Nano-banana Pro maintains a robust 66.67%, while Sora-2 collapses to 0.00%.\nThe Illusion of Reasoning in Video. A critical insight from the fine-grained metrics is the divergence between Outcome Success Rate and Process Success Rate in video models. On GSM8K, Veo-3 achieves a relatively high Outcome Success Rate of 74.00% but a low Process Success Rate of 12.00%. This substantial 62% delta indicates that video models frequently arrive at the correct final solution through ‚Äúhallucinated‚Äù or logically invalid visual transitions, effectively guessing the answer without successfully modeling the mathematical steps. In contrast, image models like Nano-banana Pro show near-perfect alignment (97.83% for both metrics), demonstrating that their outputs rely on consistent step-by-step deduction.\nAction Reflection and Error Correction. Video-specific Action Reflection scores remain critically low across the board (0‚Äì16%), implying a limited capacity for self-correction during temporal generation.\n-\n‚Ä¢\nInverse Scaling: Interestingly, different video models struggle in different regimes. Sora-2 leads video models on easier tasks (30.00% on GSM8K) but degrades on hard tasks.\n-\n‚Ä¢\nThe Wan-2.2 Anomaly: Conversely, Wan-2.2 performs poorly on simple tasks (2.00% on GSM8K) but unexpectedly outperforms other video models on the hardest benchmarks (15.56% on AIME 24), suggesting it may possess latent reasoning capabilities that are not triggered by simpler prompts, or that its training distribution favors complex visual proofs over simple arithmetic.\nFine-grained Metrics Primary Metric Model Process Success Rate Outcome Success Rate Action Reflection Overall Success Rate Dataset: GSM8K Video Models Veo-3 12.00% 74.00% 12.00% 12.00% Sora-2 38.00% 64.00% 16.00% 30.00% Wan-2.2 2.00% 2.00% 0.00% 2.00% Image Models Nano-banana 44.00% 88.00% N/A 42.00% Nano-banana Pro 97.83% 97.83% N/A 97.83% GPT-4o-image 80.00% 83.48% N/A 75.65% Qwen-image 44.00% 44.00% N/A 44.00% Dataset: MATH500 Video Models Veo-3 20.00% 52.00% 14.00% 18.00% Sora-2 34.04% 59.57% 10.64% 31.91% Wan-2.2 3.33% 6.00% 0.00% 3.33% Image Models Nano-banana 16.00% 74.00% N/A 16.00% Nano-banana Pro 91.84% 91.84% N/A 91.84% GPT-4o-image 29.14% 42.45% N/A 27.34% Qwen-image 0.00% 0.00% N/A 0.00% Dataset: AIME24 Video Models Veo-3 8.33% 8.33% 5.00% 1.67% Sora-2 8.70% 13.04% 4.35% 4.35% Wan-2.2 15.56% 22.22% 11.11% 15.56% Image Models Nano-banana 5.00% 15.00% N/A 0.00% Nano-banana Pro 63.64% 36.36% N/A 31.82% GPT-4o-image 3.33% 10.00% N/A 0.00% Qwen-image 1.12% 1.12% N/A 1.12% Dataset: AIME25 Video Models Veo-3 3.33% 11.67% 1.67% 3.33% Sora-2 8.70% 21.74% 4.35% 0.00% Wan-2.2 5.56% 10.00% 3.33% 5.56% Image Models Nano-banana 1.75% 33.33% N/A 1.75% Nano-banana Pro 71.43% 90.48% N/A 66.67% GPT-4o-image 0.00% 3.33% N/A 0.00% Qwen-image 4.00% 4.00% N/A 4.00% Dataset: Omni-MATH Video Models Veo-3 4.79% 15.57% 5.09% 3.89% Sora-2 0.62% 1.88% 6.88% 0.62% Wan-2.2 0.41% 3.46% 0.61% 0.41% Image Models Nano-banana 3.90% 39.94% N/A 3.90% Nano-banana Pro 65.77% 85.59% N/A 63.06% GPT-4o-image 0.58% 2.33% N/A 0.58% Qwen-image 15.65% 15.65% N/A 14.35%\n8.3.2 Dataset Difficulty Progression\nPerformance degradation across datasets (TableÀú20) reveals three distinct scaling behaviors: robust retention (Nano-banana Pro), rapid collapse (GPT-4o-image), and the ‚Äúmiddle-peak‚Äù stability of video models (Veo-3).\nGSM8K (Grade School): The Hallucination Gap.\nOn this baseline benchmark, image models demonstrate mastery, with Nano-banana Pro hitting a ceiling of 97.8%. In contrast, Veo-3 exhibits a defining characteristic of current video generation: the ‚Äúillusion of competence.‚Äù While it achieves a high Outcome Success Rate of 74.00%, its Process Success Rate is only 12.00% (and Overall Success matches at 12.00%). This massive disparity suggests that on simple word problems, Veo-[ADDRESS_REMOVED] answers from its training data without generating the corresponding visual reasoning to support them. Sora-2 performs better here (30.00% Overall), but still trails image models by a wide margin.\nMATH500 (High School Competition): The Video Stability Phenomenon.\nAs problem complexity increases, a counter-intuitive trend emerges for video models. While image models like GPT-4o-image suffer a 64% relative decline (75.65% 27.34%), Veo-3 actually improves, rising from 12.00% on GSM8K to 18.00% on MATH500. Similarly, Sora-2 remains stable (31.91%). This suggests that the structured, formal nature of competition mathematics‚Äîwhich often involves distinct, sequential steps‚Äîmay align better with video generation temporal priors than the varying linguistic structures of grade-school word problems. Meanwhile, Nano-banana Pro continues to dominate, maintaining a 91.84% success rate.\nAIME & Omni-MATH: The Hard-Reasoning Frontier.\nOn the most challenging benchmarks, model behaviors diverge sharply:\n-\n‚Ä¢\nThe Image SOTA: Nano-banana Pro defies the difficulty curve, achieving 66.67% on AIME25 and 63.06% on Omni-MATH, proving that visual generation models can sustain complex reasoning chains.\n-\n‚Ä¢\nThe Video Shuffle: The hierarchy among video models inverts at this level. Wan-2.2, which failed on easy tasks, spikes to 15.56% on AIME24. Furthermore, on Omni-MATH, Veo-[ADDRESS_REMOVED] video model (3.89%), significantly outperforming both Sora-2 (0.62%) and Wan-2.2 (0.41%). This indicates that while Veo-3 struggles with the ‚Äúexactness‚Äù of simple arithmetic (GSM8K), it possesses a superior capacity for generalized, albeit imperfect, reasoning on novel, high-complexity tasks compared to its peers.\n8.3.3 Omni-MATH Deep Dive\nThe fine-grained analysis of Omni-MATH reveals that difficulty and domain do not impact video and image models uniformly (TableÀú21 and TableÀú22).\nDifficulty Analysis: The ‚ÄúInverse Scaling‚Äù Anomaly.\nStandard benchmarks usually show linear degradation as difficulty increases (T0 T4). However, both model types exhibit counter-intuitive behavior here:\n-\n‚Ä¢\nVideo Non-Monotonicity: Veo-3 displays a ‚ÄúU-shaped‚Äù performance curve. It starts at 6.06% (T0), dips to a near-zero 1.47% on T3, but surprisingly recovers to 5.00% on T4 (the hardest tier). This suggests that T4 problems, while mathematically harder, may possess canonical structures (standard Olympiad templates) that video models can memorize and reproduce more effectively than the ambiguous, semi-structured problems found in intermediate tiers (T1‚ÄìT3).\n-\n‚Ä¢\nImage Inverse Scaling: Nano-banana Pro exhibits arguably the most shocking result in the dataset: it performs better on the hardest problems than the easiest ones. Its Overall Success Rate skyrockets from 26.67% on T1 to a commanding 82.76% on T4. This implies the model is highly optimized for formal, high-complexity mathematical proofs rather than simpler, variable-heavy word problems.\nCategory Analysis: The Visual vs. Abstract Divide.\nDomain breakdowns expose the specific cognitive limitations of video generation.\n-\n‚Ä¢\nGeometry: Video models perform best in Geometry (Veo-3: 8.33%), outperforming their own averages in other domains. This confirms that video generation logic aligns well with geometric construction, where reasoning steps (e.g., drawing auxiliary lines) correspond directly to visual frame transitions.\n-\n‚Ä¢\nThe Abstract Collapse: Conversely, video models face catastrophic failure in domains requiring abstract symbolic manipulation. In Calculus, Discrete Math, Applied Math, and Precalculus, video models collapse to an Overall Success Rate of 0.00%. Notably, in Precalculus, Veo-3 achieves a 21.74% Outcome Success Rate but 0.00% overall, illustrating extreme ‚Äúhallucination‚Äù‚Äîguessing the right number through invalid visual morphing.\n-\n‚Ä¢\nImage Robustness: Unlike video models, Nano-banana Pro maintains high process correctness across all domains, including those where video fails (e.g., 53.33% in Calculus; 68.75% in Applied Math). This confirms that SOTA image models have successfully internalized the symbolic logic required for abstract math, whereas video models remain tethered to visual pattern matching.\n8.3.4 Key Findings and Implications\nThree critical insights emerge from the evaluation, pointing to fundamental divergences in how image and video architectures process mathematical logic.\n1. The Temporal Penalties of Reasoning.\nThe results establish that video generation currently penalizes mathematical reasoning rather than enhancing it. While image-based models like Nano-banana Pro have achieved near-mastery of complex logic (66%+ on AIME), video models face a ‚Äútemporal tax.‚Äù The requirement to maintain frame-to-frame coherence appears to compete with logical consistency, leading to a massive performance gap‚Äîoften exceeding 4‚Äì[ADDRESS_REMOVED] benchmarks. This suggests that current video architectures treat mathematical derivation as a visual texture to be morphed, rather than a semantic chain to be constructed, necessitating new architectures that decouple reasoning states from visual rendering.\n2. The ‚ÄúHallucination‚Äù of Competence in Video.\nA defining failure mode of current video models is solution-answer dissociation. Models like Veo-3 frequently achieve high Outcome Success Rates (e.g., 74% on GSM8K) while failing Process Success (12%), indicating they reach correct answers through invalid or hallucinated visual transitions. This contrasts sharply with SOTA image models, where process and outcome metrics are tightly coupled ( correlation for Nano-banana Pro). This dissociation undermines the utility of video for educational or explanatory tasks, as the visual \"proof\" provided by the video is often mathematically illusory despite the final answer being correct.\n3. Domain-Specific ‚ÄúIslands of Aptitude.‚Äù\nVisual mathematical reasoning is not a monolithic capability but highly domain-dependent.\n-\n‚Ä¢\nThe Geometry Bias: Video models show a clear inductive bias for Geometry (8.33% success), where the reasoning process (construction, transformation) is inherently spatial-temporal.\n-\n‚Ä¢\nThe Abstract Barrier: Conversely, video models collapse to near-zero performance on abstract domains like Calculus and Discrete Math.\n-\n‚Ä¢\nThe ‚ÄúInverse Scaling‚Äù Paradox: Unexpectedly, models like Wan-2.2 (Video) and Nano-banana Pro (Image) perform significantly better on the hardest benchmarks (AIME, Omni-MATH T4) than on intermediate ones. This implies that high-difficulty problems often follow rigid, canonical templates that models can memorize, whereas ‚Äúsimpler‚Äù problems involving variable linguistic structures or non-standard arithmetic expose the fragility of their generalized reasoning.\n8.3.5 Case Study Analysis: Veo-3 Failure Modes\nVisual case studies of Veo-3‚Äôs generation traces reveal three structural pathologies that explain the ‚ÄúReasoning-Outcome Disconnect‚Äù observed in the quantitative results.\n1. Temporal Drift and Visual Hallucination.\nThe most pervasive failure mode is Temporal Drift, where the logical integrity of the solution degrades as the video progresses, even if the initial setup is correct. This is the primary driver of the massive gap between Outcome Success (74.00%) and Process Success (12.00%) on GSM8K. In 62% of cases, Veo-3 ‚Äúteleports‚Äù to the correct final answer via visually fluid but mathematically invalid transitions‚Äîmorphing numbers arbitrarily or skipping essential derivation steps. This suggests the model minimizes visual prediction error (pixel consistency) at the expense of logical semantic error, prioritizing a smooth-looking video over a mathematically valid proof.\n2. The \"Correction Paradox\" (Toxic Reflection).\nCounter-intuitively, the model‚Äôs self-correction mechanisms often act as noise injection rather than error mitigation. While Veo-3 exhibits an Action Reflection rate of 14.00% on MATH500, these edits rarely salvage the solution. Qualitative analysis shows that when the model attempts to ‚Äúbacktrack‚Äù and rewrite a frame, it frequently introduces continuity errors or hallucinates new, irrelevant constraints. Rather than exhibiting genuine metacognition (detecting logical flaws), the reflection appears to be a stochastic process‚Äîrandomly modifying parts of the visual field‚Äîwhich disrupts the linear chain of reasoning required for multi-step problems.\n3. Spatial Attention Collapse (Positional Bias).\nVeo-[ADDRESS_REMOVED] ‚Äúfoveal bias,‚Äù disproportionately attending to the center of the visual field while neglecting constraints or auxiliary figures located at the periphery. This bias is particularly detrimental in Geometry tasks (where Veo-3 otherwise performs well, relative to other domains). In failed instances, the model successfully renders the central geometric construction but fails to incorporate numerical values or variable definitions positioned in the upper or lower margins. This spatial neglect results in ‚Äúcorrectly solved‚Äù wrong problems‚Äîlogic that is internally consistent but divorced from the specific boundary conditions of the prompt.\nFine-grained Metrics Primary Metric Model Process Success Rate Outcome Success Rate Action Reflection Overall Success Rate T0 Video Models Veo-3 6.06% 12.12% 3.03% 6.06% Sora-2 3.03% 6.06% 15.15% 3.03% Wan-2.2 0.98% 4.90% 2.94% 0.98% Image Models Nano-Banana 0.00% 33.82% N/A 0.00% Nano-Banana Pro 53.85% 84.62% N/A 53.85% GPT-4o-image 0.00% 0.00% N/A 0.00% Qwen-image 22.00% 22.00% N/A 22.00% T1 Video Models Veo-3 4.55% 4.55% 1.52% 3.03% Sora-2 0.00% 3.12% 3.12% 0.00% Wan-2.2 0.00% 6.06% 0.00% 0.00% Image Models Nano-Banana 0.00% 19.70% N/A 0.00% Nano-Banana Pro 26.67% 60.00% N/A 26.67% GPT-4o-image 0.00% 0.00% N/A 0.00% Qwen-image 8.42% 13.68% N/A 8.42% T2 Video Models Veo-3 5.56% 16.67% 2.78% 5.56% Sora-2 0.00% 0.00% 2.86% 0.00% Wan-2.2 0.93% 3.70% 0.00% 0.93% Image Models Nano-Banana 4.17% 47.22% N/A 4.17% Nano-Banana Pro 70.00% 96.67% N/A 66.67% GPT-4o-image 2.78% 2.78% N/A 2.78% Qwen-image 7.92% 8.91% N/A 7.92% T3 Video Models Veo-3 1.96% 13.24% 1.47% 1.47% Sora-2 0.00% 0.00% 9.68% 0.00% Wan-2.2 0.00% 1.96% 0.00% 0.00% Image Models Nano-Banana 5.88% 45.59% N/A 5.88% Nano-Banana Pro 70.83% 79.17% N/A 62.50% GPT-4o-image 0.00% 2.86% N/A 0.00% Qwen-image 16.67% 12.22% N/A 12.22% T4 Video Models Veo-3 5.17% 36.67% 15.00% 5.00% Sora-2 0.00% 0.00% 3.45% 0.00% Wan-2.2 0.00% 0.00% 0.00% 0.00% Image Models Nano-Banana 9.52% 53.97% N/A 9.52% Nano-Banana Pro 82.76% 93.10% N/A 82.76% GPT-4o-image 0.00% 6.67% N/A 0.00% Qwen-image 25.68% 22.97% N/A 22.97%\nFine-grained Metrics Primary Metric Model Process Success Rate Outcome Success Rate Action Reflection Overall Success Rate Algebra Video Models Veo-3 4.17% 12.50% 4.17% 4.17% Sora-2 4.35% 8.70% 13.04% 4.35% Wan-2.2 0.00% 5.56% 0.00% 0.00% Image Models Nano-banana 4.17% 45.83% N/A 4.17% Nano-banana Pro 61.11% 83.33% N/A 55.56% GPT-4o-image 0.00% 4.00% N/A 0.00% Qwen-image 19.70% 24.24% N/A 19.70% Applied Math Video Models Veo-3 0.00% 20.00% 8.00% 0.00% Sora-2 0.00% 0.00% 8.00% 0.00% Wan-2.2 0.00% 5.33% 0.00% 0.00% Image Models Nano-banana 16.00% 32.00% N/A 16.00% Nano-banana Pro 68.75% 87.50% N/A 68.75% GPT-4o-image 4.00% 4.00% N/A 4.00% Qwen-image 14.49% 14.49% N/A 14.49% Calculus Video Models Veo-3 5.00% 10.00% 5.00% 0.00% Sora-2 0.00% 0.00% 10.00% 0.00% Wan-2.2 0.00% 1.67% 0.00% 0.00% Image Models Nano-banana 0.00% 35.00% N/A 0.00% Nano-banana Pro 66.67% 80.00% N/A 53.33% GPT-4o-image 0.00% 0.00% N/A 0.00% Qwen-image 15.79% 15.79% N/A 15.79% Discrete Math Video Models Veo-3 0.00% 12.00% 4.00% 0.00% Sora-2 0.00% 0.00% 4.17% 0.00% Wan-2.2 0.00% 5.33% 0.00% 0.00% Image Models Nano-banana 0.00% 24.00% N/A 0.00% Nano-banana Pro 54.55% 72.73% N/A 54.55% GPT-4o-image 0.00% 0.00% N/A 0.00% Qwen-image 15.94% 17.39% N/A 15.94% Geometry Video Models Veo-3 8.33% 25.00% 4.17% 8.33% Sora-2 0.00% 0.00% 0.00% 0.00% Wan-2.2 0.00% 1.39% 4.17% 0.00% Image Models Nano-banana 0.00% 45.83% N/A 0.00% Nano-banana Pro 78.57% 92.86% N/A 78.57% GPT-4o-image 0.00% 0.00% N/A 0.00% Qwen-image 23.64% 20.00% N/A 18.18% Precalculus Video Models Veo-3 4.35% 21.74% 8.70% 0.00% Sora-2 0.00% 0.00% 0.00% 0.00% Wan-2.2 0.00% 1.45% 0.00% 0.00% Image Models Nano-banana 13.04% 65.22% N/A 13.04% Nano-banana Pro 68.18% 86.36% N/A 68.18% GPT-4o-image 0.00% 4.17% N/A 0.00% Qwen-image 20.90% 16.42% N/A 16.42% Number Video Models Veo-3 4.17% 8.33% 12.50% 4.17% Sora-2 0.00% 0.00% 0.00% 0.00% Wan-2.2 1.39% 1.39% 0.00% 1.39% Image Models Nano-banana 0.00% 50.00% N/A 0.00% Nano-banana Pro 64.29% 92.86% N/A 64.29% GPT-4o-image 0.00% 4.00% N/A 0.00% Qwen-image 2.82% 4.23% N/A 2.82%\n9 Embodied Navigation\nWe develop the Embodied Navigation task to evaluate 3D Spatial Understanding, 2D Spatial Grounding, Temporal Reasoning, and Physical Commonsense. This task probes a model‚Äôs ability to reason about ego-centric environments, interpreting scene geometry, anticipating future states, and respecting physical constraints, and challenges its multi-step planning ability by requiring it to generate a video or image that depicts the agent‚Äôs trajectory as it navigates toward the goal.\n9.1 Task Definition\nFigureÀú14 illustrates the four Embodied Navigation task settings evaluated in our benchmark:\n-\n‚Ä¢\nPanoramic View Last-Mile Navigation (L.M.Nav.) (SectionÀú10) presents a panoramic environment from a third-person ‚Äúover-the-shoulder‚Äù perspective, requiring models to reason over wide-field visual context for short-range navigation.\n-\n‚Ä¢\nTop-down View Real-World Navigation (T.V.R.-W.Nav.) (SectionÀú11) uses a fixed bird‚Äôs-eye camera, emphasizing global spatial planning and long-horizon path prediction.\n-\n‚Ä¢\n3D Real-World Navigation (3D R.-W.Nav.) (SectionÀú12) adopts cutaway/dollhouse-style renderings to expose full 3D structure, challenging models to ground navigation decisions in multi-room, multi-level layouts from a fixed third-person view.\n-\n‚Ä¢\nSimultaneous Localization and Generation (SLAG) (SectionÀú13) combines both 3D and top-down environment views, requiring models to jointly localize the agent and generate the surrounding scene layout. Together, these settings form a comprehensive testbed for evaluating spatial reasoning, geometric understanding, and scene-generation capabilities of modern video and image generative models.\n9.[ADDRESS_REMOVED]-Level Control\nTo build a consistent and controllable evaluation suite across the four embodied navigation tasks‚ÄîPanoramic View Last-Mile Navigation, Top-down View Real-World Navigation, 3D Real-World Navigation, and Simultaneous Localization and Generation (SLAG)‚Äîwe structure the dataset along four hard-level axes: Environmental Complexity, View Fidelity, Trajectory Distance, and Destination Specification. Each axis is defined through a unified set of principles shared across tasks, while accommodating modality-specific differences in sensing, spatial representation, and scene geometry. This organization ensures cross-task comparability while preserving the unique challenges intrinsic to each navigation setting.\n9.2.1 Environmental Complexity\nWe source scenes from photo-realistic indoor scans in Matterport3D (Chang et al., 2017) and HM3D (Ramakrishnan et al., 2021b), as well as rendered environments in Habitat (Savva et al., 2019). Environmental complexity varies by the structural layout visible to the agent. In Panoramic View Last-Mile Navigation, complexity is determined by the spatial arrangement captured within a single panorama: floor01 scenes correspond to single-floor homes with no vertical transitions, whereas floor02plus scenes include multi-level structures with either implicit stairs not fully visible in the panorama or explicit staircases enabling vertical navigation. For the remaining three tasks, which rely on rendered 3D views or top-down maps, multi-level environments present fully connected floors and additional branching regions. These multi-floor layouts may include basements, attics, outdoor pools, and gardens. By varying structural scale while keeping other factors fixed, the benchmark ensures controlled yet diverse navigation challenges.\n9.2.[ADDRESS_REMOVED] rendering modalities, we define view fidelity in a task-specific yet cross-comparable manner. For Panoramic View Last-Mile Navigation, fidelity captures how much of the environment is visually accessible. Human raters evaluate the extent and spatial distribution of occlusions from foreground objects‚Äîi.e., how much of the room layout is obstructed and how many landmarks remain visible. Scores range from 3 to 5, corresponding to quality03 through quality05. For Top-down View Real-World Navigation, 3D Real-World Navigation, and SLAG, fidelity reflects a more holistic assessment of scene realism and navigability. Raters consider factors such as the presence of holes or cracks, furnishing quality, door openness, and the plausibility of interaction with the environment. These scenes are likewise scored on the 3‚Äì5 scale aligned with quality03 to quality05.\n9.2.3 Trajectory Distance\nTrajectory distance is defined as the geodesic separation between the agent‚Äôs starting point and the target location. We categorize trajectories into two types. Short trajectories involve relatively direct motion: they require no major turns and may include vertical movement (e.g., reaching an upper floor) without substantial directional changes. Long, In contrast,trajectories include at least one significant turn. To maintain comparability across the four tasks, long trajectories are selected so that they share partial path structure with the corresponding short cases at the same hard level.\n9.2.[ADDRESS_REMOVED] visual annotation or through a natural-language description of the same location. For color-based targets (color mark), annotators highlight the target region in the input image using a pure red overlay (#ff0000), ensuring that it is visually distinguishable from its surroundings. For semantic targets (location description), annotators describe the corresponding region using natural language. Although both specifications are intended to reference the same location, ambiguity may arise when multiple similar regions exist. In such cases, annotators include additional disambiguating details‚Äîsuch as floor identifiers and spatially anchored landmarks‚Äîto ensure the description uniquely identifies the intended target area.\nThese four axes allow us to systematically control difficulty across all 24 configuration slots summarized in the table, while keeping the evaluation consistent across tasks that vary in sensing modality, scene rendering, and navigation objective. TableÀú23 provides the detailed distribution of the Embodied Navigation samples.\n9.3 Evaluation and Metrics\nEvaluating navigation-conditioned video or image generation requires metrics that jointly capture geometric fidelity and visual-semantic reasoning. Our evaluation focuses on whether the agent moves plausibly through the environment, correctly interprets the navigation instruction, adheres to the physical layout of the scene, and preserves the identity and spatial integrity of the target destination. All metrics are binary and are computed directly from the agent‚Äôs execution trace and the generated video frames. The definitions below consolidate the evaluation protocol used across all navigation tasks, drawing from the criteria specified in the evaluation prompt templates and supplementary rules. As illustrated in FigureÀú16, this framework is adaptive: while several fine-grained metrics are shared across multiple tasks, others are task-specific to address unique navigation modalities.\n9.3.[ADDRESS_REMOVED] destination based solely on geometric information. They intentionally ignore visual fidelity, semantic correctness, and physical plausibility, which are evaluated by separate metrics.\n-\n‚Ä¢\nSuccess Score (S.S. 2D). Measures whether the agent‚Äôs final position lies within the highlighted or textually specified goal region in the 2D overhead map. The score is if the final coordinates fall entirely inside the goal footprint; otherwise .\n-\n‚Ä¢\nOracle Success Score (O.S. 2D). Provides partial credit when the agent comes sufficiently close to the 2D goal during navigation. The score is if the agent‚Äôs path ever intersects or touches the goal region, even if it does not stop there; otherwise .\n-\n‚Ä¢\nSuccess Score (S.S. 3D). Checks whether the agent ends inside the correct destination volume in the 3D navigation sequence. This metric is purely geometric and independent of any visual discrepancies at the destination. The score is if the final 3D position is within the target volume; otherwise .\n-\n‚Ä¢\nOracle Success Score (O.S. 3D). Grants credit when the agent enters the vicinity of the correct 3D destination at any point during its rollout. The score is if the trajectory ever crosses the predefined proximity threshold around the target; otherwise .\n-\n‚Ä¢\nTrajectory Alignment Score. Evaluates whether the agent‚Äôs 2D projected route is consistent with its 3D motion path, focusing on major turns and spatial transitions. A score of indicates strong correspondence between the two trajectories; otherwise .\n9.3.2 Video Quality in Physical Understanding\nThese metrics assess whether the agent‚Äôs motion obeys basic physical principles and remains consistent with the underlying scene geometry. They focus on physical plausibility, continuity, and spatial coherence rather than destination correctness.\n-\n‚Ä¢\nObject Semantic Score (Obj. Sem.). Evaluates whether the agent interacts with the environment in a physically valid way. The agent must not collide with, pass through, or visually intersect solid structures such as walls, furniture, or appliances. Score is if no collision or penetration is observed; otherwise .\n-\n‚Ä¢\nAgent Consistency Score (Agent Con.). Measures temporal continuity and identity preservation of the navigating agent. For image generation tasks: (1) The agent‚Äôs trajectory must remain continuous across frames, and (2) exactly one agent should appear throughout the navigation sequence. Score is if the same agent moves smoothly and consistently across all frames; otherwise .\n-\n‚Ä¢\nSpatial Alignment Score (Spa. Ali.). Checks whether the agent‚Äôs heading, motion direction, and elevation changes remain coherent with the expected physical layout. For image generation tasks: (1) The initial position must be visually identifiable when provided, and (2) the agent‚Äôs initial facing direction must align with its first movement. Score is if heading, transitions, and movement direction are physically and visually consistent; otherwise .\n9.3.3 Video Quality in Instruction Following\nBecause a video generation model can ‚Äúcheat‚Äù in embodied navigation‚Äîby fabricating a visually plausible destination, altering the environment, or painting a new target beneath the agent‚Äîwe impose strict constraints to ensure faithful instruction following. These metrics evaluate whether the generated video preserves the intended destination and maintains a static, coherent scene.\n-\n‚Ä¢\nDestination Integrity Score (Des. Inte.). Assesses whether the destination region is preserved and correctly interpreted by the model. According to the supplementary rules: (1) The red-marked target region must remain unchanged in size, position, texture, and overall appearance; (2) The agent must not rely on hallucinated alternatives (e.g., newly created look-alike objects or fabricated goal markers). The score is if the original destination remains intact and the agent stops within that region; otherwise .\n-\n‚Ä¢\nScene Consistency Score (Scene Con.). Evaluates whether the environment remains static throughout the video. No objects, lighting, geometry, or layout elements may appear, disappear, deform, or shift in a way that violates the static-scene assumption. The score is if the scene stays unchanged across all frames; otherwise .\nAcross all four navigation tasks, these video-quality metrics employ a unified binary (pass/fail) definition to ensure consistent evaluation.\n9.3.4 Holistic Performance\nAcross all embodied navigation tasks, we define an Overall metric that measures end-to-end success under a strict, holistic criterion: a sample is considered correct only if all fine-grained evaluation metrics simultaneously achieve a score of . This requirement highlights a key observation about current generative models: strong performance on isolated metrics does not necessarily translate into coherent, successful task execution. The Overall score thus captures the true difficulty of producing videos that are simultaneously geometrically correct, physically plausible, visually consistent, and instruction-faithful.\nWe run both automatic VLM-based evaluation and human evaluation for Embodied Navigation task. The automatic setup uses Gemini-2.5-Pro (Comanici et al., 2025) with a structured evaluation prompt; it receives (i) the model-generated video or image and (ii) task-specific context, and returns binary metric scores and short thinking of the justifications. Human raters are given the exact same media and prompts to ensure their labels align with the same judgment criteria.\n9.4 Overall Evaluation Results\nWe evaluate the embodied navigation capabilities of state-of-the-art generators by consolidating per-task performance into four metric families: Task Completeness, Physical Understanding, Instruction Following, and the strict Holistic Overall metric. The comparative results across models are detailed in TableÀú24, while TableÀú25 provides a fine-grained human evaluation of Veo-3 to diagnose specific failure modes.\n9.4.1 Comparative Analysis of Generators\nAs detailed in TableÀú24, Nano-banana establishes a distinct lead across most axes, demonstrating superior instruction adherence and holistic accuracy. It notably surpasses holistic accuracy on both Panoramic and 3D navigation tasks. Among video models, performance is highly variable. Veo-3 generally outperforms other video generators like Sora-2 and Wan-2.2, particularly in maintaining physical plausibility (reaching in Panoramic views). However, video models often exhibit a \"completeness vs. coherence\" trade-off: while they frequently achieve decent physical understanding scores, they struggle to combine this with instruction following, resulting in significantly lower holistic scores compared to the image-based Nano-banana. GPT-4o-image generally lags behind, struggling to maintain temporal coherence across sequences.\nPerformance varies significantly by task complexity:\n-\n‚Ä¢\nPanoramic View (L.M.Nav.): Nano-banana achieves its second-highest holistic score here (). Among video models, Veo-[ADDRESS_REMOVED] ( holistic) and actually achieves the highest physical understanding score of any model (). In contrast, Sora-2 and Wan-2.2 fail to produce holistically valid trajectories (), despite moderate physical understanding.\n-\n‚Ä¢\nTop-down View (T.V.R.-W.Nav.): This task proves universally difficult, yet it is the only category where a video model outperforms Nano-banana. Veo-[ADDRESS_REMOVED] holistic accuracy (), surpassing Nano-banana () and Sora-2 (). This suggests that Veo-3 possesses superior overhead spatial reasoning capabilities compared to its peers.\n-\n‚Ä¢\n3D Real-World Navigation (3D R.-W.Nav.): This task highlights the widest gap between architecture types. Nano-banana achieves a dominant holistic score of . While Wan-2.2 () and Veo-3 () manage to complete some routes, Sora-2 fails completely (), indicating that current video generators struggle to maintain long-horizon coherence in 3D environments.\n-\n‚Ä¢\nSimultaneous Localization and Generation (SLAG): Nano-banana leads with holistic accuracy. Contrary to simpler tasks, Sora-2 () slightly outperforms Veo-3 () here, while Wan-2.[ADDRESS_REMOVED] entirely (). While video models do not fail completely (scoring on fine-grained metrics), their low holistic scores indicate a critical inability to maintain the precise cross-view alignment required for SLAG.\nTask / Model Task Completeness Physical Understanding Instruction Following Holistic Overall Panoramic View Last-Mile Navigation (L.M.Nav.) Veo-3 76.2 93.3 83.8 60.0 Sora-2 0.4 84.4 0.4 0.0 Wan2.2 25.0 71.4 39.6 0.0 Nano-banana 79.2 91.1 85.4 74.2 GPT-4o-Image 0.0 32.2 0.0 0.0 Top-down View Real-World Navigation (T.V.R.-W.Nav.) Veo-3 66.1 69.8 44.1 19.5 Sora-2 38.1 75.4 13.6 3.4 Wan2.2 32.2 66.7 22.5 5.1 Nano-banana 46.5 74.1 43.8 11.1 GPT-4o-Image 16.5 37.3 4.7 3.4 3D Real-World Navigation (3D R.-W.Nav.) Veo-3 76.7 79.4 47.1 22.5 Sora-2 15.4 76.9 11.2 0.0 Wan2.2 60.4 68.1 69.2 24.2 Nano-banana 79.9 96.8 86.8 79.2 GPT-4o-Image 14.6 63.1 16.2 13.3 Simultaneous Localization and Generation (SLAG) Veo-3 41.2 56.6 38.8 11.2 Sora-2 36.4 76.7 61.6 12.9 Wan2.2 12.9 24.0 25.8 0.8 Nano-banana 56.7 81.3 73.5 28.8 GPT-4o-Image 30.2 61.7 58.3 16.1\n9.4.2 Fine-Grained Analysis of Veo-3\nTo better understand why video models struggle despite strong visual quality, we report a detailed human evaluation of Veo-3 in TableÀú25. The results uncover a sharp disconnect between component-level physical understanding and holistic task success.\n-\n‚Ä¢\nLocal vs. Global Success: In the Panoramic setting, Veo-[ADDRESS_REMOVED] Semantic Score () and Agent Consistency Score(). However, the Overall success rate collapses to because the model rarely satisfies all validity checks simultaneously.\n-\n‚Ä¢\nThe ‚ÄúPlausible but Wrong‚Äù Problem: In 3D Real-World Navigation, Veo-3 retains high Physical Quality scores ( Agent Consistency) but fails almost entirely on Instruction Following ( Overall). The video generation looks physically realistic‚Äîobjects are stable and lighting is consistent‚Äîbut the agent fails to navigate to the correct destination.\n-\n‚Ä¢\nGeometric Failures: The SLAG task results are particularly telling. While Scene Consistency is remarkably high (), Trajectory Alignment is nearly non-existent (). This confirms that while the model can generate temporally consistent frames, it lacks the geometric grounding necessary to align those frames with a specified trajectory map.\nMetric Panoramic View Last-Mile Navigation Top-down View Real-World Navigation 3D Real-World Navigation Simultaneous Localization and Generation Task Completeness S. S. 3D 70.00 N/A 76.67 42.68 O. S. 3D 71.67 N/A 85.00 53.66 S. S. 2D) N/A 33.05 N/A 23.17 O. S. 2D) N/A 65.25 N/A 28.04 Traj. Ali. N/A N/A N/A 6.10 Physical Understanding Quality Obj. Sem. 87.50 38.14 78.33 34.15 Agent Con. 92.50 70.34 77.50 65.85 Spa. Ali. 96.67 58.47 68.33 65.85 Instruction Following Quality Des. Inte. 60.00 65.25 45.83 54.88 Scene Con. 64.17 81.36 16.67 93.90 Holistic Performance Overall 26.67 5.93 1.67 0.00\nNote: S.S. = Success Score; O.S. = Oracle Success Score; Traj. Ali. = Trajectory Alignment Score; Obj. Sem. = Object Semantic Score; Agent Con. = Agent Consistency Score; Spa. Ali. = Spatial Alignment Score; Des. Inte. = Destination Integrity Score; Scene Con. = Scene Consistency Score. ‚ÄúN/A‚Äù indicates the metric is not applicable to the specific task.\n9.5 Key Observations\nIn this section, we distill the key insights that emerge from evaluating state-of-the-art video generation models across the four embodied navigation tasks. These findings offer a high-level view of current strengths and limitations, setting the stage for deeper investigation. In the following sections‚ÄîSectionsÀú10.3, 11.3, 12.3 and 13.3‚Äîwe introduce each subtask in detail and provide comprehensive quantitative and qualitative analyses.\nFindings 1: Strong Capability in Ego-Centric Navigation Video Generation.\nOur results indicate that video generation models possess a surprisingly robust capability for third-person, off-the-shoulder perspectives navigation generation. Beyond simple frame coherence, these models demonstrate abilities that have been a long-pursuing goal in embodied navigation (Batra et al., 2020; Anderson et al., 2018a), such as the understanding of scene layouts (Li et al., 2023; Fuentes-Pacheco et al., 2015; Henriques & Vedaldi, 2018) and semantic meanings (Cartillier et al., 2021; Chaplot et al., 2020b; Zhou et al., 2025; Irshad et al., 2021; Wani et al., 2020; Blanco et al., 2008; Konolige et al., 2011; Gomez et al., 2020; An et al., 2022), effectively recognizing goal objects and maintaining contextual consistency even when the agent‚Äôs body is visible within the frame. Notably, they exhibit strong capabilities in imagining spatial relationships (Koh et al., 2021; Qin et al., 2025; Sridhar et al., 2024; Bar et al., 2025; Shah et al., 2025), allowing them to consistently model the geometric interaction between the agent and the environment. This confirms that video diffusion models implicitly learn powerful geometric priors and environment continuity, which are critical for perceiving the immediate surroundings from an embodied perspective. However, such spatial imagination does not translate into functional navigation except in the Panoramic View Last-Mile Navigation task. Only when the goal is already visible, and the required trajectory is short and locally constrained, do models like Veo-3 achieve meaningful success rates. For longer-horizon or multi-view settings, their ability to reason over distance, occlusion, or multi-step spatial transformations rapidly deteriorates, exposing the gap between ego-centric perception and actionable planning.\nFindings 2: Free-form Generation is Insufficient for Navigation.\nTasks requiring controlled movement, like Top-down View Real-World Navigation, SLAG, and Instruction-based Target Search, reveal a consistent failure pattern: video models excel at producing aesthetically coherent frames but fail to follow navigation constraints. Models such as Sora-2 often generate plausible but irrelevant motion, drift off the intended path, or remain static despite instructions. Even with perfect semantic grounding (e.g., Sora-2 achieving 87.35% Object Semantic Score), Success Scores remain near zero, underscoring that generative quality does not equal navigational utility. The inability to reliably execute directed motion suggests that current video models lack mechanisms for consistent temporal control, trajectory commitment, or adherence to spatial rules imposed by the prompt.\nFindings 3: Cross-View Spatial Alignment Exists, But Must Be Explicitly Activated.\nOur SLAG experiments reveal an unexpected finding: while trajectory alignment remains highly challenging, scene consistency improves markedly when models are evaluated in a cross-view alignment framework. In SLAG, aligning generated motion with a 2D top-down map forces the model to maintain geometric coherence across viewpoints, and this pressure appears to activate latent spatial priors that remain unused in other tasks. Veo-3, in particular, maintains high Scene Consistency even when its navigation fails, suggesting robust internal world modeling. However, this ability does not emerge in free-form video generation or text-only prompting. It requires structured multimodal conditioning, indicating that cross-view alignment is a learnable but currently underutilized capability in video models.\nAdditional Profile of Strengths and Weaknesses.\nOur results suggest a preliminary ‚Äúnavigation capability profile‚Äù for video generation models:\n-\n‚Ä¢\nStrengths: strong semantic grounding, accurate object identity preservation, coherent ego-centric spatial reasoning, and latent ability for cross-view geometric alignment.\n-\n‚Ä¢\nWeaknesses: poor rule-following under long-horizon instructions, inability to maintain goal-directed motion, hallucination under abstract destination descriptions (e.g., Veo-3 generating an entirely new scene matching the semantic description), and brittleness to environmental complexity.\nSummary.\nTaken together, these findings suggest that current video generation models possess strong ego-centric spatial perception, semantic grounding, and latent cross-view alignment capability, but they fail to reliably execute goal-driven navigation under free-form generation. Their cross-view understanding must be explicitly elicited through structured multimodal constraints such as SLAG, while long-horizon trajectory control and rule following remain largely unsolved. In the following sections, we provide detailed analyses for each navigation task, including fine-grained quantitative and qualitative breakdowns.\n[ADDRESS_REMOVED]-Mile Navigation (L.-M. Nav.)\n10.[ADDRESS_REMOVED]-Mile Navigation task evaluates fine-grained, embodied decision-making when a target is already within the agent‚Äôs visual field. Distinct from large-scale route planning, last-mile navigation isolates the critical final phase of movement: precisely localizing a visible goal and generating an optimal short-horizon trajectory toward it. In this setting, the model receives a single panoramic RGB observation and must infer a feasible motion plan leading directly to the destination. Targets are either explicitly defined by a red marker or implicitly specified via object-class descriptions, necessitating a synthesis of geometric reasoning and semantic recognition. A successful execution by Veo-3 is illustrated in FigureÀú17. Ultimately, this task probes a model‚Äôs capacity to interpret egocentric spatial layouts, estimate relative pose and depth, and execute precise actions to bridge perception and control in the final meters of navigation.\n10.2 Evaluation and Metrics\nEvaluating navigation-conditioned video generation presents a dual challenge: quantifying geometric precision while ensuring visual-semantic fidelity. Our framework explicitly assesses whether the agent executes plausible motion, maintains structural consistency, and preserves the semantic integrity of the destination. All metrics are binary (pass/fail) and come from the rollout traces and generated frames.\nWe adopt the shared embodied evaluation protocol in Section 9.3: all metrics are binary and labeled via the automatic VLM/human rules on the rollout traces and generated frames. The Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged; here we only note how task completeness metrics are used for Panoramic Last-Mile Navigation and how the gated scores are formed.\n10.2.1 Task Completeness Metrics (Geometry Only)\nThese metrics isolate navigational accuracy, evaluating whether the agent reaches the destination based purely on geometric coordinates. Visual fidelity and physical consistency are excluded here and assessed by subsequent metrics.\n-\n‚Ä¢\nSuccess Score 3D (S.S. 3D). Measures whether the final state of the agent coincides with the target location. This metric is strictly geometric; it is satisfied if the agent terminates its trajectory within the defined volume of the destination, regardless of visual preservation.\n-\n‚Ä¢\nOracle Success Score 3D (O.S. 3D). A relaxed variation of Success Score 3D that credits the agent for reaching the target at any point during the trajectory. The metric passes if the agent enters the destination‚Äôs proximity threshold at any timestep, irrespective of where it ultimately stops.\n10.2.2 Gate Metrics and Holistic Performance\nTo ensure a rigorous assessment, we introduce composite ‚ÄúGate Metrics‚Äù that intersect geometric success with semantic and physical validity. These metrics serve as strict filters, disqualifying trajectories that achieve goal conditions through generative artifacts such as hallucination or geometry warping.\n-\n‚Ä¢\nSuccess (3D) with Original Destination. This metric imposes a strict visual-geometric consistency check. It counters the tendency of generative models to exploit scene manipulation‚Äîsuch as fabricating a target at the agent‚Äôs feet or warping the room layout‚Äîto satisfy stopping conditions artificially. A trajectory is successful under this metric only if it satisfies the Success Score 3D (S.S. 3D) while simultaneously passing the Destination Integrity (Des. Inte.) and Scene Consistency (Scene Con.) checks.\n-\n‚Ä¢\nPhysics Validity. A holistic measure of physical plausibility, ensuring that movement is grounded in reality rather than dream-like transitions. Geometric success is disregarded if the agent violates fundamental physical laws. This metric requires the simultaneous satisfaction of three conditions: (1) Object Semantic Score (Obj. Sem.) (no collisions); (2) Agent Consistency Score Agent Con.) (identity persistence); and (3) Spatial Alignment Score (Spa. Ali.) (pose coherence).\n-\n‚Ä¢\nOverall Success = Success Score 3D Oracle Success Score 3D Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; a sample passes only when all seven binary checks are . This strict criterion highlights that strong performance on individual metrics does not guarantee successful, plausible end-to-end task completion.\n10.3 Evaluation Results\n10.3.1 VLM-Based Evaluation\nThe quantitative results in TableÀú[ADDRESS_REMOVED]-Mile Navigation task. The performance landscape is strictly binary: Veo-3 and Nano-banana demonstrate functional navigation capabilities, while Sora-2 and GPT-4o-image fail to complete the task entirely (0.00% Overall Success).\nTop-Tier Performance (Veo-3 vs. Nano-banana).\nWhile Veo-[ADDRESS_REMOVED], Nano-banana serves as a surprisingly robust baseline, frequently outperforming the video model in complex scenarios.\n-\n‚Ä¢\nEnvironmental Robustness: In simple environments (floor01), both models achieve parity with a 73.33% Overall Success rate. However, as environmental complexity increases (floor02plus), Veo-3‚Äôs performance degrades sharply to 46.67%, whereas Nano-banana maintains a high success rate of 75.00%.\n-\n‚Ä¢\nVisual Fidelity Scaling: A notable divergence occurs in how the models handle visual quality. Nano-banana exhibits positive scaling with fidelity, improving from 60.00% success at quality03 to 82.50% at quality05. In contrast, Veo-3 plateaus, peaking at only 62.50% regardless of increased visual fidelity.\nInstruction Following Capabilities.\nA critical differentiator found in the results is the response to destination specifications.\n-\n‚Ä¢\nVisual vs. Textual Guidance: When the destination is specified by a visual marker (color mark), both models perform comparably (70%).\n-\n‚Ä¢\nDescriptive Navigation: When the destination is defined by text instructions (location description), Veo-3 struggles, dropping to 50.00% success. Conversely, Nano-banana excels, achieving its highest success rate of 78.33%. This suggests that while Veo-3 offers superior temporal consistency (Agent Consistency >93% across most settings), Nano-banana possesses superior multimodal understanding, allowing it to map textual descriptions to visual goals more effectively.\nNavigation Failures (Sora-2 and GPT-4o-image).\nThe failing models illustrate two distinct modes of error.\n-\n‚Ä¢\nStatic vs. Dynamic Failure: GPT-4o-image achieves high Object Semantic Scores (up to 100%) but fails to generate the temporal trajectory required for navigation, resulting in a 0.00% success rate.\n-\n‚Ä¢\nSteerability Failure: Sora-2 presents a ‚Äústeerability‚Äù problem. While it maintains decent Physical Understanding (e.g., 88.33% Object Semantic Score in floor01) and generates coherent video, it lacks Destination Integrity (0.00%). The model hallucinates plausible scenes but cannot be constrained to a specific coordinate or target path.\nImpact of Trajectory Distance.\nAs anticipated, performance for the leading video model, Veo-3, degrades with trajectory length, dropping from 66.67% (Short) to 53.33% (Long). Nano-banana remains more resilient to distance, maintaining a 66.67% success rate even on long trajectories, further validating the robustness of image-based stepwise generation over holistic video generation for this specific navigation benchmark.\n10.3.2 Human vs. Automated Evaluation Discrepancy\nTableÀú27 presents a critical finding: Automatic evaluation metrics are significantly over-optimistic compared to Human Expert evaluation. This discrepancy exposes the limitations of current auto-evaluators in detecting physical and logical inconsistencies in video generation.\nThe ‚ÄúOverall Success‚Äù Collapse.\nThe most striking divergence appears in the Holistic Metric. While Auto Evaluation suggests Veo-3 is a competent navigator, human judges strongly disagree.\n-\n‚Ä¢\nBaseline Discrepancy: In the simplest setting (floor01), Auto Evaluation reports a 73.33% success rate, whereas Human Evaluation rates it at only 25.00%.\n-\n‚Ä¢\nComplexity Penalty: This gap widens in complex scenarios. For Long Trajectories, Auto claims a 53.33% success rate, while humans find that only 11.67% of the trajectories are actually successful.\nThe ‚ÄúHallucination‚Äù Blind Spot (Physics & Consistency).\nThe primary driver of the reliability gap is the auto-evaluator‚Äôs inability to penalize ‚Äúdream-like‚Äù logic violations.\n-\n‚Ä¢\nPhysics Validness: There is a massive disconnection in physical grounding. For floor01, Auto reports 81.67% validity, while humans report 25.00%. In Long Trajectories, human-rated physics validity plummets to 16.67%, indicating that over longer durations, the model frequently clips through objects, floats, or violates lighting consistencies‚Äîerrors that auto-metrics (likely based on frame-to-frame pixel similarity) fail to capture.\n-\n‚Ä¢\nScene Stability: Auto-evaluators consistently overrate Scene Consistency (e.g., 98.33% for floor01). In contrast, humans rate it at 66.67%. This confirms that auto-metrics struggle to detect subtle temporal morphing of walls, textures, or landmarks that humans immediately recognize as inconsistent. Notably, for Location Description tasks, Scene Consistency drops to 48.33% in human eyes, suggesting that text-conditioning may induce greater visual instability than visual prompting.\nTask Completion vs. Reality.\nEven when the agent appears to move correctly, it often fails to satisfy the exact conditions of the destination. Auto-metrics (Destination Integrity) suggest the agent reaches the target 90.00% of the time in simple settings. Humans, however, judge this at only 55.00%, indicating that the agent often stops short, overshoots, or faces the wrong direction upon arrival‚Äînuances the auto-evaluator misses.\nAreas of Agreement: The Camera Motion Paradox.\nInterestingly, Spatial Alignment is the only metric where humans frequently rate the model higher than the automatic system. For Long Trajectories, humans rated Spatial Alignment at 98.33%, exceeding the Auto rating of 95.00%. This suggests that the camera movement itself (the ‚Äúego-motion‚Äù) is highly convincing to human observers. The failure of Veo-3 is not in moving like an agent, but in maintaining the world around the agent. The motion is realistic; the environment is not.\nConclusion.\nWhile Veo-3 demonstrates state-of-the-art potential in generating semantically correct paths, the stark contrast between the 73.33% (Auto) and 25.00% (Human) success rates serves as a warning. Current automatic metrics for video generation prioritize visual similarity over physical logic, making them insufficient proxies for measuring true ‚ÄúWorld Modeling‚Äù capabilities.\n10.4 Case Study: Qualitative Failure Modes\nThis case study, visualized in FigureÀú18, evaluates the ‚Äúworld modeling‚Äù capabilities of video generation models during a last-mile navigation task. In this scenario, a humanoid robot must traverse a 3D environment to reach a specific target marked on the floor. The analysis reveals that while models can generate plausible motion, they suffer from distinct breakdowns in object permanence, physical laws, and instruction grounding.\nVeo-3 (Temporal Identity Collapse).\nInitially, Veo-3 demonstrates strong spatial reasoning. The agent successfully perceives a pillar as a solid obstacle and navigates around it, exhibiting valid collision avoidance. However, the generation fails critically in Agent Consistency at the 5-second mark. The model suffers from a ‚Äúdoppelg√§nger‚Äù hallucination, where a second agent spontaneously manifests and approaches the camera while the initial agent retreats. This loss of agent identity results in an Oracle Success Score of 0, as the intended actor never completes the trajectory.\nSora-2 (Semantic and Physical Drift).\nSora-2 struggles to maintain the fundamental constraints of the scene. By Frame 2s, it exhibits severe semantic drift, abruptly shifting the visual style and ungrounding the instruction: the model incorrectly remaps the red target marker from the floor to a vertical wall. Furthermore, the physics engine collapses; the agent is observed floating on a non-existent ‚Äúair platform‚Äù and subsequently clipping through a solid railing. This confirms that Sora-2 prioritizes visual fluidity over collision geometry or logical consistency.\nWan-2.2 (Environmental Instability).\nIn contrast to the others, Wan-2.2 maintains high Agent Consistency, producing smooth, continuous motion without teleportation or scaling artifacts. However, it fails to maintain the ‚Äústage‚Äù around the actor. As the camera angle shifts (Frame 2s), the model suffers from background object permanence failures, inexplicably erasing a glass door and hallucinating new pillars within the room. While it preserves Destination Integrity better than Sora-2 (keeping the target relative to the agent), the volatility of the environment ultimately prevents successful navigation.\n11 Top-down View Real-World Navigation (T.V.R.-W.Nav.)\n11.1 Task Definition\nThe Top-down Real-World Navigation task benchmarks a model‚Äôs ability to synthesize advanced spatial reasoning with semantic understanding in complex, human-centric environments. This task requires models to interpret diverse and partially occluded top-down layouts‚Äîsuch as floor plans‚Äîto generate optimal trajectories. Simultaneously, it tests robust semantic grounding by demanding goal identification based on textual descriptions of varying abstraction.\n11.2 Evaluation and Metrics\nWe adopt the shared embodied evaluation protocol in Section 9.3: Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged. Here we only note how task completeness metrics are used for Top-down View Real-World Navigation and how the gated scores are formed.\n11.2.1 Task Completeness Metrics (Geometry Only)\nThese metrics isolate navigational accuracy, evaluating whether the agent reaches the destination based purely on geometric coordinates. Visual fidelity and physical consistency are excluded here and assessed by subsequent metrics.\n-\n‚Ä¢\nSuccess Score 2D (S.S. 2D). This metric evaluates whether the agent terminates its trajectory strictly within the designated goal region in the top-down 2D view. Success is achieved if and only if the agent‚Äôs final position is contained entirely within the goal footprint.\n-\n‚Ä¢\nOracle Success Score 2D (O.S. 2D). This metric determines if the agent encounters the goal region at any point during the rollout, regardless of the stopping location. Credit is awarded if the agent‚Äôs trajectory intersects with or passes adjacent to the 2D goal boundary at any timestep.\n11.2.[ADDRESS_REMOVED]-Mile Navigation, gates guard against generative shortcuts, but here the challenge comes from the 2D top-down view: success must hold on a static map, and Physical Validness is stricter because object recognition and agent orientation are blurrier from overhead.\n-\n‚Ä¢\nSuccess (2D) Original Destination. This composite metric mitigates the tendency of video generation models to hallucinate solution shortcuts. It is satisfied if and only if the Success Score 2D, Destination Integrity Score, and Scene Consistency Score are met simultaneously. This ensures that successful arrival is valid and not the result of the model altering the scene layout or destination coordinates to simplify the task.\n-\n‚Ä¢\nPhysical Validity. To enforce physical realism, this composite metric acts as a strict consistency check. It is satisfied only when the Object Semantic Score, Agent Consistency Score, and Spatial Alignment Score are simultaneously met. This precludes physical anomalies such as object clipping, teleportation, or unnatural drift.\n-\n‚Ä¢\nOverall Success = Success Score 2D Oracle Success Score 2D Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; a sample passes only when all seven binary checks are .\nFinally, across all embodied navigation tasks, we introduce the Overall metric to evaluate holistic performance. A sample is considered successful only if all fine-grained evaluation metrics are satisfied. This strict criterion demonstrates that strong performance on individual sub-metrics does not guarantee successful end-to-end task completion.\n11.3 Evaluation Results\nThe Top-down Real-World Navigation task evaluates a model‚Äôs ability to synthesize spatial planning with semantic understanding. The following analysis dissects model performance across environmental complexities, instruction modalities, and visual fidelities, followed by a critical examination of automatic metric reliability against human judgment.\n11.3.1 VLM-Based Evaluation\nVideo vs. Image-based Navigation.\nTableÀú28 establishes Veo-3 as the state-of-the-art model for this benchmark, consistently outperforming peers in navigation success and trajectory adherence.\n-\n‚Ä¢\nThe Temporal Advantage: In the baseline setting (floor01), Veo-3 achieves a Success Score (2D) of 65.71%, significantly surpassing the image-based Nano-banana (41.67%) and the video-based Sora-2 (22.81%). This advantage stems from Veo-3‚Äôs superior Spatial Alignment Score (91.43%), indicating that video generation models with strong temporal attention can better maintain trajectory coherence than frame-by-frame image generators.\n-\n‚Ä¢\nThe Semantic Trade-off: Interestingly, while Nano-banana lags in navigation, it remains highly competitive in static recognition. It achieves an Object Semantic Score of 88.89% in simple environments‚Äîsurpassing Veo-3 (77.14%) and Sora-2 (82.46%). This suggests that current image models excel at ‚Äúwhat‚Äù is in the scene (grounding), while video models excel at ‚Äúhow‚Äù to move through it (dynamics).\nResilience to Complexity and Noise.\n-\n‚Ä¢\nEnvironmental Complexity: Performance degrades universally as environments become more intricate. Transitioning from floor01 to floor02plus, Veo-3‚Äôs Overall Success drops sharply from 37.14% to 13.89%. However, its Oracle Success Score remains robust (83.33%), implying that the model often identifies the correct path but fails to execute it without violating physical constraints (e.g., collisions), a hypothesis supported by the low Physics Validness (33.33%).\n-\n‚Ä¢\nView Fidelity: The results indicate a positive correlation between visual quality and navigation success for video models. Veo-3‚Äôs Overall Success improves from 20.83% in lower fidelity settings (quality03) to 29.17% in higher fidelity (quality04), suggesting that clearer visual cues are critical for maintaining the ‚Äúdriver‚Äù capability in video generation.\nInstruction Following: The Modality Gap.\nA distinct performance gap exists between visual and textual grounding (see Destination Specification).\n-\n‚Ä¢\nVisual Markers: When targets are specified via a simple ‚Äúcolor mark,‚Äù Veo-[ADDRESS_REMOVED] Overall Success of 38.89%.\n-\n‚Ä¢\nTextual Descriptions: When targets require semantic parsing (‚Äúlocation description‚Äù), success rates collapse. Veo-3 falls to 11.43%, and Nano-banana drops to 8.33%. This confirms that mapping abstract linguistic descriptions to spatial layouts remains a significant hurdle compared to direct visual matching.\n11.3.2 Human vs. Automatic Evaluation Discrepancy\nTo validate the automatic benchmarking protocols, we conducted a side-by-side comparison with human evaluators for the top-performing model, Veo-3 (TableÀú29). The data reveals critical divergences in how algorithms versus humans perceive ‚Äúsuccess.‚Äù\nThe Physics Hallucination Problem.\nAutomatic metrics consistently overestimate physical realism. In floor01, the Auto Metric reports a Physics Validness of 54.29%, while humans rate it at only 22.41%. This discrepancy leads to a massive inflation of the Overall Success metric in automatic evaluations (37.14%) compared to the human ground truth (10.34%). This indicates that while models satisfy geometric path constraints (high Spatial Alignment), they frequently hallucinate physically impossible traversals (e.g., clipping through walls) that simple 2D projection metrics fail to penalize.\nThe Consistency Paradox.\nConversely, automatic metrics appear overly punitive regarding visual consistency.\n-\n‚Ä¢\nAgent Consistency: Humans rated Veo-3‚Äôs agent consistency in floor01 at a near-perfect 96.55%, whereas the automatic scorer only awarded 62.86%.\n-\n‚Ä¢\nScene Consistency: Similarly, humans perceived the environment as stable (79.31%), significantly higher than the algorithmic assessment (45.71%).\nThis suggests that current computer vision metrics for temporal consistency (likely based on pixel-wise or feature-wise similarity) are sensitive to minor generative artifacts that human observers naturally filter out as temporally coherent motion.\nGrounding Reliability.\nThe evaluation gap widens with task abstractness. For simple ‚Äúcolor mark‚Äù destinations, the Auto and Human Success Scores (2D) are relatively close (52.78% vs. 48.28%). However, for ‚Äúlocation descriptions,‚Äù the auto metric claims a 37.14% success rate while humans find only 18.33% of trajectories successful. This implies that automatic evaluators are prone to false positives when validating complex semantic grounding, likely crediting trajectories that end near the target by chance rather than by understanding the textual clue.\n11.4 Case Study: Qualitative Failure Modes\nThis analysis, presented in FigureÀú20, examines the performance of three video generation models‚ÄîVeo-3, Sora-2, and Wan-2.2‚Äîon a top-down embodied navigation task. Each model is instructed to generate a video depicting an agent navigating from a blue triangular start position to a red target within a static environment, enabling a direct comparison of their spatial reasoning, trajectory consistency, and goal-directed behavior.\nVeo-3: Strong Spatial Grounding with Control and Termination Failures. Veo-[ADDRESS_REMOVED] initial spatial grounding among the three models, correctly identifying the agent‚Äôs starting position at the blue triangle and demonstrating partial semantic awareness of obstacles. In particular, the agent recognizes the sofa as a navigable obstruction and initially routes around it. However, the generation deteriorates due to failures in movement physics and action termination. The agent displays a critical orientation error, moving laterally to the left while facing forward, which results in a Spatial Alignment Score of 0 at Frame 5s. Although the agent briefly enters the red-marked target region (Oracle Success Score = 1), it fails to terminate upon arrival. Instead, the trajectory continues beyond the goal, deviates unpredictably, and ultimately collides with the sofa. This post-arrival drift causes both the Success Score and Object Semantic Score to drop to 0, revealing a failure to couple goal completion with action cessation.\nSora-2: Severe Scene Hallucination and Physical Constraint Violations. Sora-2 fails to preserve both the spatial ground truth and the physical constraints of the input environment. From the outset, the model reconstructs an entirely new scene in which the agent no longer originates from the blue start location. Temporal consistency further collapses as the generation introduces severe hallucinations, including the spontaneous appearance of a second agent at Frame 4s. The agent also violates fundamental physical boundaries by passing through solid obstacles (the black block), yielding an Object Semantic Score of 0. While an agent eventually reaches a red-marked region, the complete loss of scene integrity and environmental correspondence renders the navigation invalid with respect to the original prompt, highlighting a failure in maintaining world-state continuity.\nWan-2.2: Surface-Level Scene Consistency with Semantic Role Confusion. Wan-2.2 preserves the visual style and low-level layout of the scene more effectively than Sora-2, achieving an initial Scene Consistency Score of 1. However, this apparent stability masks a deeper semantic failure. The model misidentifies the controllable agent, erroneously animating a dining table as the acting entity, which immediately induces spatial misalignment. The resulting motion is disjointed and drifting, lacking coherent locomotion or intentional control. Compounding this issue, the model fails to maintain destination integrity: the red target region shifts from its original floor location to a corner of the scene, causing the Destination Integrity Score to drop to 0. As a result, the agent never reaches the correct target, demonstrating that visual fidelity alone is insufficient for semantically grounded navigation.\n12 3D Real-World Navigation (3D R.-W.Nav.)\n12.1 Task Definition\nThis task evaluates an agent‚Äôs foundational capabilities in 3D Spatial Reasoning and Visual-Semantic Grounding within real-world scanned environments (Chang et al., 2017; Ramakrishnan et al., 2021b; Savva et al., 2019; Anderson et al., 2018b; Zhu et al., 2017). It probes the agent‚Äôs ability to interpret egocentric visual streams and parse complex 3D environmental geometry using datasets such as Matterport3D, HM3D, and Habitat (Chang et al., 2017; Ramakrishnan et al., 2021b, a; Chaplot et al., 2020a). Furthermore, the task challenges the model‚Äôs sequential decision-making by requiring the generation of valid action trajectories (e.g., move forward, turn left) and tests its semantic reasoning by demanding the identification of goals based on varied abstract or textual descriptions.\n12.2 Evaluation and Metrics\nWe adopt the shared embodied evaluation protocol in Section 9.3: all metrics are binary and labeled via the automatic VLM/human rules on rollout traces and generated frames. Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged; here we outline how they are applied in 3D Real-World Navigation and how the gates are formed.\n12.2.1 Task Completeness Metrics (Geometry Only)\nWe report Success Score 3D and Oracle Success Score 3D as in Section 9.3, testing whether the agent stops inside or ever enters the destination volume. These ignore visual fidelity and physical plausibility, which the gate metrics enforce.\n-\n‚Ä¢\nSuccess Score 3D (S.S. 3D). This checks whether the agent stops at the correct location in the 3D navigation sequence. The evaluation is geometric only and does not depend on whether the destination appearance is preserved. It is satisfied if the agent stops inside the intended destination volume.\n-\n‚Ä¢\nOracle Success Score 3D (O.S. 3D). This credits the agent if it ever comes within a proximity threshold of the correct 3D destination, regardless of its final stop. It is satisfied if the agent reaches the destination vicinity at any time during the video.\n12.2.2 Gate Metrics and Holistic Performance\nIn 3D R.-W.Nav., the gates surface common failure modes: agents may ‚Äúteleport‚Äù between floors, cut through occluded rooms, or alter the dollhouse layout to satisfy geometric hits. Requiring destination integrity and scene consistency prevents shortcutting via hallucinated staircases or duplicated goals, while the physics gate catches identity drift and implausible motion in the third-person view.\nAs with the Panoramic and Top-down tasks, strict conjunctions block generative shortcuts (e.g., hallucinating nearer goals or warping 3D structure), but the 3D setting stresses multi-floor occlusions and third-person viewpoint drift:\n-\n‚Ä¢\nSuccess (3D) Original Destination. This composite metric addresses the tendency of video generation models to hallucinate solution shortcuts. It is satisfied only if the Success Score 3D, Destination Integrity Score, and Scene Consistency Score are all met simultaneously. This metric ensures that a successful arrival is valid and not the result of the model altering the scene layout or destination location to simplify the task.\n-\n‚Ä¢\nPhysics Validness. To provide a holistic view of physical understanding, this composite metric serves as a strict gate. It is satisfied only if the Object Semantic Score, Agent Consistency Score, and Spatial Alignment Score are all met simultaneously. This ensures the agent does not clip through objects, teleport, or drift unnaturally.\n-\n‚Ä¢\nOverall Success = Success Score 3D Oracle Success Score 3D Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; a sample passes only when all seven binary checks are .\n12.3 Evaluation Results\n12.3.1 VLM-Based Evaluation\nTableÀú30 benchmarks Veo-[ADDRESS_REMOVED] Sora-2, Nano-banana, and GPT-4o-image. The results highlight a distinct hierarchy in embodied navigation capabilities, separated by model modality and architectural stability.\nImage Models vs. Video Models (Nano-banana Dominance).\nNano-banana emerges as the distinct leader, showcasing that image generation is currently far more reliable for embodied tasks than video generation.\n-\n‚Ä¢\nConsistency as a Foundation: Nano-banana maintains near-perfect scores in Scene Consistency (86‚Äì100%) and Physics Validness (94‚Äì100%) across all complexity levels. This stability allows it to achieve high Overall Success rates (75‚Äì87.5%) that video models cannot approach.\n-\n‚Ä¢\nResilience to Complexity: As environment complexity increases (from floor01 to floor02plus), Nano-banana actually improves its overall success from 75% to 83.33%. In sharp contrast, video models collapse; Veo-3‚Äôs holistic success drops to 0% in complex multi-floor environments, unable to reconcile geometric consistency with longer navigation horizons.\nVideo Model Trade-offs (Veo-3 vs. Sora-2).\nWhile both video models struggle with holistic success (mostly 0%‚Äì3%), they exhibit different failure modes:\n-\n‚Ä¢\nSora-2 (Better Geometry, Worse Adherence): Sora-2 generally outperforms Veo-3 in Spatial Alignment (e.g., 97.22% vs 63.33% in floor01) and Scene Consistency (44.44% vs 15.00%). However, it suffers from a critical flaw in Gate Metric: Success (Original Destination), scoring 0.00% across almost all categories. This indicates that while Sora-2 generates smooth, consistent video, it hallucinates the destination or drifts significantly from the prompt‚Äôs specific target.\n-\n‚Ä¢\nVeo-3 (Better Adherence, Worse Physics): Veo-3 is more ‚Äúcompliant‚Äù with the task, scoring higher on reaching the original destination (11.67% in floor01). However, it achieves this by sacrificing physical laws, as evidenced by its abysmal Scene Consistency scores (dropping to 5.00% in location tasks) and heavy penalization in human evaluations for physics violations.\nBaseline Comparison.\nGPT-4o-image acts as a semantic baseline. While it demonstrates strong object recognition (Object Semantic Score 80‚Äì90%), it lacks the spatial reasoning to navigate, resulting in low success scores ( 13‚Äì17%). This confirms that embodied navigation requires more than just ‚Äúseeing‚Äù the scene; it requires a consistent internal world model that GPT-4o-image lacks.\n12.3.2 Human vs. Automated Evaluation Discrepancy\nTableÀú31 exposes a systemic bias in current automated benchmarking. While automated metrics suggest that Veo-3 possesses moderate embodied competency, human evaluation reveals a ‚Äúcompetence illusion,‚Äù where high task completion rates mask fundamental failures in physical realism.\nThe ‚ÄúPlausibility Gap‚Äù in Auto-Eval.\nAutomated metrics exhibit severe inflation regarding agent reliability.\n-\n‚Ä¢\nMetric Collapse: In the floor01 environment, the Auto Evaluation reports a respectable Holistic Metric (Overall Success) of 25.00%. Human evaluators, however, penalize the model rigorously, causing this metric to collapse to just 3.33%.\n-\n‚Ä¢\nSuperficial Success: Crucially, the raw Task Completeness (Success) scores are nearly identical between machines (88.89%) and humans (85.00%). This discrepancy proves that auto-evaluators correctly identify destination arrival but completely fail to penalize the impossible trajectories used to get there.\nBlindness to Physics and Scene Integrity.\nThe divergence stems from the auto-evaluator‚Äôs inability to detect violations of physical laws, creating a ‚Äúphysics-blind‚Äù assessment loop.\n-\n‚Ä¢\nScene Consistency: Human evaluators rate Scene Consistency drastically lower than the auto-evaluator (e.g., 15.00% vs. 44.44% in floor01). This indicates that the model frequently warps the environment‚Äîshifting walls, deleting obstacles, or altering lighting‚Äîto facilitate navigation, a behavior the auto-evaluator ignores.\n-\n‚Ä¢\nThe Validity Gate: While the auto-evaluator estimates Physics Validness at 50‚Äì60%, humans rate it between 30‚Äì40%. Because Overall Success is a gated metric (requiring both arrival and valid physics), this 20-point drop in validity effectively zeroes out the holistic success rate in human trials.\nOverall Metric Analysis: The Consistency Bottleneck.\nThe holistic data reveals a clear ‚ÄúFragile Success‚Äù paradox. While the model achieves a high peak performance in isolation‚Äîscoring 80.56% in Success Score (3D)‚Äîit fails to integrate these capabilities into a coherent simulation.\n-\n‚Ä¢\nThe 69% Drop: Only 20.83% of samples satisfy the strict holistic criterion, representing a massive 69.45% degradation relative to the model‚Äôs best individual metric (Spatial Alignment, 90.28%).\n-\n‚Ä¢\nRoot Cause: The dominant limiting factor is Scene Consistency, which remains dangerously low at 40.28%. This confirms that the model is better at moving the agent than it is at maintaining the world.\n-\n‚Ä¢\nImplication: Successful trajectories are frequently invalidated by environmental drift (e.g., vanishing objects, morphing geometry). This underscores that for high-quality motion generation is futile without the temporal stability required to keep the environment static.\n12.4 Case Study: Qualitative Failure Modes\nThis case study in FigureÀú22 evaluates the ability of three video generation models‚ÄîVeo-3, Sora-2, and Wan-2.2‚Äîto simulate a humanoid robot navigating a multi-story indoor space to a specific red target zone. The prompt requires strict adherence to physical constraints, step-by-step stair climbing, and a consistent ‚Äúdollhouse‚Äù isometric view.\nVeo-3 Analysis: Task Success with Physical Compromises. Veo-3 was the only model to achieve a successful arrival at the destination (Oracle Success Score = 1). However, this success came with significant hallucinations and physics violations:\n-\n‚Ä¢\nScene Consistency: The model failed to maintain the structural integrity of the input scene, notably hallucinating a straight staircase in place of the original curved one.\n-\n‚Ä¢\nPhysical Fidelity: While the agent initially demonstrated understanding of vertical movement, it ultimately failed the physics constraint by jumping directly from the second floor to the ground floor, resulting in the body clipping into the floor. Despite these errors, the agent maintained consistency without teleporting earlier in the sequence.\nSora-2 Analysis: Severe Hallucination and Physics Failures. Sora-2 struggled significantly with both scene adherence and physical laws:\n-\n‚Ä¢\nStyle and Destination Drift: The model completely disregarded the input image, creating a new scene with a different video style. Crucially, it altered the task parameters by moving the red destination marker from the floor to the wall.\n-\n‚Ä¢\nPhysics Violations: The agent displayed zero understanding of solid geometry (Object Semantic Score = 0), standing on ‚Äúair‚Äù platforms near the stairs and walking directly through the railing. Consequently, the agent never arrived at the valid destination.\nWan-2.2 Analysis: Temporal Instability and Agent Drifting. Wan-2.2 initially maintained the scene details well but quickly devolved into temporal incoherence:\n-\n‚Ä¢\nEnvironmental Instability: The static environment proved unstable; by Frame 1s, furniture (a table) shifted position from right to left.\n-\n‚Ä¢\nAgent Control: The navigation was disjointed. The agent drifted, stood on top of tables, and walked backwards down the stairs. This resulted in a failure to reach the destination.\nWhile Veo-3 was the only model to ‚Äúcomplete‚Äù the navigation task, all three models struggled with strict physical plausibility in a 3D space. Veo-3 prioritized path completion over geometry; Sora-2 hallucinated an entirely new reality; and Wan-2.2 failed to keep the static environment stationary.\n13 Simultaneous Localization and Generation (SLAG)\n13.1 Task Definition\nThe Simultaneous Localization and Generation (SLAG) task extends the paradigm of Simultaneous Localization and Mapping (SLAM) Durrant-Whyte & Bailey (2006) by coupling 3D spatial navigation with real-time generative mapping. Unlike traditional methods that estimate position within a static or progressively built map, SLAG actively synthesizes a synchronized 2D top-down trajectory that corresponds to the robot‚Äôs movement through a 3D environment. As a humanoid robot navigates complex indoor scenes‚Äîsuch as photorealistic apartments rendered in a ‚Äúdollhouse‚Äù perspective‚Äîthe system dynamically generates a 2D representation of its progress from start to goal. The core objective of SLAG is to achieve precise spatiotemporal alignment between physical 3D navigation and generative 2D plotting, demonstrating the synergy between real-time perception and generative modeling in spatial understanding.\n13.2 Evaluation and Metrics\nSLAG differs from the previous tasks by requiring simultaneous 3D navigation and 2D generative mapping, making trajectory alignment the primary bottleneck. Similarly, we adopt the shared embodied evaluation protocol in Section 9.3. Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged. Here we focus on the SLAG-specific task completeness set and gates.\n13.2.1 Task Completeness Metrics (Geometry Only)\nFive task completeness metrics are evaluated jointly: Success Score 2D, Oracle Success Score 2D, Success Score 3D, Oracle Success Score 3D (all from Section 9.3), and Trajectory Alignment to ensure the projected 2D path matches the 3D motion. The 2D map is provided, but the destination is not; alignment hinges on correctly projecting the 3D navigation onto the given map, with 2D success conditioned on the same 3D destination.\n-\n‚Ä¢\nSuccess Score (S.S. 2D). Measures whether the agent‚Äôs final position lies within the highlighted or textually specified goal region in the 2D overhead map. The score is if the final coordinates fall entirely inside the goal footprint; otherwise .\n-\n‚Ä¢\nOracle Success Score (O.S. 2D). Provides partial credit when the agent comes sufficiently close to the 2D goal during navigation. The score is if the agent‚Äôs path ever intersects or touches the goal region, even if it does not stop there; otherwise .\n-\n‚Ä¢\nSuccess Score (S.S. 3D). Checks whether the agent ends inside the correct destination volume in the 3D navigation sequence. This metric is purely geometric and independent of any visual discrepancies at the destination. The score is if the final 3D position is within the target volume; otherwise .\n-\n‚Ä¢\nOracle Success Score (O.S. 3D). Grants credit when the agent enters the vicinity of the correct 3D destination at any point during its rollout. The score is if the trajectory ever crosses the predefined proximity threshold around the target; otherwise .\n-\n‚Ä¢\nTrajectory Alignment Score. Evaluates whether the agent‚Äôs 2D projected route is consistent with its 3D motion path, focusing on major turns and spatial transitions. A score of indicates strong correspondence between the two trajectories; otherwise .\n13.2.2 Gate Metrics and Holistic Performance\nAs in the prior tasks, gates block generative shortcuts, but only a 3D destination gate is possible because the 2D map has no fixed ground-truth target (it is generated, not given):\nIn SLAG, the holistic gate exposes compounded failures: models often align one panel but not the other, hallucinate extra corridors in the 2D drawing, or ‚Äúteleport‚Äù in 3D while the 2D trace stays smooth. Because the 2D goal is synthesized, the 3D destination gate plus trajectory alignment are key to preventing such mismatches.\n-\n‚Ä¢\nSuccess (3D) with Original Destination = Success Score 3D Destination Integrity Scene Consistency.\n-\n‚Ä¢\nPhysics Validness = Object Semantic Agent Consistency Spatial Alignment.\n-\n‚Ä¢\nOverall Success = Success Score 2D Oracle Success Score 2D Success Score 3D Oracle Success Score 3D Trajectory Alignment Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; a sample passes only when all ten binary checks are .\n13.3 Evaluation Results\nThe Simultaneous Localization and Generation (SLAG) task represents a significant leap in embodied AI, requiring models to maintain strict temporal and spatial alignment between 3D physical navigation and 2D generative mapping. The following analysis dissects the performance of state-of-the-art models, revealing a critical trade-off between the visual fluidity of video models and the logical adherence of image-based models.\n13.3.1 VLM-Based Evaluation\nTableÀú32 highlights the performance disparities across Veo-3, Sora-2, Nano-banana, and GPT-4o-image. While video generation models often excel at visual fidelity, the requirement for synchronized spatial logic proves challenging.\nThe Logic vs. Motion Trade-off.\nQuantitative evaluation again ranks Nano-banana as the strongest SLAG performer: it posts the top Holistic Metric on floor01 (27.78%), edging out GPT-4o-image (25.00%), while Veo-3 and Sora-2 remain far lower (11.86% and 10.34%).\n-\n‚Ä¢\nTrajectory Alignment Dominance: Nano-banana achieves a dominant Trajectory Alignment Score of 88.89% on floor01. Video baselines trail sharply (Veo-3: 44.07%, Sora-2: 55.17%), underscoring that smooth motion does not guarantee coordinate-level alignment with the map.\n-\n‚Ä¢\nGate-Induced Failures for Video Models: Despite respectable physical validity (Physics Validness: Veo-3 at 45.76%, Sora-2 at 65.52%), the Success (3D) Original Destination gate collapses to 18.64% and 22.41%, respectively, versus Nano-banana‚Äôs 38.89%. Low destination integrity (20.34%‚Äì32.76% for video models) is the primary culprit behind their depressed holistic scores.\nThe Grounding Gap: Visual vs. Linguistic Complexity.\nPerformance across instruction types shows that linguistic grounding is a steeper barrier than visual grounding.\n-\n‚Ä¢\nDrastic Drop on Text Prompts: When the destination is specified by a Color Mark, Nano-banana excels (81.25% 3D Success, 50.00% Holistic). Switching to a Location Description collapses 3D Success to 17.65% and Holistic to 8.82%.\n-\n‚Ä¢\nModel Consistency: GPT-4o-image mirrors this trend, sliding from 44.83% to 3.33% (3D Success) and from 31.03% to 1.67% (Holistic). Video models show the same gap (Veo-3: 17.24% 5.17% Holistic; Sora-2: 19.64% 6.67%), confirming that textual spatial instructions remain the weakest link.\nEnvironmental Scaling.\nAs complexity increases from floor01 to floor02plus:\n-\n‚Ä¢\nNano-banana loses geometric reliability (3D Success 55.56% 40.00%) yet nudges its Holistic Metric upward (27.78% 30.00%) thanks to stable Scene Consistency (100.00% 96.67%) and stronger 2D alignment.\n-\n‚Ä¢\nSora-2 remains limited (Holistic 10.34% 15.52%) despite high physical priors (Physics Validness 65.52% 56.90%), indicating that added floors do not resolve its instruction-following issues.\n-\n‚Ä¢\nGPT-4o-image degrades sharply (Holistic 25.00% 6.90%), revealing brittleness of single-image rollouts once multi-floor reasoning is required.\n13.3.2 Human vs. Automated Evaluation Discrepancy\nTableÀú33 presents a critical divergence between automatic metrics and human judgment regarding the Veo-3 model. This comparison exposes the ‚ÄúUncanny Valley‚Äù of physics simulation in current video models.\nThe ‚ÄúHallucination‚Äù of Quality: Scene Consistency.\nHumans consider the videos visually stable (98.15% on floor01, 100% on quality04), yet auto-metrics flag instability (Scene Consistency 45.76% on floor01). Veo-3 produces temporally smooth, coherent frames to the eye, but algorithms detect subtle pixel/geometry shifts that humans overlook.\nPhysics Validness: Humans Are Harsher.\nOn floor01, the auto-evaluator assigns Physics Validness of 45.76%, while humans give just 5.56%. Simple collision or alignment checks let runs pass automatically, but humans catch ‚Äúsoft‚Äù violations (teleportation, clipping, floating), driving the human Holistic Metric to 0.00%.\nTrajectory Alignment Reality Check.\nAlignment between the 2D map and 3D view is sharply contested: Auto scores 44.07% on floor01, whereas humans rate only 9.26%. The map may look structurally plausible to an algorithm yet fails to reflect the exact turns and timing seen in the 3D video.\nOverall Metric Analysis.\nSLAG shows a severe ‚Äúwood barrel‚Äù effect: overall success collapses to 3.64% (2/55 samples) because weak links‚ÄîDestination Integrity (25.45%), Trajectory Alignment (29.09%), Physics Validness‚Äîgate everything else. Image models (Nano-banana) occasionally assemble logically consistent frames, but video models (Veo-3, Sora-2) remain ‚Äúdreamers,‚Äù generating visually compelling yet spatially and physically unreliable navigations.\n13.4 Case Study: Qualitative Failure Modes\nThe Simultaneous Localization and Generation task highlights a common failure mode across all three models in Figure 24: the inability to maintain semantic linkage between split-screen representations over time.\nVeo-3: High Fidelity, Low Logic.\nVeo-[ADDRESS_REMOVED] stable visual experience, maintaining high Scene Consistency as confirmed by human evaluation (98.15%). In the case study, the agent correctly identifies the start position and successfully reaches the red-marked goal in the 3D view. However, the Trajectory Alignment fails. As the 3D agent moves forward, the 2D dot on the map drifts independently, failing to mirror the agent‚Äôs path. This exemplifies the ‚Äúdisembodied‚Äù nature of the generation‚Äîthe model understands it needs to generate a map and a video, but treats them as separate artistic tasks rather than coupled data streams.\nSora-2: Severe Hallucination.\nSora-2 struggles to maintain the input reality. (1) Scene Hallucination: It immediately fails Scene Consistency () by replacing the input floor plan with a hallucinated layout. (2) Physics Violations: Consistent with the low Physics Validness scores, the agent is observed clipping through a cabinet.(3) Phantom Turns: The 2D agent executes a turn that the 3D agent never makes, further emphasizing the lack of cross-modal attention in the model architecture.\nWan-2.2: The Teleportation Problem.\nWan-2.[ADDRESS_REMOVED] erratic behavior, leading to complete task failure. (1) Agent Instability: The model suffers from severe Agent Consistency failures (). At Frame 1s, the agent teleports‚Äîeffectively ‚Äúflying‚Äù‚Äîonto an indigo wall, treating a vertical surface as a walkable floor. (2) Navigation Failure: Unlike Veo-3, Wan-2.2 fails to reach the destination in either view. The agent stops short, resulting in zero scores for both Success (2D) and Success (3D). This model demonstrates that without strong physical priors, video generation models devolve into surrealism rather than embodied simulation.\n14 Physical Commonsense\nWe introduce the Physical Commonsense task to assess a model‚Äôs foundational understanding of ‚Äúintuitive physics‚Äù (Battaglia et al., 2013; Yi et al., 2020; Wu et al., 2015), a critical prerequisite for robust world modeling. This task probes Physical Reasoning by requiring models to generate videos that combine photorealism with physical plausibility (Bear et al., 2021; Riochet et al., 2021; Piloto et al., 2022). Beyond static visual fidelity, the task evaluates whether the model captures essential principles‚Äîsuch as gravity, momentum, collisions, and material properties (rigidity, fluid dynamics)‚Äîand correctly models causal dynamics (Bakhtin et al., 2019; Allen et al., 2020). The evaluation spans both 3D Spatial Reasoning (spatial object interactions) and Temporal Reasoning (sequential cause-and-effect), organized along two complementary axes: (1) Physical Concepts, which tests fundamental principles using the VideoPhy ontology (Bansal et al., 2024); and (2) Sports Scenarios, which probe compositional reasoning through dynamic, high-velocity human movements.\n14.1 Data Sources and Task Structure\nPhysical Concepts (Fundamental Interactions).\nTo systematically evaluate atomic physical principles, we adopt the structured ontology from VideoPhy (Bansal et al., 2024) and VideoPhy-2 (Bansal et al., 2025). We draw from a diverse pool of captioned interactions, including: Solid‚ÄìSolid (e.g., rigid collisions, stacking), Solid‚ÄìFluid (e.g., splashing, buoyancy), and Fluid‚ÄìFluid (e.g., diffusion, mixing). These prompts isolate specific physical laws, allowing us to test statics, dynamics, and kinematics in controlled environments.\nSports Scenarios (Compositional Reasoning).\nTo evaluate physical reasoning in complex, real-world contexts, we synthesize a complementary dataset of sports-oriented prompts. These scenarios naturally require the integration of multiple physical laws simultaneously. The dataset spans: Precision & Arts (e.g., ballet pirouettes requiring angular momentum), Winter Sports (e.g., skiing moguls involving friction and gravity), Aquatics (e.g., diving and swimming involving fluid resistance), and Athletics. These prompts test the model‚Äôs ability to maintain physical consistency during complex human-object interactions.\n14.[ADDRESS_REMOVED]-Level Control and Evaluation Taxonomy\nTo ensure a rigorous assessment, we curate a balanced evaluation set of 100 samples (see TableÀú34), stratified along three dimensions of difficulty:\n-\n‚Ä¢\nInteraction Type (The ‚ÄúWhat‚Äù): We categorize samples by the material properties involved:\n-\n‚Äì\nSolid-Solid: Interactions between rigid bodies (testing impulse, friction, and collision response).\n-\n‚Äì\nSolid-Fluid: Interactions between solids and liquids (testing displacement, splashing, and floating).\n-\n‚Äì\nFluid-Fluid: Dynamics of miscible and immiscible fluids (testing viscosity, mixing, and turbulence).\n-\n‚Äì\n-\n‚Ä¢\nScenario Context (The ‚ÄúWhere‚Äù): We distinguish between controlled physics experiments (Physical Concepts) and unconstrained, dynamic environments (Sports Scenarios), aiming to test generalization from atomic laws to complex behaviors.\n-\n‚Ä¢\nInteraction Complexity (The ‚ÄúHow‚Äù): Across all categories, we vary the complexity level:\n-\n‚Äì\nSimple: Single-object motion or static equilibrium.\n-\n‚Äì\nComplex: Multi-object interactions with simultaneous forces.\n-\n‚Äì\nChain-Reaction: Causal sequences where an initial action triggers cascading effects.\n-\n‚Äì\n14.3 Evaluation and Metrics\nModeling physical commonsense inherently requires capturing temporal dynamics such as force propagation, momentum transfer, and continuous motion. Because static image generators lack temporal modeling capabilities and cannot represent causal interactions unfolding over time, they are fundamentally limited in assessing physical plausibility. Therefore, our evaluation focuses exclusively on video generative models, capable of producing temporally coherent sequences (Cai et al., 2025).\nTo ensure consistent, structured judgments, we employ a Vision-Language Model (VLM) evaluator‚ÄîGemini-2.5-Pro (Comanici et al., 2025)‚Äîprompted to act as a physics and video expert. For each generated video, the VLM assesses four specific dimensions of quality. To standardize the evaluation, each dimension is treated as a binary metric (0/1), where a score of 1 indicates the criteria are fully satisfied and 0 indicates a failure or significant violation.\nWe define the four fine-grained metrics as follows:\n-\n‚Ä¢\nPhysics Accuracy (0/1): Evaluates whether the generated video strictly obeys fundamental physical laws. The model checks if motion adheres to gravity, momentum, and friction, and verifies that object interactions are plausible. A score of 0 is assigned if there are violations such as objects floating, moving at unrealistic speeds for the context, displaying incorrect trajectories, or deviating from the scenario‚Äôs ‚ÄúPhysics Focus.‚Äù\n-\n‚Ä¢\nMotion Quality (0/1): Assesses the temporal coherence and naturalness of the movement. This metric verifies that the motion follows the expected pattern described in the scenario and remains smooth and continuous. A score of 0 is assigned if the motion is jerky, exhibits unnatural accelerations, or suffers from temporal discontinuities and inconsistency.\n-\n‚Ä¢\nVisual Realism (0/1): Measures the visual fidelity and believability of the scene. The model checks if objects and materials appear realistic, whether lighting and shadows are consistent, and if the scene composition is plausible. A score of 0 is assigned if there are significant visual artifacts, glitches, or if the scene lacks photorealism.\n-\n‚Ä¢\nPrompt Adherence (0/1): Determines whether the video semantically matches the user input. This metric verifies that all key elements (objects, setting) are present and that the specific action described actually occurs. A score of 0 is assigned if there are significant mismatches between the generated content and the text prompt.\n-\n‚Ä¢\nOverall Success (Aggregated Metric): To provide a holistic measure of generation capability, we compute a strict Overall Success score. A generated video is marked as successful (1) if and only if it satisfies all four fine-grained metrics simultaneously.\nThis rigorous aggregation ensures that high performance requires a model to generate videos that are not only physically accurate but also visually coherent, smooth, and semantically correct.\n14.4 Case Study\nTo better illustrate the strengths and limitations revealed by our Physical Commonsense evaluation, we present three representative case studies generated by Veo-3 (FigureÀú25, 26, and 27). Each example highlights how our metrics disentangle distinct dimensions of physical reasoning: physics accuracy, prompt adherence, motion quality, and visual realism. These cases further demonstrate that high visual fidelity alone is not indicative of correct physical behavior‚Äîunderscoring why physics-focused evaluation is necessary for robust real-world video generation.\nSuccess Case: Physically Plausible Parachute Inflation.\nFigureÀú25 shows a prompt involving the inflation of a parasail by a two-person crew. Veo-3 performs strongly across all evaluation dimensions, achieving full scores on physics accuracy, prompt adherence, motion quality, and visual realism. The model captures the physical mechanics of inflation: air fills the canopy, internal pressure increases, the fabric transitions from wrinkled to taut, and the lines tighten appropriately as load is applied. The motion unfolds smoothly and continuously, without discontinuities or unnatural accelerations. Detailed textures, realistic lighting, and consistent shading further contribute to the clip‚Äôs visual plausibility. This example illustrates Veo-3‚Äôs ability to correctly represent gradual force buildup, material deformation, and multi-agent coordination.\nFailure Case I: Missing Solid‚ÄìSolid Interaction in Coffee Grinding.\nFigureÀú26 highlights a failure case involving a metal grinder crushing coffee beans (Solid‚ÄìSolid interaction). While the static frames resemble a real grinder setup, the temporal dynamics violate fundamental physical laws. Instead of showing a realistic grinding process‚Äîwhere rigid beans fracture into progressively smaller particles‚Äîthe video abruptly transitions from whole beans to uniform fine grounds, without any mechanical cause. This discontinuous ‚Äústate-change‚Äù effect resembles a visual morph rather than a physical transformation, resulting in violations of mass conservation and material fracture mechanics. Although the model adheres partially to the prompt in terms of objects present, it fails to depict the required interaction, leading to zeros in physics accuracy, prompt adherence, motion quality, and visual realism.\nFailure Case II: Incorrect Angular Momentum Dynamics in Ballet Rotation.\nFigureÀú27 presents a ballet scenario requiring a fouett√© turn, a physically demanding movement involving rapid leg whipping to generate angular momentum and sustain rotation. Veo-3 fails across all physics-focused dimensions. The dancer‚Äôs extended leg and upper body become unnaturally distorted mid-rotation, deviating from realistic human biomechanics. The model also misinterprets the action: instead of producing a sharp, continuous whipping motion that drives a true fouett√©, the dancer performs a slow, controlled rotation with no momentum-generating movement. This breaks both prompt adherence and key physical principles such as angular momentum conservation and joint kinematic constraints. Although the visual appearance remains high-quality at a glance, closer inspection reveals anatomy inconsistencies and motion artifacts that undermine physical realism.\nTakeaways.\nThese case studies highlight the need to evaluate video models beyond photorealism. Veo-3 can produce visually convincing clips, but frequently fails in scenarios requiring nuanced physical reasoning‚Äîespecially where material properties, continuous force propagation, or human biomechanics play a central role. The structured metrics in our Physical Commonsense task make these failure modes explicit, providing a robust diagnostic tool for guiding future model improvements.\n14.5 Evaluation Results\nFine-grained Metrics Primary Metric Model Physics Accuracy Motion Quality Visual Realism Prompt Adherence Overall Scenario Type: Physical Concepts Veo-3 62.50% 54.17% 83.33% 50.00% 41.67% Sora-2 84.00% 80.00% 96.00% 76.00% 76.00% Wan-2.2 58.67% 53.33% 72.00% 38.67% 26.67% Scenario Type: Sports Scenarios Veo-3 80.00% 68.00% 92.00% 68.00% 60.00% Sora-2 88.00% 72.00% 88.00% 68.00% 64.00% Wan-2.2 42.67% 33.33% 96.00% 21.33% 21.33% Average Veo-3 71.43% 61.22% 87.76% 59.18% 51.02% Sora-2 86.00% 76.00% 92.00% 72.00% 70.00% Wan-2.2 50.67% 43.33% 84.00% 30.00% 24.00%\n14.5.1 VLM-Based Evaluation\nTableÀú35 reports quantitative results on the Physical Commonsense task, evaluated using Gemini-2.5-Pro (Comanici et al., 2025) as a VLM-based evaluator. We compare three state-of-the-art video generation models‚ÄîVeo-3, Sora-2, and Wan-2.2‚Äîacross two complementary scenario types: Physical Concepts and Sports Scenarios.\nOverall Performance Trends.\nSora-2 consistently outperforms competing models, achieving the highest overall success rate (70.00%) and leading across all fine-grained metrics. In contrast, Veo-3 exhibits moderate but uneven performance (51.02%), while Wan-2.2 substantially underperforms (24.00%) despite strong visual realism. Notably, all models score highly on Visual Realism (84‚Äì92%), yet exhibit large variance in Physics Accuracy (50.67‚Äì86.00%) and Prompt Adherence (30.00‚Äì72.00%), reinforcing that perceptual quality alone is an unreliable indicator of physical correctness.\nScenario-Specific Difficulty.\nAcross all models, Sports Scenarios are consistently easier than Physical Concepts. Veo-3 improves from 41.67% overall success on Physical Concepts to 60.00% on Sports Scenarios, while Sora-2 maintains strong performance across both (76.00% vs. 64.00%). In contrast, Wan-2.2 fails to generalize even in Sports Scenarios, achieving only 21.33% overall success. These results suggest that contemporary video models benefit from learned biomechanical motion patterns in human-centric activities, while struggling with fine-grained material interactions such as collisions, splashing, and mixing.\nBy Sport Type By Difficulty Model Ballet Diving Skiing Swimming Easy Medium Hard Veo-3 33.3% 50.0% 71.4% 83.3% 60.0% 62.5% 57.1% Sora-2 33.3% 50.0% 85.7% 83.3% 60.0% 75.0% 57.1% Wan-2.2 44.4% 0.0% 28.6% 11.1% 16.7% 33.3% 14.3%\nBy States of Matter By Difficulty Model Solid-Solid Solid-Fluid Fluid-Fluid Action/Other Easy Hard Veo-3 0.0% 75.0% 50.0% 40.0% 53.3% 22.2% Sora-2 100.0% 75.0% 100.0% 66.7% 75.0% 77.8% Wan-2.2 33.3% 25.0% 83.3% 17.8% 35.4% 11.1%\nSport-Specific Patterns.\nFine-grained analysis (TableÀú36) reveals sport-specific challenges. Ballet proves most difficult for all models (33‚Äì44% success), while Swimming achieves the highest scores (83% for Veo-3 and Sora-2). Wan-2.2 fails completely on Diving (0%) yet achieves its best performance on Ballet (44.4%), suggesting model-specific biases in motion priors. By difficulty level, Sora-[ADDRESS_REMOVED] consistent performance across Easy (60%), Medium (75%), and Hard (57%) prompts, while Wan-2.2 collapses uniformly across all difficulty levels (14‚Äì33%).\nStates-of-Matter Analysis.\nTableÀú37 reveals interaction-specific challenges. Solid-Solid interactions prove most difficult: Veo-3 achieves 0% success on rigid body collisions, while Sora-[ADDRESS_REMOVED] 100%. Fluid-Fluid interactions show more variance, with Wan-2.2 achieving 83.3%‚Äîits highest category score‚Äîwhile Veo-3 scores only 50%. By difficulty level, all models show degradation on hard cases, with Wan-2.[ADDRESS_REMOVED] severe collapse (35.4% Easy ‚Üí 11.1% Hard).\nThe Visual‚ÄìPhysical Disconnect.\nWan-2.[ADDRESS_REMOVED] dissociation between visual quality and physical reasoning. Despite achieving the highest Visual Realism score on Sports Scenarios (96.00%), it records the lowest Physics Accuracy (42.67%) and Prompt Adherence (21.33%) in the same setting. This failure mode suggests that the model prioritizes surface-level appearance over causal and physical consistency, producing videos that ‚Äúlook right‚Äù but violate core physical principles.\n14.5.2 Human Evaluation\nTo validate VLM-based assessments, we conducted human evaluation on Veo-3 generated videos (). Human annotators assessed multiple dimensions including Physics Accuracy, Motion Quality, Visual Realism and Prompt Adherence, which we map to the same metrics used in AutoEval (TableÀú35).\nScenario Eval Mode Physics Accuracy Motion Quality Visual Realism Prompt Adherence Overall Physical Concepts AutoEval 62.50% 54.17% 83.33% 50.00% 41.67% Human Eval 77.27% 86.36% 83.64% 72.73% 77.27% Sports Scenarios AutoEval 80.00% 68.00% 92.00% 68.00% 60.00% Human Eval 91.30% 78.26% 84.35% 65.22% 82.61% Average AutoEval 71.43% 61.22% 87.76% 59.18% 51.02% Human Eval 84.44% 82.22% 84.00% 68.89% 80.00%\nAutoEval vs Human Eval Gap.\nTableÀú38 reveals a striking discrepancy between automated and human evaluation. Human evaluation consistently rates Veo-3 higher across all metrics, with the overall success rate increasing from 51.02% (AutoEval) to 80.00% (Human Eval)‚Äîa 29-point improvement. Motion Quality shows the largest gap: 82.22% (Human) vs. 61.22% (AutoEval), suggesting that temporal artifacts flagged by the VLM evaluator are often imperceptible or acceptable to human observers. Physics Accuracy similarly improves from 71.43% to 84.44%, indicating that VLM-based evaluation applies overly strict criteria that may not align with human perception of physical plausibility.\nBy Sport Type By Difficulty Eval Mode Ballet Diving Skiing Swimming Easy Medium Hard AutoEval 33.3% 50.0% 71.4% 83.3% 60.0% 62.5% 57.1% Human Eval 50.0% 100.0% 100.0% 83.3% 90.0% 57.1% 100.0%\nBy States of Matter By Difficulty Eval Mode Solid-Solid Solid-Fluid Fluid-Fluid Action/Other Easy Hard AutoEval 0.0% 75.0% 50.0% 40.0% 53.3% 22.2% Human Eval 66.7% 100.0% 100.0% 73.3% 78.6% 77.8%\nSport-Specific Insights.\nFine-grained analysis (TableÀú39) reveals that human evaluators rate Veo-3‚Äôs Diving and Skiing scenarios at 100% Visual Realism, compared to AutoEval scores of 50.0% and 71.4% respectively. Ballet remains consistently challenging across both evaluation modes (50.0% Human, 33.3% AutoEval), confirming that sustained rotational dynamics in pirouettes and fouett√©s pose genuine difficulties for current video generators. By difficulty level, human evaluators show an unexpected pattern: hard scenarios achieve 100% success while medium scenarios score only 57.1%, suggesting that brief, dramatic actions (ski jumps, cliff dives) are easier to generate plausibly than sustained complex motions.\nPhysical Concept Insights.\nTableÀú40 shows that for Physical Concepts, Solid-Fluid and Fluid-Fluid interactions achieve 100% human-rated Visual Realism, while Solid-Solid interactions remain challenging at 66.7%. The AutoEval-Human gap is most pronounced for Solid-Solid: AutoEval rates Veo-3 at 0%, while humans rate it at 66.7%‚Äîa complete reversal that highlights the strictness of VLM-based collision detection. Across difficulty levels, human evaluation maintains consistent performance (78.6% Easy, 77.8% Hard), whereas AutoEval shows severe degradation on hard cases (53.3% Easy, 22.2% Hard).\n15 Conclusion\nWe introduce a comprehensive benchmark framework MMGR to evaluate the reasoning capabilities of generative models across five complementary abilities. Our evaluation reveals a critical gap between perceptual quality and reasoning capability, with stark performance hierarchies suggesting models primarily learn pattern matching rather than true symbolic reasoning. The gap between closed-source and open-source models indicates progress relies heavily on scale rather than architectural innovations.\nFurthermore, our analysis identifies a unique ‚Äútemporal tax‚Äù on reasoning in video generation, where the requirement to maintain frame-to-frame coherence actively competes with logical consistency. This is evidenced by video models consistently underperforming their image-based counterparts on complex logic tasks, treating mathematical derivation as a visual texture to be morphed rather than a semantic chain to be constructed. We also observe a prevalent ‚Äúhallucination of competence,‚Äù where models frequently generate correct final outcomes despite invalid intermediate reasoning steps, confirming that they are often memorizing answer patterns rather than executing genuine multi-step deduction.\nThese limitations stem from three fundamental bottlenecks in current training recipes: a severe scarcity of structured symbolic reasoning data compared to naturalistic physical data; architectural constraints that prioritize local visual plausibility over global, long-horizon consistency; and optimization objectives that reward perceptual fidelity rather than logical validity. To bridge the gap from image animation to true world simulation, future work must look beyond mere scaling to develop architectures that decouple reasoning states from visual rendering and integrate auxiliary objectives for causal consistency.\nReferences\n- Allen et al. (2020) Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum. Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning. Proceedings of the National Academy of Sciences, 117(47):[POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2020.\n- An et al. (2022) Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao. Bevbert: Topo-metric map pre-training for language-guided navigation. arXiv preprint arXiv:2212.[POSTAL_CODE_REMOVED], 2022.\n- Anderson et al. (2018a) Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S√ºnderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3674‚Äì3683, 2018a.\n- Anderson et al. (2018b) Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S√ºnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018b.\n- Baillargeon (1987) Ren√©e Baillargeon. Object permanence in 3¬°- and 4¬°-month-old infants. Developmental Psychology, 23(5):655‚Äì664, 1987.\n- Bakhtin et al. (2019) Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: A new benchmark for physical reasoning. 2019.\n- Bansal et al. (2024) Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.[POSTAL_CODE_REMOVED], 2024.\n- Bansal et al. (2025) Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: A challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.[POSTAL_CODE_REMOVED], 2025.\n- Bar et al. (2025) Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2025.\n- Barrett et al. (2018) David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. 2018.\n- Batra et al. (2020) Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects. In arXiv:2006.[POSTAL_CODE_REMOVED], 2020.\n- Battaglia et al. (2013) Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):[POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2013.\n- Bear et al. (2021) Daniel M Bear, Elias Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li Fei-Fei Cao, Surya Ganguli, Daniel LK Yamins, et al. Physion: Evaluating physical prediction from vision in humans and machines. 2021.\n- Biederman (1987) Irving Biederman. Recognition-by-components: a theory of human image understanding. Psychological Review, 94(2):115‚Äì147, 1987.\n- Blanco et al. (2008) Jose-Luis Blanco, Juan-Antonio Fern√°ndez-Madrigal, and Javier Gonzalez. Toward a unified bayesian approach to hybrid metric‚Äìtopological slam. IEEE Transactions on Robotics, 24(2):259‚Äì270, 2008.\n- Blattmann et al. (2023) Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. 2023.\n- Cai et al. (2025) Zefan Cai, Haoyi Qiu, Haozhe Zhao, Ke Wan, Jiachen Li, Jiuxiang Gu, Wen Xiao, Nanyun Peng, and Junjie Hu. From preferences to prejudice: The role of alignment tuning in shaping social bias in video diffusion models. arXiv preprint arXiv:2510.[POSTAL_CODE_REMOVED], 2025.\n- Cartillier et al. (2021) Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, and Dhruv Batra. Semantic mapnet: Building allocentric semantic maps and representations from egocentric views. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 964‚Äì972, 2021.\n- Chang et al. (2017) Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV), 2017.\n- Chaplot et al. (2020a) Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. In ICLR, 2020a.\n- Chaplot et al. (2020b) Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33:4247‚Äì4258, 2020b.\n- Chollet (2019) Fran√ßois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.[POSTAL_CODE_REMOVED], 2019.\n- Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.[POSTAL_CODE_REMOVED], 2021.\n- Comanici et al. (2025) Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.[POSTAL_CODE_REMOVED], 2025.\n- DeepMind (2024) Google DeepMind. Veo: Google‚Äôs most capable video generation model. [URL_REMOVED] 2024. Technical report.\n- DeepMind (2025a) Google DeepMind. Veo 3: Generative video with native audio and cinematic control. Technical report, Google DeepMind, May 2025a. URL [URL_REMOVED]\n- DeepMind (2025b) Google DeepMind. Gemini 3 pro image (nano banana pro): High-fidelity image generation with reasoning. Technical report, Google, 2025b. URL [URL_REMOVED] Also covers Nano-banana model variants.\n- Deitke et al. (2020) Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, et al. Robothor: An open simulation-to-real embodied ai platform. In CVPR, 2020.\n- Durrant-Whyte & Bailey (2006) Hugh Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: part I. IEEE Robotics & Automation Magazine, 13(2):99‚Äì110, 2006. 10.1109/MRA.[PHONE_REMOVED].\n- Epstein et al. (1999) Russell A Epstein, Alison Harris, Damian Stanley, and Nancy Kanwisher. The parahippocampal place area: Recognition, navigation, or encoding? Neuron, 23(1):115‚Äì125, 1999.\n- Fuentes-Pacheco et al. (2015) Jorge Fuentes-Pacheco, Jos√© Ruiz-Ascencio, and Juan Manuel Rend√≥n-Mancha. Visual simultaneous localization and mapping: a survey. Artificial intelligence review, 43(1):55‚Äì81, 2015.\n- Gao et al. (2024) Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: A universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.[POSTAL_CODE_REMOVED], 2024.\n- Gibson (1979) James J Gibson. The Ecological Approach to Visual Perception. Houghton Mifflin, 1979.\n- Girdhar & Ramanan (2020) Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and temporal reasoning. In ICLR, 2020.\n- Gomez et al. (2020) Clara Gomez, Marius Fehr, Alex Millane, Alejandra C Hernandez, Juan Nieto, Ramon Barber, and Roland Siegwart. Hybrid topological and 3d dense mapping through autonomous exploration for large indoor environments. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 9673‚Äì9679. IEEE, 2020.\n- Goyal et al. (2017) Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \"something something\" video database for learning and evaluating visual common sense. In ICCV, 2017.\n- Guo et al. (2025) Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.[POSTAL_CODE_REMOVED], 2025.\n- Ha & Schmidhuber (2018) David Ha and J√ºrgen Schmidhuber. World models. arXiv preprint arXiv:1803.[POSTAL_CODE_REMOVED], 2018.\n- Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.[POSTAL_CODE_REMOVED], 2021.\n- Henriques & Vedaldi (2018) Joao F Henriques and Andrea Vedaldi. Mapnet: An allocentric spatial memory for mapping environments. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8476‚Äì8484, 2018.\n- Ho et al. (2022a) Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.[POSTAL_CODE_REMOVED], 2022a.\n- Ho et al. (2022b) Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.[POSTAL_CODE_REMOVED], 2022b.\n- Hong et al. (2022) Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.[POSTAL_CODE_REMOVED], 2022.\n- Huang et al. (2025) Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. Why vision language models struggle with visual arithmetic? towards enhanced chart and geometry understanding. arXiv preprint arXiv:2502.[POSTAL_CODE_REMOVED], 2025.\n- Huang et al. (2024) Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. 2024.\n- Hudson & Manning (2019) Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019.\n- Irshad et al. (2021) Muhammad Zubair Irshad, Niluthpol Chowdhury Mithun, Zachary Seymour, Han-Pang Chiu, Supun Samarasekera, and Rakesh Kumar. Sasra: Semantically-aware spatio-temporal reasoning agent for vision-and-language navigation in continuous environments. arXiv preprint arXiv:2108.[POSTAL_CODE_REMOVED], 2021.\n- Ivanitskiy et al. (2023) Michael Igorevich Ivanitskiy, Rusheb Shah, Alex F. Spies, Tilman R√§uker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. A configurable library for generating and manipulating maze datasets, 2023. URL [URL_REMOVED]\n- Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. 2017.\n- Kahneman (2011) Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011.\n- Koh et al. (2021) Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: A world model for indoor navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2021.\n- Konolige et al. (2011) Kurt Konolige, Eitan Marder-Eppstein, and Bhaskara Marthi. Navigation in hybrid metric-topological maps. In 2011 IEEE International Conference on Robotics and Automation, pp. 3041‚Äì3047. IEEE, 2011.\n- Kosslyn (1980) Stephen M Kosslyn. Image and Mind. Harvard University Press, 1980.\n- Kuaishou (2024) Kuaishou. Kling: Large-scale video generation model. [URL_REMOVED] 2024. Technical report.\n- Lake et al. (2017) Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017.\n- LeCun (2022) Yann LeCun. A path towards autonomous machine intelligence. Open Review, 2022.\n- Li et al. (2023) Mingxiao Li, Zehao Wang, Tinne Tuytelaars, and Marie-Francine Moens. Layout-aware dreamer for embodied visual referring expression grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1386‚Äì1395, 2023.\n- Liu et al. (2024a) Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. 2024a.\n- Liu et al. (2024b) Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. 2024b.\n- Marcus (2001) Gary F Marcus. The Algebraic Mind: Integrating Connectionism and Cognitive Science. MIT Press, 2001.\n- Mathematical Association of America (2024) Mathematical Association of America. American invitational mathematics examination (aime) 2024. [URL_REMOVED] 2024. American Invitational Mathematics Examination problems from 2024.\n- Mathematical Association of America (2025) Mathematical Association of America. American invitational mathematics examination (aime) 2025. [URL_REMOVED] 2025. American Invitational Mathematics Examination problems from 2025.\n- Michotte (1963) Albert Michotte. The Perception of Causality. Methuen, 1963. Original work published 1946.\n- OpenAI (2024a) OpenAI. Gpt-4o system card. Technical report, OpenAI, 2024a. URL [URL_REMOVED]\n- OpenAI (2024b) OpenAI. Video generation models as world simulators. OpenAI Technical Report, 2024b. URL [URL_REMOVED]\n- OpenAI (2025) OpenAI. Sora [ADDRESS_REMOVED]: Advanced video generation with physics simulation. Technical report, OpenAI, September 2025. URL [URL_REMOVED]\n- Piergiovanni et al. (2020) AJ Piergiovanni, Vincent Casser, Michael S Ryoo, and Anelia Angelova. Evolving space-time neural architectures for videos. In ICCV, 2020.\n- Piloto et al. (2022) Luis Piloto, Ari Weinstein, Peter Battaglia, and Matthew Botvinick. Intuitive physics grounded scene generation and understanding. In ICLR, 2022.\n- Qin et al. (2025) Yiran Qin, Ao Sun, Yuze Hong, Benyou Wang, and Ruimao Zhang. Navigatediff: Visual predictors are zero-shot navigation assistants. arXiv preprint arXiv:2502.[POSTAL_CODE_REMOVED], 2025.\n- Qwen (2024) Qwen. Qwen-image: A 20b parameter mmdit-based visual generation model. arXiv preprint arXiv:2508.[POSTAL_CODE_REMOVED], 2024.\n- Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. 2021.\n- Ramakrishnan et al. (2021a) Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. 2021a.\n- Ramakrishnan et al. (2021b) Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021b. URL [URL_REMOVED]\n- Riochet et al. (2021) Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V√©ronique Izard, and Emmanuel Dupoux. Intphys 2019: A benchmark for visual intuitive physics understanding. 2021.\n- Salimans et al. (2016) Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016.\n- Savva et al. (2019) Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In ICCV, 2019.\n- Seely et al. (2025) Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench: Evaluating creative reasoning with sudoku variants. arXiv preprint arXiv:2505.[POSTAL_CODE_REMOVED], 2025.\n- Shah et al. (2025) Hardik Shah, Jiaxu Xing, Nico Messikommer, Boyang Sun, Marc Pollefeys, and Davide Scaramuzza. Foresightnav: Learning scene imagination for efficient exploration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 5236‚Äì5245, 2025.\n- Singer et al. (2022) Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.[POSTAL_CODE_REMOVED], 2022.\n- Spelke & Kinzler (2007) Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental Science, 10(1):89‚Äì96, 2007.\n- Sridhar et al. (2024) Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey Levine. Nomad: Goal masked diffusion policies for navigation and exploration. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 63‚Äì70. IEEE, 2024.\n- Tolman (1948) Edward C Tolman. Cognitive maps in rats and men. Psychological Review, 55(4):189‚Äì208, 1948.\n- Tong et al. (2025) Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, and Xipeng Qiu. Thinking with video: Video generation as a promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.[POSTAL_CODE_REMOVED], 2025.\n- Tulyakov et al. (2018) Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In CVPR, 2018.\n- Ullman et al. (2017) Tomer D Ullman, Elizabeth S Spelke, Peter Battaglia, and Joshua B Tenenbaum. Mind games: Game engines as an architecture for intuitive physics. Trends in Cognitive Sciences, 21(8):586‚Äì599, 2017.\n- Unterthiner et al. (2018a) Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. 2018a.\n- Unterthiner et al. (2018b) Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.[POSTAL_CODE_REMOVED], 2018b.\n- Vondrick et al. (2016) Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. 2016.\n- Wan (2025) Wan. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.[POSTAL_CODE_REMOVED], 2025.\n- Wani et al. (2020) Saim Wani, Shivansh Patel, Unnat Jain, Angel Chang, and Manolis Savva. Multion: Benchmarking semantic map memory using multi-object navigation. Advances in Neural Information Processing Systems, 33:9700‚Äì9712, 2020.\n- Webb et al. (2023) Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent symbols through binding in external memory. 2023.\n- Wiedemer et al. (2025) Thadd√§us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.[POSTAL_CODE_REMOVED], 2025.\n- Wu et al. (2015) Jiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Physics 101: Learning physical object properties from unlabeled videos. In BMVC, 2015.\n- Wu et al. (2022) Xin Wu, Zheng Qi, Zexiang Liu, and He Wang. 3d shape reconstruction from 2d images with disentangled attribute flow. In CVPR, 2022.\n- Xiao et al. (2020) Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer. Audiovisual slowfast networks for video recognition. 2020.\n- Xu et al. (2024) Dawei Xu, Yifan Zhang, and Bo Han. Can llms solve arc-agi tasks? arXiv preprint arXiv:2411.[POSTAL_CODE_REMOVED], 2024.\n- Yan et al. (2021) Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.[POSTAL_CODE_REMOVED], 2021.\n- Yi et al. (2020) Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer: Collision events for video representation and reasoning. In ICLR, 2020.\n- Zacks & Tversky (2001) Jeffrey M Zacks and Barbara Tversky. Event structure in perception and conception. Psychological Bulletin, 127(1):3‚Äì21, 2001.\n- Zhang et al. (2021) Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Abstract spatial-temporal reasoning via probabilistic abduction and execution. 2021.\n- Zhong et al. (2020) Yichen Zhong, Yunzhu Liu, Junyu Liang, Jessica Hodgins, and Antonio Torralba. Learning 3d part assembly from a single image. ECCV, 2020.\n- Zhou et al. (2025) Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pp. 260‚Äì278. Springer, 2025.\n- Zhou et al. (2022) Meng-Jiun Zhou, Zheng Shou, and Bernard Ghanem. Video timeline modeling for news story understanding. 2022.\n- Zhu et al. (2017) Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. 2017."
  },
  {
    "article": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation\nAbstract\nRecent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening. For more details, please visit our website: [URL_REMOVED]\nI Introduction\nRecent advances in humanoid robots have enabled impressive progress in agile locomotion skills, allowing robots to walk, run, backflip, and even dance with human-like fluidity. However, despite these achievements, humanoid robots remain limited in manipulation and can often interact only with lightweight objects. A key challenge in humanoid manipulation is controlling robots to apply consistent and controllable interaction forces. In traditional tabletop manipulators, this problem is typically addressed using a model-based compliance force controller, such as impedance or admittance control. Yet, the agility of modern humanoids primarily stems from reinforcement learning (RL) based controllers, which lack a generalizable formulation for compliant force control.\nRL-based compliance controllers have shown promising results on quadruped robots [unifp, facet] by tracking motion spring-damper-like reference motion. However, these methods require substantial effort to generate large volumes of synthetic data that emulate spring-damper end-effector dynamics. Extending these approaches to humanoids is challenging, as it is difficult to ensure that such synthetic perturbation data remains within the distribution of natural human motion. In parallel, advances in RL-based motion tracking [beyondmimic, twist2, any2track] have demonstrated that learning from human demonstrations is a scalable approach for constructing general-purpose humanoid controllers. However, these methods do not provide a principled way to integrate compliant or impedance control into humanoid tracking frameworks. In this work, we pose the central question: Can we develop a scalable and generalizable approach for reconciling the high-gain stiffness required for humanoid agile motions with the variable compliance needed for safe contact-rich manipulation? Some concurrent efforts [gentlehumanoid, softmimic] attempt to extend FACET [facet] to humanoids by augmenting retargeted human motion data with synthetic observations per responses under force perturbations, and then relying on tracking rewards to encourage imitation of the augmented trajectories. Yet, as is common in RL-based motion tracking [hover, omnih2o, twist2, amo, head], reward computation typically depends on rich reference information such as link poses, velocities, joint positions, and joint velocities, which are challenging to modify reliably during data augmentation.\nOur key insight for tackling this problem is that reference motion can be interpreted, in hindsight, as the robot‚Äôs compliant response to perturbations, allowing it to be used directly for dense reward computation without modification. Rather than altering the reference trajectory, we modify the sparse tracking goals as input obsesration to the policy, which are far easier to edit in a controlled manner. Specifically, we subtract the effect of a perturbation from the original reference motion to define the tracking goal in hindsight, while keeping the reference motion intact to provide high-quality, dense tracking rewards.\nTo this end, we introduce Compliant Humanoid natural control through hIndsight Perturbation (CHIP). This method integrates reinforcement learning with adaptive compliance control and can be directly incorporated into existing humanoid motion tracking frameworks with minimal modification. We demonstrate that generalist humanoid motion trackers trained with CHIP can perform a diverse range of forceful manipulation tasks‚Äîsuch as wiping, writing, cart pushing, and door opening while preserving their agility in tasks like dancing, running, and squatting. Furthermore, for tasks that require world-space coordination among multiple robots, we train a global 3-point tracking policy (i.e., tracking the head and two hands) using CHIP, enabling compliant multi-robot collaboration, including synchronized grasps and cooperative object transport. In summary, our core contributions are:\n-\n‚Ä¢\nCHIP, a generalizable and scalable plug-and-play module that enables adaptive compliance control in a humanoid tracking framework through hindsight perturbation.\n-\n‚Ä¢\nA compliant and natural local 3-point tracking controller that unlocks forceful whole-body teleoperation and autonomous Vision Language Action (VLA) policy learning, while maintaining the agile motion capabilities.\n-\n‚Ä¢\nA compliant global 3-point tracking controller that unlocks stable multi-robot object grasping and moving.\nII Related work\nII-A Variable and adaptive compliance manipulation\nAdaptive compliance control has been used in manipulation to handle contact uncertainty and solve tasks that require force control. Prior work [vices, acp] shows that variable impedance control could enable a robot to perform tasks that require significant contact force control, such as door opening, or to maintain surface contact, such as wiping. Under contact uncertainty, an adaptive compliance controller can also help robots perform stable grasps [springgrasp, hmc]. Most prior adaptive compliance control studies use a tabletop arm with a model-based impedance controller. In our work, we draw inspiration from task design in these works, but apply it to a learned adaptive compliance controller on a humanoid robot.\nII-B Compliant control of legged robot\nRecent success in legged locomotion and loco-manipulation benefits from large-scale reinforcement learning in simulations. RL motion tracking has become a pivotal task enabling general humanoid control. Policies trained purely for motion tracking, often inspired by frameworks such as DeepMimic [deepmimic], tend to be stiff and fragile under force perturbations. When the objective treats any deviation from a reference motion as an error that must be aggressively corrected, the robot generates large, uncontrolled forces during unexpected contact. To address this, recent research has pursued two distinct paradigms: learning to resist external forces or learning to comply with them.\nLearning to Resist Disturbances. One line of work develops controllers designed to maintain tracking performance despite large, unknown external forces. These methods typically train policies with randomized external pushes to encourage robustness. Approaches that combine motion-tracking rewards with random external forces instruct the robot to adhere to the reference trajectory despite interaction forces [ze2025twist, fu2022dwbc, zhang2025falcon]. A prominent example is FALCON [zhang2025falcon], which uses a torque-aware 3D force curriculum, enabling the robot to gradually learn in simulation to resist strong disturbances and pull heavy carts. While effective for applications requiring forceful opposition, this stiffness-centric approach is ill-suited for contact-rich manipulation scenarios‚Äîsuch as robot-human collaboration, wiping, or handling fragile objects‚Äîwhich require the robot to yield in a controlled, spring-like manner.\nLearning to Comply with Interactions. In contrast to force rejection, a second line of research aims to learn controllers that yield to external forces. One approach creates impedance behaviors ‚Äùfrom scratch‚Äù using RL. For example, UniFP [unifp] and FACET [facet] train unified policies to mimic target spring-mass-damper dynamics. UniFP learns a force estimator for admittance control, while FACET modulates policy stiffness as a control input, enabling tasks such as wiping using only proprioception. However, because they are primarily designed for quadruped robots with a single end-effector, scaling these methods to humanoids is challenging because generating synthetic compliance data that matches the distribution of natural human movement is difficult.\nTo sidestep the difficulty of discovering behaviors from scratch, prior work has attempted to build compliance into motion-tracking frameworks. SoftMimic [softmimic] proposes a learning-from-examples framework, using an offline inverse kinematics (IK) solver to generate an augmented dataset of stylistically compliant motions. An RL policy then learns to reproduce these pre-authored responses. Similarly, GentleHumanoid [gentlehumanoid] augments keypoint reference motions with desired spring-damper dynamics during reward calculation. However, this creates an optimization conflict between motion tracking rewards and compliance objectives, often leading to degraded tracking accuracy or reduced agility. To make things worse, relying on offline data augmentation or reward tuning limits their applicability for scaling up to large, diverse motion datasets.\nOur work builds upon these insights but proposes a more direct and scalable alternative. We introduce CHIP, a training recipe that integrates principled impedance-control objectives directly into the online RL loop via hindsight perturbation, without compromising tracking performance. CHIP obviates the need for the extensive synthetic interaction curriculum [unifp, facet] or the offline motion augmentation pipelines of SoftMimic [softmimic] and GentleHumanoid [gentlehumanoid]. By specifically targeting modifications in the input space, our method offers a simple yet powerful way to equip standard humanoid tracking systems with adaptive compliance, narrowing the gap between agile motion and safe, controllable physical interaction.\nII-C Humanoid control interfaces\nRecent advances in reinforcement learning for humanoid whole-body control have produced remarkable results in agile locomotion [beyondmimic, kungfubot, hub, sonic] and whole-body manipulation [ze2025twist, amo, homie]. Extending motion-tracking approaches, several methods [ze2025twist, gmt] use human whole-body motion‚Äîencompassing both link- and joint-level states‚Äîto drive humanoid robot control. However, such control interfaces are complicated to use for both teleoperation and specifying high-level task commands. To simplify humanoid control, several methods [homie, amo] adopt a decoupled upper-lower-body policy, in which the robot receives root-velocity and height commands for locomotion and upper-body target-motion commands for manipulation. Other approaches explore keypoint-based control [omnih2o, hover, sonic, head, twist2, clone], enabling intuitive teleoperation by mapping head and wrist movements from a VR headset to humanoid motion. CHIP upgrades humanoid keypoint control with an adaptive-compliance module. It is compatible with both local keypoint tracking, as in SONIC [sonic], OmniH2O [omnih2o], and global keypoint tracking [clone, head], while additionally providing a plug-and-play module that directly integrates task-aware control into existing keypoint-based frameworks.\nIII Method\nOur method is a lightweight plug-and-play module that integrates seamlessly into any keypoint-based motion-tracking framework and enables adaptive whole-body control policies with end-effector-level compliance. As shown in Fig. 2, CHIP takes the 3-point tracking goal , end-effector compliance coefficients , proprioception , and past actions as input, and outputs actions that enables a humanoid robot to track the 3-point targets while yielding to external perturbations, with the desired level of compliance modulated by the continuous input coefficients.\nThe core of CHIP lies in modifying the training procedure within the motion tracking framework. During training, we apply a perturbation force in a random direction to the end-effector for a random period of time. The policy also receives compliance coefficients that allow it to follow an impedance control law, mimicking spring‚Äìdamper dynamics under external disturbances.\nPrior work [unifp, facet] and recent concurrent work [gentlehumanoid, softmimic] train policies that observe tracking goals from the original reference motion, but use tracking goals modified by perturbations, , to calculate the reward that encourages the robot to track the desired compliant response under force perturbations rather than the original motions (Fig. 3(b)):\nwhere is the current end-effector pose. This approach requires modifying the reference trajectory while maintaining its feasibility (Fig. 3(c)), a process that becomes increasingly difficult for dynamic skills such as running, where motion editing needs to satisfy both kinematic and dynamic constraints.\nInstead, our method modifies the observed tracking goals as input to the policy in a hindsight manner,\nsuch that the original goals are reinterpreted as the perturbation‚Äôs resulting motion and are directly used to compute the reward function, (Fig. 3(a)), eliminating the need to alter the reference motion.\nThis design has several advantages:\n-\n‚Ä¢\nIt takes less effort to modify the sparse goal in the observation than the dense reference motion in reward computation, allowing such modification to be done online during training.\n-\n‚Ä¢\nRobots are always encouraged to produce motion in the distribution of reference motion, which improves motion naturalness.\n-\n‚Ä¢\nRobots are exposed to observations from both the motion dataset and outside it, thereby improving policy robustness at test time.\nThis formulation streamlines training while enhancing robustness and agility. Unlike methods such as [unifp, facet], which explicitly learn an external force estimator, our approach uses a single policy model optimized solely with the PPO objective. The policy implicitly acquires force awareness through proprioceptive observations and past actions. To further improve sensitivity to external perturbations during training, we provide the critic with the ground-truth external force as a privileged observation. In addition, we supply both the actor and critic with a 10-step history of proprioception and past actions, allowing the policy to infer perturbation-related information from noisy observations. At test time, the policy only receives the tracking goal and proprioception. It implicitly estimates the perturbation force and generates actions that counteract disturbances according to the commanded compliance. To enable compliant following, similar to FACET [facet], we also implement a damper model on the target end-effector pose. Under an applied force, the end-effector pose deviates from the previous target ; we update the target using:\nThis update allows the robot to adjust its target in response to force-induced deviations smoothly.\nIV Applications\nWe showcase two applications that utilize our compliance humanoid control: multi-humanoid manipulation and compliant teleoperation. The former application requires 3-point tracking in the global frame, and the latter depends only on a local tracking policy.\nIV-A Compliant multi-robot grasping\nFor tasks requiring global coordination, such as multi-robot collaborative grasping or object transport, we train a whole-body control policy that tracks the 6D pose of the head and the 3D positions of both wrists in the world frame:\nThis target space provides a simple interface for commanding robot end-effectors to reach desired locations in the global coordinate frame. Training uses a global tracking reward that penalizes the 3-point deviation between the robot and the reference motion in the world frame. However, because 3-point global poses provide only sparse information, it remains ambiguous for the robot to determine whether to walk or extend its arm and head when the 3-point target moves. We use 0.2 seconds of future information (5 frames, four frame skip per step) to resolve such ambiguity.\nWith an adaptive, compliant global 3-point controller, multiple robots can position their end-effectors in the world frame and apply desired forces, enabling multi-robot collaborative grasping and the movement of large objects beyond the grasping capability of a single robot.\nTo obtain a two-robot grasp, we extend the dexterous compliant grasp generation algorithm SpringGrasp [springgrasp] to a multi-humanoid collaborative grasp setting. We solve an optimization problem to obtain the 6D head poses and 3D wrist positions of two robots:\nensuring a stable, collision-free, compliant grasp.\nWe then plan a minimum-curvature trajectory between the current and target poses, interpolating wrist targets along the path. The grasping procedure consists of three phases:\n-\n1.\nApproach: robots follow the trajectory to pre-grasp poses near the object.\n-\n2.\nGrasp: wrist targets penetrate slightly into the object to establish a compliant grasp.\n-\n3.\nLift: head and wrist targets move upward to lift the object collaboratively.\nAfter the object has been lifted, we can simultaneously translate the 3-point target poses of both robots to move the grasped large object. For multi-robot collaboration to work in diverse environments, we require high-frequency, infrastructure-free robot localization. Following BeyondMimic [beyondmimic], we combine:\n-\n‚Ä¢\nFoot-odometer based root velocity estimation at 50 Hz,\n-\n‚Ä¢\nFast-LIO2 based pose updates at 10 Hz [fastlio2],\n-\n‚Ä¢\nRoot-IMU angular velocity at 50 Hz,\nyielding a fused 50 Hz root state estimate.\nBoth robots also need to share a consistent global frame. We first built an environment point map using Fast-LIO2. During initialization, each robot registers its live point cloud to the prebuilt map, aligning its coordinate frames.\nIV-B Teleoperation and vision language action model training\nIV-B1 Local 3-point tracking policy\nFor tasks that do not require global coordination in the task space, we train a local policy that tracks the SE(3) poses of the head and wrists relative to the robot‚Äôs root frame, while also tracking lower-body joint positions and velocities to prevent drift in the absence of global information. Such a formulation acts as a decoupled policy that separates upper-body movement from lower-body locomotion at the command level.\nThe reward minimizes the difference between the reference and robot 3-point poses expressed in the root frame. At deployment time, desired 3-point poses are provided either through a VR-based teleoperation system or via rollouts from a Vision-Language-Action (VLA) policy. Lower-body trajectories are generated by a kinematic planner that receives commands for root velocity, heading, and height.\nFor tasks that do not require global localization, the compliant local 3-point policy is straightforward to deploy. Following SONIC [sonic], the robot is teleoperated using VR-tracked wrist poses along with root velocity and height commands from VR joysticks as shown in Fig. 4. Lower-body control is handled by a kinematic planner driven by these commands. The kinematic planner takes root velocity, angular velocity, and height as commands and outputs planned joint positions and velocities. Compliance coefficients for each end-effector are adjusted by the operator in VR. Teleoperation sessions collect paired action data and compliance coefficients from the teleoperation interface, as well as binocular RGB observations from the robot‚Äôs egocentric OAK camera. These data are used to finetune the GR00T N1.5 [groot] VLA model.\nV Experiments and Results\nWe conducted an extensive experiment to answer the following research questions.\n-\n‚Ä¢\nQ1: How well does CHIP track position and force targets?\n-\n‚Ä¢\nQ2: What task(s) do adaptive compliance unlock through teleoperation?\n-\n‚Ä¢\nQ3: How does CHIP facilitate multi-robot collaboration?\n-\n‚Ä¢\nQ4: Can CHIP enable VLA models to perform autonomous forceful manipulation?\nWe compared CHIP with two baselines: 1) FALCON [zhang2025falcon], a policy that does not observe compliance but was trained with force perturbation, and 2) No force, a standard tracking policy that neither observes compliance nor was trained with force perturbation.\nV-A Experiment setup\nWe used Unitree G1 as our testing platform, and an OAK-D W camera was mounted on the robot head to provide egocentric binocular vision. All policies were trained on 64 Nvidia L40S GPUs, with 4096 environments per GPU, for 4 days. During deployment, the local 3-point tracking policy, kinematic motion planner, and teleoperation server were running on Unitree G1 Jetson NX onboard computer with TensorRT acceleration. The global 3-point tracking policy, together with the state estimator, was running on a desktop machine with an i7-13700K CPU and an RTX 3090 GPU. We use the same desktop computer to run VLA policies.\nV-B Tracking performance of CHIP\nWe analyze the tracking accuracy and the robot‚Äôs response to force perturbations under different end-effector compliance settings. Fig. I shows the tracking accuracy measured over [ADDRESS_REMOVED] dataset [ze2025twist]; it shows that the use of hindsight perturbation training yields position tracking on par with the no-compliance and no-force-perturbation baseline, and only slightly affects orientation tracking performance. Meanwhile, the results show that varying end-effector stiffness does not significantly affect tracking performance, indicating that the policy can distinguish between the joint‚Äôs internal driving force required for agile performance and the external perturbation force. We also measured how the compliance coefficient affects the robot‚Äôs responses to force perturbations. Fig. 6 shows that for CHIP, displacement increases linearly while increasing the compliance coefficient. For the baseline, which naively adds a reward for tracking the modified end-effector trajectory, the robot is less responsive to changes in the compliance coefficient because of conflicts among the tracking reward terms.\nV-C Multi-robot grasping and object transportation\nTo demonstrate the effectiveness of our adaptive compliance controller for multi-robot collaborative grasp, we evaluated our method on the collaborative grasping of two objects of different heights, as shown in Fig. 5. Robots first approached and grasped the object with their end-effectors, and the grasp was considered successful if they could lift it. Tab. II shows the grasping success rate. Overall, using an adaptive compliance controller achieves an average grasp success rate of 80%, which is +75% higher than that of an always-stiff controller and +40% higher than that of a controller without force perturbation. In failure cases with our method, the main reason was the robot‚Äôs knee colliding with the object before grasping, which did not fully showcase our controller‚Äôs capabilities. We noticed that an always-stiff controller performed worse than the controller trained without force, because a stiff controller usually applied a much larger force to push the object, creating a large opposing force between the two robots and the object, causing the object to destabilize immediately after the contact. However, thanks to the intrinsic compliance, the controller without force perturbation could still grasp the object from time to time. We also show that after grasping the object, robots can stably move the object and place it in a new location, as shown in Fig. 5. We used the keyboard command to translate end-effector target locations to move the object.\nV-D Teleoperation\nWe demonstrated that the compliant local 3-point tracking controller can be teleoperated to accomplish a wide variety of contact-rich manipulation tasks, as illustrated in Fig. 7. The same controller supports both compliance-demanding tasks‚Äîsuch as lifting, transporting boxes, and wiping surfaces- as well as force-demanding tasks‚Äîsuch as opening a spring-loaded door, actuating a gantry, or flipping heavy boxes.\nWe further show that it can complete tasks requiring different compliance settings on both hands, for example, holding a small whiteboard rigidly with one hand (stiff mode) while writing on it with the other (compliant mode). The user can also adjust compliance online during long-horizon tasks, such as first opening a marker in stiff mode and then writing on the whiteboard in compliant mode. Additional results are shown in our video and on our website.\nV-E VLA inference\nUsing data collected from teleoperation, we fine-tuned a VLA policy to perform tasks autonomously. For the large whiteboard wiping task, we collected 400 teleoperated trajectories. During data collection, the compliance coefficients of the wiping hand were set to 0.05. After fine-tuning, the robot was able to complete the task autonomously, as shown in Fig. 7. We evaluated the policy over 10 autonomous rollouts. A rollout was considered successful if the robot erased all text on the whiteboard within 2 minutes. Under this criterion, the policy achieved a 60% success rate. The dominant failure modes were (i) the text being occluded by the robot hand and (ii) the text leaving the field of view of the robot-mounted camera.\nWe also demonstrate that the VLA model can exploit different stiffness settings for different end-effectors. As illustrated in Fig. 7, the hand holding the small whiteboard was commanded with high stiffness, while the wiping hand used lower stiffness to maintain compliant contact. Using 200 teleoperated trajectories for this bimanual setup, we fine-tuned a separate VLA policy that enables the robot to autonomously hold and wipe the whiteboard, achieving an 80% success rate.\nVI Conclusion\nWe introduced CHIP, a framework for learning humanoid natural adaptive compliance through hindsight replay. CHIP serves as a lightweight plug-and-play module for keypoint-based humanoid motion-tracking systems, enabling end-effector-level adaptive compliance control with minimal modification to existing pipelines. We demonstrated that our method produces adaptive local compliance policies capable of executing force-demanding tasks such as wiping, door opening, and box carrying. Moreover, CHIP supports a global adaptive 3-point tracking controller that enables multiple humanoid robots to manipulate and transport large objects collaboratively. Due to limitations in the proprioceptive accuracy of the Unitree G1 platform, our current implementation employs unidirectional stiffness to specify end-effector compliance. Incorporating accurate wrist force‚Äìtorque sensing would enable more precise and versatile impedance control, potentially unlocking fine-grained manipulation tasks such as peg-in-hole insertion. We leave these improvements to future work.\nVI-A Reward Design\nOur tracking reward design is inspired by prior work [sonic, beyondmimic]. Tab. III shows our reward terms and weights.\nVI-B Force perturbation\nDuring training, we apply random force perturbation to the robot‚Äôs end-effectors. We sample uniformly random force magnitude between 0 and 40 N for each perturbation in a uniformly random direction. For each perturbation, we draw a duration uniformly from 1 to 3 seconds. During the perturbation, the force magnitude changes following the schedule shown in Fig. 8. We design this pattern to mimic fundamental contact forces under impedance control, in which the force magnitude gradually increases to its peak at contact and then decreases to zero at break."
  },
  {
    "article": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization\nAbstract\nRecent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at [URL_REMOVED] Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.\nKeywords: spoken dialogue summarization, paralinguistic cues, audio-language models, multimodal dataset\nSpoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization\nAbstract content\n1. Introduction\nRecent progress in Audio-LLMs‚Äîsuch as WavLLM hu2024wavllm, SALMONN tangsalmonn, Qwen-Audio chu2023qwen, and LTU-AS gong2024listen‚Äîdemonstrates the feasibility of directly modeling speech for downstream language tasks, from translation to question answering. However, most of the existing benchmarks target a single task (e.g. ASR on LibriSpeech panayotov2015librispeech, emotion recognition on IEMOCAP busso2008iemocap). Even when multiple tasks are merged in a single model, these abilities are separately trained and combined with different prompts, but omit the interaction between semantic content and acoustic information. Therefore, we propose Spoken DialogSum, the first large-scale spoken dialogue summarization corpus that is paired with both text-based and emotion-rich summaries based on paralinguistic information.\nDialogue summarization datasets such as SAMSum gliwa2019samsum and DialogSum chen2021dialogsum drive advances in text-based summarization. However, they rely solely on transcripts of written dialogues. In contrast, spontaneous-speech corpora such as SwitchBoard godfrey1992switchboard, MELD poria2019meld capture genuine turn-taking and vocal signals but lack human-labeled summaries altogether. For example, DialogSum provides concise summaries of daily-life dialogues but originates from scripted transcriptions with no backchannels or disfluencies. Therefore, it fails to reflect the actual speakers‚Äô interaction.\nTo address this gap, we built a framework that transforms DialogSum‚Äôs transcriptions into rich annotated speech interactions as Spoken DialogSum. Inspired by the post-process in Behavior-SD lee2025behavior, our pipeline proceeds in three steps: First, we apply an LLM as a style-conversion model to process the dialogues with real conversational transcript examples from SwitchBoard. We rewrite each scripted dialogue to include natural disfluencies, fillers, and natural phrasing. Next, we further insert backchannels at contextually appropriate points in the dialogues as listener engagement. Lastly, we assign one overall emotion style and generate an emotion-focused summary that complements the primary summary for each dialogue. We synthesize emotion-rich, high-fidelity speech for over 13K dialogues (165 hours) using Zonos zonos as the TTS model with 20K clean speech prompts annotated by age group and gender from GigaSpeech gigaspeech.\nSpoken DialogSum is the first corpus to pair raw multi-speaker audio with both factual and emotion-rich summaries while also providing utterance-level labels for speaker emotion, gender, and age. We benchmark three complementary tasks: (1) text-only factual summarization, (2) cross-modal emotion-rich summarization, and (3) acoustic-only paralinguistic-attribute classification. We evaluate two modeling paradigms: a cascaded ASR ‚Üí LLM pipeline and an end-to-end Audio-LLM that consumes raw waveforms plus extracted paralinguistic cues. Experiments show that the Audio-LLM improves ROUGE-L on emotion-rich summarization by 29% over the cascaded baseline, when evaluated against emotion-rich references derived from speech emotion labels. Taken together, these results demonstrate the value of joint semantic and acoustic modeling across all three tasks.\n2. Related Work\n2.1. Text‚Äêbased Dialogue Summarization\nExisting dialogue summarization benchmarks focused on text-based summarization. The SAMSum corpus provides 16K messenger‚Äêstyle dialogues with abstractive summaries, highlighting challenges such as informal language, multiple speakers, and implicit context gliwa2019samsum. DialogSum is a multi-turn dataset of real-life spoken dialogues drawn from DailyDialog li2017dailydialog, DREAM sun2019dream, MuTual cui2020mutual, and an English-speaking practice website, covering daily-life topics such as education, work, and healthcare, with conversations between friends, colleagues, and service providers and customers chen2021dialogsum. Large‚Äêscale benchmarks such as MediaSum (463K media‚Äêinterview transcripts) and SummScreen (TV episode transcripts) demonstrate the continued need for entity tracking and role‚Äêbias modeling in dialogue summarizers zhu2021mediasum; chen2022summscreen. To address low-resource scenarios, LLMs are further applied for data synthesis in creating new dialogues or summaries he-etal-2024-semi; lu2025mutual. Moreover, even without any few-shot dialogue‚Äìsummary pairs, directly generating dialogues via LLMs is effective suresh2025diasynth; lu2025paired.\n2.2. Spoken Dialogue Corpora with Prosodic Information\nVarious speech‚Äêbased datasets support prosodic analysis. Switchboard-NXT extends the Switchboard telephone corpus with intonation labels, disfluencies, and dialogue acts for prosodic turn‚Äêtaking studies calhoun2010nxt. The Santa Barbara Corpus provides face-to-face dialogues annotated for pauses, emphasis, and overlap du2000santa. Traditional corpora such as the London‚ÄìLund Corpus (LLC) and IViE offer tone‚Äêunit and prominence markings across dialects greenbaum1990london; grabe2003ivie. For summarization, AMI is a classic small-scale benchmark, containing less than 300 noisy, overlapping recordings of long-form meetings carletta2005ami.\n2.3. Conversational Dialogue Synthesis\nTo make synthetic speech more natural and interactive, recent TTS and feedback‚Äêmodeling inject spontaneous phenomena and listener reactions. Style‚Äêtransfer TTS systems like AdaSpeech 3 convert reading‚Äêstyle voices with filled‚Äêpause predictors and duration experts to add rhythmic variation yan2021adaptive. Backchannel models Ruede2019 and Context‚ÄêAware Backchannel Prediction park-etal-2024-improving predict both timing and type of listener responses. Integrated approaches further include speaker personality and topic park2024backchannel. Behavior-SD lee2025behavior extends this direction by introducing a large-scale synthetic dialogue dataset with a wide range of spontaneous speaker behaviors and listener responses for training realistic dialogue writing models.\n3. Realistic Spoken Dialogue Data Generation\nWe generate the Spoken DialogSum dataset using a three-stage conversion: Style Transfer, Backchannel Insertion, and Emotion Assignment. Prompts are listed in Table 1.\n3.1. Rich Text Dialogue Generation\n3.1.1. Style Transfer\nThe dialogues in the DialogSum dataset are scripted and lack natural hesitation, unlike real-world conversations. To address this, we first adapt them using Switchboard-style examples, creating more realistic and interactive dialogues that still align with their original summaries. We use a pre-trained instructed LLM model (LLAMA3.3 70B) dubey2024llama to conduct the style transfer. Using a Switchboard sample as a style guide, we prompt the LLM to insert similar fillers and hesitations, transforming the scripted lines into natural-sounding dialogue.\n3.1.2. Backchannel Insertion\nThe style-transfer step ensures that the LLM generates the same number of utterances (i.e., sentences or phrases) as the original script, maintaining alignment between the transformed and source versions. To make the conversations more interactive, we instructed the model to insert interruptions while the other speaker is talking. We use a special symbol as the insertion of mid-turn back-channels as introduced in lee2025behavior. To prevent the model from repeatedly using the same interruption words, we provide examples from Switchboard dialogues to guide more varied and natural backchannel selection. Since interruptions typically occur while the other speaker is talking, we design the backchannel utterances to overlap with the speaker‚Äôs speech. This makes the dialogues more realistic and also increases the difficulty for the model to understand them.\n3.1.3. Dialogue Evaluation\nAfter conducting style transfer and backchannel insertion using LLAMA3.3, we evaluated the generated dialogues with GPT-4o-mini to avoid self-bias, since LLMs often favor their own outputs panickssery2024llm; panicksseryllm. Using a different model reduces this effect and provides a more reliable comparison across corpora.\nAs shown in Table 2, Spoken DialogSum achieves a significantly higher score in Oral Naturalness (4.81) compared with both the source DialogSum corpus (3.86) and the Switchboard reference (4.25). The Conversational Flow metric also improves to 4.15, outperforming the other two dialogue corpora. These gains can be attributed to the inclusion of natural backchannel behaviors, which make the dialogues sound more interactive and human-like. In contrast, the Topical Coherence and Focus score slightly decreases compared to the original DialogSum from 4.59 to 4.49. This is expected since the inserted backchannels occasionally interrupt or fragment the topical continuity of an exchange, leading the model to perceive a small reduction in overall coherence despite improved conversational realism. Overall, Spoken DialogSum provides a highest average scores at 4.48 compare to both the original dialogue (4.19) and their target style reference (4.02).\n3.2. Spoken Dialogue Generation\nIn this section, we introduce our emotion-rich, realistic spoken dialogue generation pipeline: speaker bank construction, conditional TTS synthesis with prosodic adjustments, and timing‚Äêdriven overlap placement.\n3.2.1. Speaker bank construction\nWe annotate age, gender, pitch, expressiveness of tone, and speaking rate for GigaSpeech following wang2025capspeechenablingdownstreamapplications. Table 3 lists the categories. Speaker demographics are derived using a pre-trained Wav2Vec2-based age and gender estimator111https://github.com/audeering/w2v2-age-gender-how-to burkhardt2023age. Following Parler-TTS DBLP:journals/corr/abs-2402-[POSTAL_CODE_REMOVED], pitch and expressiveness are measured using speaker-level mean and utterance-level standard deviation of pitch, computed with PENN222https://github.com/interactiveaudiolab/penn. The speaker-level mean is used to generate a label for speaker pitch relative to gender, and the standard deviation is used as a proxy for how monotone or animated the utterance is. Speaking rate is calculated by dividing the number of phonemes in the transcription by the total duration, excluding any silences.\n3.2.2. Emotion Assignment\nTo generate expressive, speaker-aware speech, we annotate each utterance with sentence-level emotion, pitch, and speaking rate. These annotations are generated using GPT-4o-mini, which is prompted with the complete dialogue along with its turn-by-turn structure. GPT is instructed to (1) produce a concise emotional summary of the dialogue and (2) assign one of eight canonical emotions (Happiness, Sadness, Disgust, Fear, Surprise, Anger, Other, or Neutral) to each utterance. These emotion labels are encoded as 8-dimensional one-hot vectors, which serve as input to the TTS model.\nIn addition to emotion, we extract prosodic cues from the dialogue context. Specifically, GPT is prompted to estimate pitch standard deviation and speaking rate for each utterance in the dialogue. Both are discretized into three categories‚Äîlow (0), medium (1), and high (2)‚Äîto match the expected input range of the TTS model. These predictions are based on the perceived tone, formality, and engagement level of the speakers. The prompts used to derive both emotion and prosodic annotations are presented in Table 1.\nThe full set of style parameters (emotion vector, pitch, and speaking rate) are subsequently used as conditioning inputs to the multi-speaker TTS model described in Section 3.2, enabling generation of speech that is not only intelligible but also emotionally and prosodically appropriate.\n3.2.3. Conditional TTS model\nTo synthesize expressive multi-speaker dialogue audio, we adopt Zonos-hybrid, a conditional TTS model whose SSM‚ÄëHybrid backbone interleaves Mamba‚Äëstyle state‚Äëspace blocks with standard Transformer layers zonos. Zonos supports speaker adaptation, enabling fine-grained control over style, emotion, and voice identity. For our experiments, we leverage this by conditioning on speakers randomly selected from a bank of voices derived from GigaSpeech (Table 3) gigaspeech.\nTo further improve the stability, expressiveness, and quality of our synthetic speech, we carefully curate the pool of speech-prompt segments, selecting only those longer than [ADDRESS_REMOVED] recordings. These sources typically offer lower noise levels and higher recording fidelity compared to more variable platforms like YouTube. From this filtered set, we further restrict our selection to recordings with speech monotony values classified into one of four categories (‚Äúvery expressive and animated‚Äù, ‚Äúexpressive and animated‚Äù, ‚Äúslightly expressive and animated‚Äù, or ‚Äúmonotone‚Äù), deliberately excluding those that are excessively flat or monotonous.\nOnce suitable prompts have been assigned in given dialogues, we inject the previously generated emotion vectors into Zonos-hybrid. To compensate for the TTS model‚Äôs tendency toward under-expressive affect in short utterances, we deliberately elevate the baseline inputs for pitch standard deviation and speaking rate. Concretely, we map ‚Äúlow‚Äù, ‚Äúmedium‚Äù, and ‚Äúhigh‚Äù pitch levels to 60.0, 85.0, and 110.0, respectively, and analogous speaking-rate levels to 15.0, 18.0, and 21.0 (in units of phonemes per second). Additionally, we observed that Zonos can truncate ultra-short backchannel phrases (e.g., ‚Äúgot you‚Äù) too abruptly; to mitigate this, every backchannel utterance is synthesized at the lowest speaking-rate (0), and we append one second of silence after these extremely brief segments. By carefully filtering reference prompts, adjusting prosodic inputs, and introducing silence padding, we achieve more natural, emotionally resonant, and smoothly transitioned multi-speaker dialogue synthesis.\n3.2.4. Timing-driven utterance placement\nWhen merging interrupt and backchannel segments into the original audio, we adjust their timing to mirror natural conversations. To guide placement, we use timing statistics from the real‚Äêworld spoken dialogue corpus CANDOR CANDOR. This corpus shows that interruptions typically occur in a normal distribution before the previous speaker finishes CANDOR. To account for the typical lead-in and trailing silences of an utterance, and to create a more perceptible overlap, we insert an additional 1-second buffer in interruptions, placed 1.[ADDRESS_REMOVED]‚Äôs turn. For backchannels, the delay is drawn from a normal distribution after the previous speaker‚Äôs turn CANDOR. Because utterances naturally include brief leading and trailing silences, we treat those silences as natural delay and place the backchannel at the start of the following speaker‚Äôs turn. This combination of statistical timing and silent padding better replicates the flow of spontaneous dialogue.\n4. The Spoken DialogSum Datasets\nSpoken DialogSum comprises 13,460 multi‚Äêspeaker dialogues and 251,575 utterances, totaling roughly 160 hours of audio. Each dialogue is accompanied by both a concise summary and an emotion‚Äêrich summary. The details of statistics are shown in Table 4. The 160 hours of well‚Äêcurated, speech‚Äêstyle‚Äìannotated audio is one of the largest emotion‚Äêrich, full‚Äêduplex spoken dialogue datasets with summaries available. Figure 2 illustrates the utterance‚Äêlevel emotion distribution: 32.3 of turns are labeled Happiness, 9.07 Sadness, 1.[ADDRESS_REMOVED], 2.99 Fear, 4.76 Surprise, 1.68 Anger, 2.32 Other, and 45.72 Neutral. Unlike many existing corpora that skew heavily toward Neutral or lack fine‚Äêgrained affect, Spoken DialogSum shows a more balanced spread: over 40 of utterances convey clear positive (Happiness) or negative (Sadness) and about 13 of utterances convey nuanced (Surprise, Fear, and etc.) states, making it suitable for training and evaluating emotion‚Äêaware models. The generated dialogue audios and dataset are available333https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/.\nIn Table 5, we report summary statistics alongside human evaluation outcomes for several spoken‚Äêdialogue collections. Specifically, we benchmark Spoken DialogSum against human‚Äêrecorded corpora such as Switchboard switchboard1992 and MELD poria2019meld, human‚Äêread conversations from DailyTalk dailytalk2023, and synthetic dialogues from Behavior-SD lee2025behavior. To gather perceptual judgments, we recruited 12 university-affiliated student raters. They rated 480 audio segments (each 20‚Äì30 seconds long) on a 1‚Äì5 scale across four criteria: Naturalness, Emotion Expressivity, Emotion Consistency, and Sound Quality. Naturalness assesses how closely prosody and pacing mimic spontaneous human speech without obvious synthesis artifacts; Emotion Expressivity determines whether the delivery is monotone or richly expressive; Emotion Consistency judges whether the emotional tone matches the content and context of the dialogue; and Sound Quality measures the degree to which recordings are free of noise and distortion and meet professional audio standards.\nAs shown in Table 5, Spoken DialogSum is the first large-scale spoken dialogue corpus to include emotion-rich summaries, setting it apart from existing datasets. While MELD draws from the Friends TV show and offers highly natural, emotionally rich speech, with fine-grained emotion annotations and occasional background noise, it is limited to 12 hours of audio without available summaires.\nIn contrast, Spoken DialogSum demonstrates consistently strong performance across all human evaluation criteria, with an overall average of 3.78, second only to MELD (4.12). Notably, Spoken DialogSum achieves high ratings for naturalness (3.64) and emotion-related metrics (3.84 for expressivity, 3.75 for consistency), clearly surpassing other TTS-generated corpora such as Behavior-SD and rivaling human-read collections like DailyTalk. Furthermore, Spoken DialogSum‚Äôs sound quality (3.89) exceeds that of large recorded dialogue corpora like Switchboard (2.88) and MELD (3.58), highlighting its robustness despite being synthesized.\nBeyond perceptual strengths, Spoken DialogSum offers approximately 160 of audio, far exceeding MELD‚Äôs 12 and DailyTalk‚Äôs 20, and uniquely provides per-utterance pitch-std and speaking-rate labels. By combining large scale, strong perceptual quality, rich style annotations, and dedicated emotion-focused summaries, Spoken DialogSum is well-suited for emotion summarization and related large-scale spoken dialogue tasks.\n5. Experimental Setup\nAs shown in Table 6, Spoken DialogSum provides a three-way examination of dialogue understanding:\nTask 1 ‚Äì Factual Summarization (purely semantic). The model condenses a dialogue‚Äôs propositional content using only textual cues, evaluating its ability to perform semantic abstraction.\nTask 2 ‚Äì Emotion/Gender/Age Classification (purely paralinguistic): With transcripts removed, the model infers speaker emotion, gender, and age directly from vocal characteristics, assessing competence on paralinguistic cues alone.\nTask 3 ‚Äì Emotion-Rich Summarization (semantic paralinguistic). The system must fuse lexical meaning with vocal affect, capturing what was said and how it was expressed, so that the summary reflects both semantic content and emotional nuance, thereby testing cross-modal integration.\nTogether, tasks 1‚Äì3 form a continuum from text-only reasoning to multimodal fusion and audio-only interpretation, giving Spoken DialogSum a broad view of multimodal dialogue comprehension.\n5.1. Baseline Models\nLLM (Transcript-Only). We bypass audio entirely and feed the reference transcripts to LLaMA-2-7B-chat genai2023llama. The model then produces both factual and emotion-aware summaries.\nWhisper + LLM (Cascaded). Whisper Large V2 first transcribes the speech, and LLaMA-2-7B-chat summarizes the resulting text. This pipeline lets us separate ASR quality from downstream language understanding.\nWavLLM (End-to-End). The architecture consists of a Conformer encoder that extracts acoustic features, which are then fused into a LLaMA decoder through dual cross-attention blocks. This design forms a fully speech-to-text framework.\nQwen-Audio-Chat (End-to-End). The model consists of a Whisper encoder that provides latent speech representations to a Qwen language model through a lightweight fusion adapter, enabling integration of acoustic and semantic information.\nAudio-Flamingo3 (End-to-End). Built on AF-Whisper, it jointly encodes speech, sound, and music, projecting them through adaptor layers into a Qwen-2.5-7B decoder to achieve seamless cross-modal reasoning.\nLTU-AS (End-to-End). Speech is processed by a frozen Whisper encoder and passed into a LLaMA decoder through a time- and layer-wise Transformer bridge. This keeps the ASR front end fixed while introducing alignment layers for modality fusion.\nSALMONN (End-to-End). The architecture combines a frozen Whisper encoder with a Vicuna decoder, linked by a Q-Former alignment module. This configuration preserves strong language priors while establishing an audio‚Äìtext interface.\nWav2Vec2-Based. We use a wav2vec 2.0‚Äìbased model, fine‚Äêtuned on aGender burkhardt2010database, Mozilla Common Voice ardila2020common, TIMIT garofolo1993timit, and VoxCeleb 2 chung2018voxceleb2, to perform age and gender classification tasks.\n5.2. Evaluation Framework\nWe perform our evaluation on the Spoken DialogSum test split, which comprises 500 dialogues, each paired with three human-written summaries. The dialogue summarization score is computed by averaging the results across those three reference summaries. Table 6 shows the abbreviated prompts used in evaluation.\nDialogue Summarization. We evaluate whether the systems can generate concise and coherent summaries based on their semantic content. For text-only models, the input is the ground truth transcript, while for all other models, the full dialogue audio is provided. All models are prompted with the same instruction, and are expected to produce a 2‚Äì3 sentence summary. To assess summary quality, we use ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore. Each generated summary is compared against three ground truth references, and the final score is computed by their average.\nEmotion-Rich Dialogue Summarization. To test the model‚Äôs performance on emotional reasoning, we give the full spoken dialogue as input, and the model is prompted to generate a one-sentence summary describing the emotional expression of each speaker. To assess whether the model can reliably capture such speaker-level affective cues, we use the same automatic metrics as in dialogue summarization‚ÄîROUGE-1, ROUGE-2, ROUGE-L, and BERTScore against the corresponding emotion-rich summary.\nParalinguistic Attribute Prediction. This task is designed to assess whether the models are able to evaluate acoustic cues for identifying speaker-level attributes‚Äîspecifically, age group, gender, and emotion‚Äîfrom full spoken dialogues. Since these attributes rely heavily on prosodic and acoustic features that are absent in pure text, we exclude the text-only and cascaded models from this evaluation. Each dialogue is fed into the model as a whole, and the model is prompted to predict the age and gender of both speakers, as well as the overall emotion expressed in the conversation. The age group is selected from four categories: teenager, young adult, middle-aged adult, and elderly; gender is classified as either male or female; and emotion is predicted as one of positive, negative, or neutral. For evaluation, we report both accuracy and weighted F1-score to reflect robustness and account for class imbalance.\n6. Results\n6.1. Dialogue and Emotion-Rich Summarization Results\nTable 7 compares the baseline models on evaluation axes involving semantic reasoning, both in isolation and when combined with paralinguistic cues. For Task 1 (purely semantic reasoning), where only the semantic content matters, the transcript-only LLaMA-2 and its cascaded Whisper + LLaMA-[ADDRESS_REMOVED], confirming that text-centric LLMs are most effective when no paralinguistic cues are required. When we switch to Task 3 (semantic paralinguistic interaction)‚Äîemotion-rich summarization‚Äîthe ranking reverses. The audio-conditioned SALMONN-13B delivers the best overall scores, with WavLLM close behind, demonstrating their ability to fuse acoustic affect with lexical meaning. Text-only baselines slump sharply, while cross-modal models such as Qwen-Audio, LTU-AS, and SALMONN-7B exhibit mixed gains, underlining that both architecture and training strategy influence how well semantic and acoustic evidence are integrated. Taken together, these results validate Spoken DialogSum‚Äôs design: Task 1 isolates a model‚Äôs semantic abstraction ability, whereas Task 3 probes its competence at weaving affective acoustics into coherent summaries.\n6.2. Paralinguistic Attribute Prediction\nTask 2 evaluates a model‚Äôs ability to infer nonverbal speaker attributes including age group, gender, and emotion from acoustic signals. Table 8 shows results for age and gender classification. Wav2Vec2 achieves the strongest performance (66.3 Acc, 65.2 F1 for age; 95.4 Acc/F1 for gender), closely matching the accuracy reported on real annotated data such as EMODB (67.7 Acc, 80.7 F1 for age; 95.7 Acc/F1 for gender). This alignment suggests that our dataset effectively reflects authentic age and gender patterns. By contrast, WavLLM and Qwen-Audio show weaker results, indicating the difficulty of capturing fine-grained speaker traits without explicit supervision. Table 9 presents emotion recognition in a 4-class setup. LTU-AS slightly outperforms WavLLM (49.1 Acc vs. 42.5 on IEMOCAP; 47.8 vs. 45.8 on EmoSum), and both models show consistent trends with human-labeled benchmarks, confirming that the data also captures realistic emotional cues. Overall, Task [ADDRESS_REMOVED] acoustic modeling. The close correspondence between model performance on our benchmark and real annotated corpora further validates that the dataset captures genuine speaker characteristics.\n7. Conclusion\nWe introduced Spoken DialogSum, a large-scale benchmark that probes dialogue understanding along three separate axes: (i) factual summarization from text only, (ii) emotion-rich summarization that fuses lexical and acoustic cues, and (iii) acoustic-only prediction of speaker emotion, gender, and age. To build the corpus, we transform DialogSum scripts into Switchboard-style conversations, inserting realistic back-channels and synthesizing expressive audio with a conditional TTS pipeline. We created 13,460 dialogues (165 h) that capture authentic turn-taking, disfluencies, and emotional nuance. Baseline experiments reveal substantial performance gaps across modeling paradigms: raw speech input with Audio-LLMs improves ROUGE-L for emotional summaries by 28% compared to a cascaded ASR+LLM pipeline, and Wav2Vec 2.0‚Äìbased classifier shows strong gains in age and gender prediction at the utterance level. Human evaluations further confirm that Spoken DialogSum achieves higher naturalness and emotion consistency than prior synthetic dialogue corpora.\nEthics Statement\nSpoken DialogSum was constructed based on existing open datasets. The dialogue texts originate from publicly available corpora such as DialogSum, while the speech component is synthesized using a conditional TTS model conditioned on speaker samples from GigaSpeech. All sources are released under research licenses, and no private or personally identifiable data are included. The dataset is intended solely for academic research in speech and language processing. We caution against potential misuse, such as applying paralinguistic classifiers for demographic profiling or surveillance, which could raise ethical concerns. Our release will emphasize appropriate use for scientific purposes and transparency in data generation."
  },
  {
    "article": "Bias-Variance Trade-off for Clipped Stochastic First-Order Methods:\nFrom Bounded Variance to Infinite Mean\nAbstract\nStochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index of the noise. Nonetheless, existing complexity results often cover only the case , that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as approaches . This paper tackles the general case of noise with tail index , covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index . Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.\nKeywords: Stochastic composite optimization, Heavy-tailed noise, Gradient clipping, First-order oracle complexity\nMathematics Subject Classification: 49M37, 90C15, 90C25, 90C30, 90C90\n[ADDRESS_REMOVED]-order methods (SFOMs) have been instrumental in driving recent progress in machine learning. In contrast to deterministic first-order methods, SFOMs cannot directly access exact gradients and instead rely on stochastic gradient estimates, with estimation noise originating from various sources such as sampling [bottou2010large] and deliberate injection to improve generalization [he2019parametric, igl2019generalization] or to preserve privacy [abadi2016deep]. Motivated by their widespread applications, studies of SFOMs have garnered considerable attention, advancing the theoretical foundations of optimization and leading to powerful algorithmic frameworks that incorporate stochasticity grounded in statistical principles. In this paper, we consider the composite optimization problem:\nwhere is continuously differentiable and is lower semicontinuous and convex. Problem (1) has broad applications in machine learning, where typically represents the loss function, reflecting errors over the training data, and usually denotes a regularization term that enforces desirable properties on the model. We refer the readers to [sra2011optimization] for further details on the applications of problem (1).\nThere have been extensive algorithmic developments on SFOMs for solving (1) or its special cases; e.g., see [alacaoglu2025towards, bottou2018optimization, davis2021low, foster2019complexity, gao2024non, ghadimi2012optimal, ghadimi2013optimal, lan2012optimal, liang2024single, moulines2011non, nemirovski2009robust, nemirovski1983problem, polyak1992acceleration, robbins1951stochastic, shalev2009stochastic]. In the classical setting of SFOMs, the exact gradient is unavailable. Instead, we have access to a stochastic oracle that satisfies the unbiasedness and bounded-variance conditions:\nfor some . Under this condition and the assumption that is Lipschitz continuous, many SFOMs have been proposed for solving (1) and its special cases. In particular, when is convex and is the indicator function of a simple closed convex set, an accelerated stochastic proximal gradient method (SPGM) has been developed in [lan2012optimal] to achieve a first-order oracle complexity of for finding an -stochastic optimal solution (see Definition 1 for its precise definition). When is strongly convex, accelerated SPGMs have been developed in [ghadimi2012optimal, ghadimi2013optimal] to achieve a first-order oracle complexity of for finding an -stochastic optimal solution. Moreover, when is generally nonconvex, an SPGM with momentum have been developed in [gao2024non] to achieve a first-order oracle complexity of for finding an -stochastic stationary point (see Definition 1 for its precise definition).\nRecently, there has been growing interest in stochastic optimization under heavy-tailed noise, extending beyond condition (2)‚Äîa trend largely driven by modern applications such as transformer training [zhang2020adaptive] and advanced privacy-preserving techniques [csimcsekli2024privacy]. In particular, to model heavy-tailed noise, one may extend (2) to impose unbiasedness and a finite th central moment, for some , on the stochastic gradient ; that is, one assumes that\nfor some . As an algorithmic strategy for handling noise, clipping has been widely incorporated into modern deep learning [pascanu2013difficulty], and extensive studies have investigated clipped SFOMs for solving problem (1) or its special cases under the heavy-tailed condition in (3); see, e.g., [cutkosky2021high, gorbunov2020stochastic, liu2024high, nguyen2023improved, sadiev2023high, sadiev2025second, zhang2020adaptive]. In particular, clipped SFOMs have been analyzed in [zhang2020adaptive] for solving the unconstrained special case of problem (1) with . When is strongly convex, a first-order oracle complexity of has been established for finding an -stochastic optimal solution. In the generally nonconvex case, an oracle complexity of has been established for finding an -stochastic stationary point (i.e., a point whose gradient norm is at most in expectation). It has also been shown in [zhang2020adaptive] that both complexity bounds match the corresponding lower bounds. In addition, it has been shown in [sadiev2023high] that when is convex, the clipped SFOM achieves an oracle complexity of for finding an -stochastic optimal solution, which matches the lower bound in [nemirovski1979efficient]. More recently, it has been pointed out in [fatkhullin2025can] that the vanilla projected SGD also achieves these optimal complexity bounds for solving the special case of (1) with being an indicator function of a simple closed convex set. Furthermore, in [he2025accelerated], it has been show that the vanilla accelerated SPGM achieves the oracle complexity that is universally optimal for smooth, weakly smooth, and nonsmooth convex optimization, as well as stochastic convex optimization under the heavy-tailed condition (3). For nonconvex problems, normalized SFOMs can also attain the aforementioned optimal complexity, indicating that normalization appears to be a viable alternative to clipping for handling heavy-tailed noise; e.g., see [he2025complexity, hubler2024gradient, liu2025nonconvex, sun2025revisiting]. These results suggest that studies of clipped SFOMs under condition (3) may not fully justify the advantages of gradient clipping for handling heavy-tailed noise, as vanilla stochastic algorithms and (potentially simpler) alternative algorithms can achieve comparable complexity bounds. On another note, all these complexity bounds tend to infinity as , failing to cover cases where the noise follows heavy-tailed distributions with an infinite mean, such as the Cauchy and L√©vy distributions. This naturally raises the question:\nCan we develop and analyze SFOMs under heavy-tailed noise with a possibly infinite mean?\nThis paper provides an affirmative answer to this question. We show that clipped SPGMs can provably solve problem (1) in the presence of heavy-tailed noise with a potentially infinite mean, provided that a symmetry measure of the noise tail is appropriately controlled. Specifically, we first introduce heavy-tailed conditions for any tail index , characterized by the bounded central moment as in (5a) and the power-law condition of the density as in (5b). We then provide the regularity conditions: the noise distributions are asymptotically unbiased as in (5c), and a measure of symmetry for the noise tail is controlled as in (5d). Conditions (5c) and (5d) include heavy-tailed distributions that are symmetric about the origin, such as the standard Cauchy distribution, and are much weaker than imposing perfectly symmetry. Since the expectation of the stochastic gradient with infinite-mean noise is undefined, it is natural to consider using clipped stochastic gradients, which have finite first- and second-order moments but also introduce bias (e.g., see [koloskova2023revisiting]). To facilitate the complexity analysis of clipped SPGMs, we further investigate the bias and variance of clipped stochastic gradients, which display a clear trade-off pattern. As a result, our analysis indicates that a moderate clipping threshold is required to efficiently obtain a solution of desirable accuracy, avoiding excessive variance from a threshold that is too large or excessive bias from a threshold that is too small. Finally, we leverage the bias-variance trade-off to establish the first-order oracle complexity of a clipped SPGM (Algorithm 1) for solving problem (1) when is convex, and of a clipped SPGM with momentum when is nonconvex (Algorithm 2). Our oracle complexity bounds are summarized in Table 1 for ease of reference.\nOur main contributions are twofold.\n-\n‚Ä¢\nWe provide novel unified oracle complexity guarantees for clipped SPGMs under heavy-tailed noise with any tail index , provided that a symmetry measure of the noise tail is appropriately controlled.\n-\n‚Ä¢\nWe establish the bias-variance trade-off for clipped stochastic gradients, which can be combined with the analysis of SFOMs under light-tailed noise. As a result, we derive, to the best of our knowledge, the first complexity guarantees for clipped SPGMs for solving convex and nonconvex composite optimization problems.\nIt is noteworthy that some other works also study clipped SFOMs under (nearly) symmetric noise; see, e.g., [armacki2025large, chen2020understanding, jakovetic2023nonlinear]. These works often assume that the noise density is positive near zero and that it is either perfectly symmetric or a mixture of symmetric and asymmetric densities. This is fundamentally different from our setup: we allow the density to be void near zero and require only that the noise tail be nearly symmetric at a specific controlling rate (see Assumption 2 below). Moreover, in [puchkin2024breaking], SFOMs equipped with clipping with median-of-means gradient estimators have been developed and analyzed under a mixture of symmetric and asymmetric noise. However, their assumption on noise symmetry requires -fold convolution, which is more complicated than our setup.\nThe rest of this paper is organized as follows. In Section 2, we introduce the notation and assumptions used throughout this paper. In Section 3, we establish the bias-variance trade-off for clipping. In Section 4, we establish complexity bounds for a clipped SPGM for solving convex problems and a clipped SPGM with momentum for solving nonconvex problems. Section 5 presents numerical results. In Section 6, we provide the proofs of our main results.\n2 Notation and Assumptions\nThroughout this paper, we use to denote the -dimensional Euclidean space. We denote the Euclidean norm and -norm for vectors by and , respectively. For any set and , we denote the Euclidean projection onto as . For any proper closed convex function , we denote its subdifferential by and define the proximal mapping associated with , with parameter , as . We denote the domain of as . We let be a stochastic estimator for . For every and , we define the stochastic gradient with coordinate-wise clipping as , and define the estimation noise as , which has the coordinate representation:\nFor each and , we let be the density function of the random variable . For any , we let be if and let it be otherwise. In addition, we use to denote the standard big-O notation, to denote big-O notation with hidden logarithmic factors, and to denote the standard small-o notation.\nWe now make our main assumptions throughout this paper.\nAssumption 1.\n-\n(a)\nThe proximal operator associated with can be evaluated exactly, and its domain is bounded.\n-\n(b)\nThe gradient is Lipschitz continuous on , i.e., there exists such that for all .\n-\n(c)\nThere exist and such that the density functions satisfy\nRemark 1.\n(i) Assumption 1(a) is quite common in stochastic optimization. We define the diameter of , the upper bound for over , and the lower bound of over , respectively, as follows:\nFor convenience, we denote a lower bound for the clipping threshold as\nAssumption 1(b) is standard. It implies the following descent inequality:\n(ii) Assumption 1(c) formalizes the heavy-tailed noise condition that underpins our analysis, where (5a) and (5b) indicate that the noise has a tail index , and (5c) and (5d) serve as additional regularity conditions. We make a few remarks on (5a)-(5d) below:\n-\n‚Ä¢\nCondition (5a) indicates that the th moment of the noise is bounded, which is common in previous studies of stochastic optimization under heavy-tailed noise (e.g., see [zhang2020adaptive]), although main prior work has only considered the case . In addition, condition (5b) implies that the density function decays according to a power law at the rate of , which is useful for modeling distributions with heavy-tailed behavior (e.g., [stumpf2012critical]). As will be shown shortly in Proposition 1, conditions (5a) and (5b) are nearly equivalent under certain regularity conditions. We impose both conditions solely for convenience of presentation.\n-\n‚Ä¢\nCondition (5c) represents asymptotic unbiasedness and generalizes the common unbiasedness condition for finite-mean noise to infinite-mean noise. To see this, one can interchange the limit and the supremum in (5c) for the finite-mean case. Condition (5d) implies that the integral over the tail of the symmetry measure decays at the rate . It holds for symmetric distributions, e.g., the standard Cauchy distribution. For arbitrary heavy-tailed distributions with tail index , it also holds regardless of whether the distribution is nearly symmetric, as shown in Lemma 1.\nThe following proposition connects the bounded finite-moment condition with the power-law decay of the noise density. Its proof is deferred to Section 6.1.\nProposition 1.\nLet be given, and be a real-valued random variable with the density function . Then the following statements hold.\n-\n(i)\nSuppose that there exist and such that for all . Then, is finite for any .\n-\n(ii)\nSuppose that for some , and is eventually monotone, i.e., there exists some such that is nonincreasing over and nondecreasing over . Then, we have for all .\nRemark 2.\nEventual monotonicity in Proposition 1(ii) has commonly been used as a regularity condition in the study of heavy-tailed distributions; see Theorem 2.5 in [nair2013fundamentals].\nThe next lemma shows that if a density function has a power-law tail decaying at the rate with , then the convergence in (5c) and (5d) holds with rate . Its proof is relegated to Section 6.2.\nLemma 1.\nLet , and be a real-valued random variable with mean zero and the density function . Suppose that there exist some and such that for all . Then,\nWe remark that Assumption 1 is sufficient, as shown in Section 4, to ensure that the clipped SPGMs obtain approximate solutions to problem (1) for any target tolerance within a finite number of iterations. To derive oracle complexity with explicit dependence on the tolerance and the tail index , we make the following additional regularity assumption on the decay rates associated with (5c) and (5d). Under this assumption, we can establish tighter complexity results (see Table 1) and, more importantly, handle problems with tail index with explicit oracle complexity, a regime that, to the best of our knowledge, has not been studied previously.\nAssumption 2.\nThere exist and such that the density functions satisfy\nfor all , where and are given in Assumption 1(c).\nRemark 3.\n(i) Assumption 2 indicates that (5c) and (5d) converge with a sublinear rate of convergence. It is readily seen that this assumption applies to noise distributions with symmetric densities, including the standard Cauchy and L√©vy stable distributions. Moreover, it applies to noise with a controlled measure of near symmetry and a potentially infinite mean.\nWe next give formal definitions for approximate solutions of problem (1).\n3 Bias-Variance Trade-off for Clipping\nIn this section, we establish the bias-variance trade-off for clipping under noise with varying tail indices, including noise with an infinite mean.\nTo motivate the bias-variance trade-off, we recall that the classical analysis of SFOMs under light-tailed noise typically assumes that the noise has zero mean and finite variance. Nonetheless, when the noise is heavy-tailed, its variance, and sometimes even its mean, is infinite. If one applies clipping to heavy-tailed stochastic gradients to trim extreme values, the resulting gradient has finite mean and variance. However, while clipping controls both the mean and variance to be finite, it inevitably introduces bias: the clipped stochastic gradient cannot inherit the unbiasedness of the original stochastic gradient (before clipping). If one views the original stochastic gradient as applying clipping with an infinite threshold, then the variance increases with the clipping threshold, while the bias decreases. This raises the following questions:\n-\n‚Ä¢\nWhat is the quantitative relationship between the bias and the clipping threshold, as well as between the variance and the clipping threshold?\n-\n‚Ä¢\nBy adjusting the clipping threshold, one can trade off variance and bias. How does this bias-variance trade-off affect the oracle complexity of algorithms?\nWe address the first question in this section and answer the second by applying the bias-variance trade-off to establish the oracle complexity of clipped SPGMs in the next section.\nBefore delving into a formal analysis of the bias-variance trade-off, we provide an illustrative visualization of how the bias and variance vary with the clipping threshold for noise with different tail indices. Specifically, we denote the bias and variance of a one-dimensional clipped estimator as follows:\nwhere is a constant, is a random variable modeling noise, and is the clipping threshold. We estimate the expectations in (10) by taking the average of randomly generated samples, and Figure [ADDRESS_REMOVED] Gaussian, symmetric L√©vy stable, and standard Cauchy noise. We can observe from this figure that, as the clipping threshold increases from 0 to 100, the bias approaches zero while the variance increases toward infinity, except in the Gaussian noise case, where the variance remains finite as the threshold goes to infinity. Observe that the tail indices increase from left to right: , , , and . Moreover, the variance for the last three heavy-tailed noise cases shows different growth patterns‚Äîconcave, linear, and convex, respectively‚Äîmeaning the increase becomes steeper from left to right.\nFigure [ADDRESS_REMOVED] to the clipping threshold. For a rigorous analysis, the following lemma establishes upper bounds for bias and variance as functions of the clipping threshold. Its proof is relegated to Section 6.2.\nLemma 2.\nLet be a real-valued random variable with the density function , and and be given. Suppose that holds for some , and that there exists and such that for all . Then, for all , we have\nIn view of Lemma 2, we observe that for a clipped stochastic estimator, its bias and variance , defined in (10), satisfy the following upper bounds:\nWe further notice that\n-\n‚Ä¢\nThe bias can be decomposed into: the truncated expectation of the noise over , a symmetry measure of the tail over , and a diminishing term. By this and Lemma 1, one can see that as when . Also, when the first two terms in the upper bound for converge to zero, we obtain as . In either case, the clipped estimator becomes nearly unbiased when the clipping threshold is sufficiently large.\n-\n‚Ä¢\nThe growth curve of the variance reflects the patterns shown in Figure 1; specifically, it is concave, linear, and convex for , , and , respectively.\nWe are now ready to analyze the bias-variance trade-off for the clipped stochastic gradient. We first introduce the bias of the clipped stochastic gradient, and the set of clipping thresholds that enforce a sufficiently small bias, as follows:\nwhere is defined in (7). For convenience, we define an upper bound for the variance as\nThe following theorem shows that under Assumption 1(c), is nonempty for any and the variance of the clipped stochastic gradient is bounded by . Moreover, when , there exists with . Its proof is relegated to Section 6.2.\nTheorem 1.\n4 Complexity of Clipped Stochastic Proximal Gradient Methods\nIn this section, we use the bias-variance trade-off to establish the first-order oracle complexity of a clipped SPGM for solving convex problems and a clipped SGPM with momentum for nonconvex problems.\n4.1 Complexity of a Clipped SPGM for Convex Problems\nIn this subsection, we establish the first-order oracle complexity of a clipped SPGM for solving convex and strongly convex problems, respectively. Throughout this subsection, we let denote an optimal solution of (1), and we make the following assumption regarding the convexity of .\nAssumption 3.\nThe function is convex on , i.e., there exists such that\nThe clipped SPGM is an extension of stochastic approximation algorithms (see, e.g., [nemirovski2009robust]) in which stochastic gradients are replaced by clipped stochastic gradients, allowing the algorithm to handle problem (1) in the presence of heavy-tailed noise. In particular, this clipped SPGM generates two sequences, and . At each iteration , the clipped SPGM first updates by performing a proximal operation on a clipped stochastic gradient step. It then computes as an average of the past iterates . The details of this method are presented in Algorithm 1, with specific choices of step sizes and clipping thresholds provided in Theorems 3 and 4.\nThe following lemma gives an upper bound on the expected objective value gap for iterates generated by Algorithm 1 under Assumptions 1 and 3. Its proof is deferred to Section 6.3.\nLemma 3.\nRemark 4.\nThe relation (21) is similar to the one that can be established for SPGMs under light-tailed noise (see, e.g., [lan2012optimal, Lemma 3]). The main differences are that, in our case, the bias is no longer zero and the variance is no longer constant; both the bias and the variance depend on the clipping threshold .\nThe following theorem provides a complexity bound for Algorithm 1 to compute an -stochastic optimal solution of (1). Its proof is relegated to Section 6.3.\nTheorem 3 (convex).\nSuppose that Assumptions 1 and 3 hold. Let be arbitrarily chosen, and be a pre-chosen maximum iteration number for running Algorithm 1. Let be given in Assumption 1, and , and be defined in (6), (14) and (15), respectively. Let\nLet be generated by Algorithm 1 with for all and . Then, for all satisfying\nRemark 5.\nFrom Theorem 3, we see that Algorithm [ADDRESS_REMOVED]-order oracle complexity of , with , for obtaining an -stochastic optimal solution to convex problems. When , in view of Theorem 1(ii), one can select\nand it then follows from the definitions of and in (15) and (17), respectively, that the complexity becomes , which recovers the best-known results under condition (3); e.g., see [fatkhullin2025can, he2025accelerated, sadiev2023high]. If Assumption 2 is further imposed, then one can select\nand it follows from the definitions of and in (15) and (18), respectively, that the complexity becomes .\nThe next lemma provides an upper bound on the expected objective value gap for the iterates generated by Algorithm 1 when solving strongly convex problems. Its proof is deferred to Section 6.3.\nLemma 4.\nThe following theorem gives a complexity bound for Algorithm 2 to compute an -stochastic stationary point of (1), whose proof is deferred to Section 6.3.\nTheorem 4 (strongly convex).\nSuppose that Assumptions 1 and 3 hold with . Let be arbitrarily chosen, and let be a pre-chosen maximum iteration number for running Algorithm 1. Let be given in Assumption 1, and , and be defined in (6), (14) and (15), respectively. Let\nLet be generated by Algorithm 1 with for all and . Then, for all satisfying\nRemark 6.\nFrom Theorem 4, we observe that Algorithm [ADDRESS_REMOVED]-order oracle complexity of with . When , in view of Theorem 1(ii), one can select\nand it then follows from the definitions of and in (15) and (17), respectively, that the complexity becomes , which recovers the best-known results under condition (3); e.g., see [fatkhullin2025can, he2025accelerated, sadiev2023high, zhang2020adaptive]. If Assumption 2 is further imposed, then one can select\nand it follows from the definitions of and in (15) and (18), respectively, that the complexity becomes .\n4.2 Complexity of a Clipped SPGM with Momentum for Nonconvex Problems\nIn this subsection, we establish the first-order oracle complexity of a clipped SPGM with momentum for solving nonconvex problems.\nThe clipped SPGM with momentum is an extension of the SPGM with momentum (see, e.g., [gao2024non]) with stochastic gradients replaced by clipped stochastic gradients, which allows the algorithm to handle problem (1) in the presence of heavy-tailed noise. In particular, the algorithm is first initialized with a search direction , and then it generates two sequences, and . At each iteration , the clipped SPGM first updates by performing a proximal operation on a clipped stochastic gradient step. Then the next search direction is computed as a weighted average of the clipped stochastic gradients of evaluated at the iterates . The details of this method are presented in Algorithm 2, with the specific choices of step sizes, weighting parameters, and clipping thresholds provided in Theorem 5.\nFor convenience, we define a sequence of potentials for Algorithm 2 as\nwhere the sequence is generated by Algorithm 2, and is given in Assumption 1. The next lemma establishes a descent property for the potential sequence defined in (29). Its proof is deferred to Section 6.4.\nLemma 5.\nRemark 7.\nThe descent relation (30) resembles the one that can be established for SPGMs under light-tailed noise (see, e.g., [gao2024non, Lemma 5]), except that the bias is no longer zero and the variance is no longer constant; both the bias and the variance depend on the clipping threshold .\nThe following theorem provides a complexity bound for Algorithm 3 to compute an -stochastic stationary point of (1). Its proof is deferred to Section 6.4.\nTheorem 5 (nonconvex).\nSuppose that Assumption 1 holds. Let be arbitrarily chosen, and let be a pre-chosen maximum iteration number for running Algorithm 2. Let be given in Assumption 1, and , , and be defined in (6), (7), (14) and (15), respectively. Let\nLet be generated by Algorithm 2 with , , and for all . Then, for all satisfying\nwhere is uniformly drawn from .\nRemark 8.\nFrom Theorem 5, we observe that Algorithm [ADDRESS_REMOVED]-order oracle complexity of with . When , in view of Theorem 1(ii), one can select\nand it then follows from the definitions of and in (15) and (17), respectively, that the complexity becomes , which recovers the best-known results under condition (3); e.g., see [fatkhullin2025can, he2025complexity, sadiev2023high, zhang2020adaptive]. If Assumption 2 is further imposed, then one can select\nand it follows from the definitions of and in (15) and (18), respectively, that the complexity becomes .\n5 Numerical Results\nIn this section, we present numerical experiments evaluating the convergence behavior of Algorithms 1 and 2 under different clipping thresholds and noise levels. Both algorithms are implemented in Matlab, and all computations are performed on a laptop equipped with an Intel Core i9-14900HX processor (2.20 GHz) and 32 GB of RAM.\nIn implementing Algorithms 1 and 2, we simulate noisy gradient evaluations by injecting heavy-tailed noise into the gradients as , where is an -dimensional random vector with each coordinate follows a heavy-tailed distribution with tail index . We generate each coordinate of as , where follows a Rademacher distribution (i.e., with probability and with probability ), and follows a uniform distribution over . Then one can verify that the density function of is symmetric with respect to , and also that\nHence, the density function of , denoted by is of order , and the tail index of is .\n5.1 -Regularized Convex Regression\nIn this subsection, we consider the -regularized least-squares problem:\nwhere , with and , with being the all-ones vector, and . We randomly generate and , with each entry sampled from the standard normal distribution. We apply Algorithm 1 with different clipping thresholds to solve (33) under noise with different tail indices . For every run of Algorithm 1 with specific tail index and clipping threshold , we initialize the algorithm at the all-zero vector and tune the step size to optimize its individual performance.\nThe convergence behavior of Algorithm 1 is presented in Figure 2. Specifically, the first row presents the convergence behavior with different clipping thresholds , for fixed heavy-tail indices , and the second row presents the convergence behavior under different noise levels , for fixed clipping thresholds . From the first row, we can see that the clipped SPGM can converge if a suitable clipping threshold is applied even in cases where the noise has no finite mean (i.e., ). However, if the clipping threshold is too small, the clipped SPGM fails to reduce the objective value due to the large bias introduced by clipping. Conversely, if the clipping threshold is too large and the noise has no finite mean (i.e., ), the clipped SPGM will also fail to converge, because in this case it is close to the vanilla SPGM, whose convergence appears to be disrupted by the infinite-mean noise. From the second row, we observe that for noise with a less heavy tail (i.e., ), the clipped SPGM converges with all tested clipping thresholds. However, for noise with a heavier tail (i.e., ), the algorithm performs well only within a narrower range of clipping thresholds. This suggests that, for heavier-tailed noise, the range of suitable clipping thresholds becomes increasingly limited.\n5.2 -Regularized Nonconvex Regression\nIn this subsection, we consider the -regularized nonconvex regression problem:\nwhere , , , , with and , with being the all-ones vector, and . We randomly generate ‚Äôs and ‚Äôs, with each element sampled from the standard normal distribution. We apply Algorithm 2 with different clipping thresholds to solve (34) under noise with different tail indices . For every run of Algorithm 2 with specific tail index and clipping threshold , we initialize the algorithm at the all-zero vector and tune the step size and weighting parameter to optimize its individual performance.\nThe convergence behavior of Algorithm 2 is presented in Figure 3. Specifically, the first row presents the convergence behavior with different clipping thresholds , for fixed heavy-tail indices , and the second row presents the convergence behavior under different noise levels , for fixed clipping thresholds . From the first row, we observe that the clipped SPGM with momentum can converge when a moderate clipping threshold is applied, even in cases where the noise has no finite mean (i.e., ). However, if the clipping threshold is too small, the algorithm fails to reduce the objective value due to the large bias introduced by clipping. On the other hand, if the clipping threshold is too large, the clipped SPGM with momentum becomes close to the vanilla SPGM, which fails to converge or may even diverge under heavier-tailed noise with an infinite mean (i.e., ). From the second row, we see that when the noise has a less heavy tail (i.e., ), the clipped SPGM with momentum converges across the tested clipping thresholds selected from . In contrast, for noise with a heavier tail (i.e., ), the algorithm performs well only within a limited range of thresholds. This is partly because a too-small clipping threshold introduces large bias, while a too-large threshold induces excessive variance; therefore, a moderate clipping threshold is required to ensure optimal performance. These observations also indicate that as the noise becomes heavier-tailed, the interval of suitable clipping thresholds may become increasingly narrow.\n6 Proof of the Main Results\nIn this section, we provide the proofs of our main results presented in Sections 2, 3, and 4, specifically, Proposition 1, Lemmas 1 to 5, and Theorems 1 to 5.\n6.1 Proof of the Main Results in Section 2\nProof of Proposition 1.\nWe first prove statement (i). Fix any . Then one has that\nwhere the inequality follows from for all . Hence, statement (i) holds.\nWe next prove statement (ii). By Markov‚Äôs inequality, one has that\nLet be arbitrarily chosen. Notice that is nonincreasing over . It follows from the above inequality that\nRearranging terms of this inequality, we obtain that holds for any . For any , the same argument applies to . Hence, statement (ii) holds as desired. ‚àé\n6.2 Proof of the Main Results in Section 3\nProof of Lemma 2.\nWe first prove (11). By the definition of the projection operator, we have\nBy splitting the integral into subintervals and rearranging terms, we derive that for all ,\nUsing the fact that for all , we obtain that for all ,\nIn addition, we have that for all ,\nBy substituting (40), (41), (42) and (43) into (39), we can obtain that (11) holds as desired.\nFor convenience, we denote\nThen, by , one has that\nProof of Theorem 1.\nIn view of Assumption 1(c), (4), and (45), we see that the assumptions of Lemma 2 holds with . It then follows from Lemma 2 that for any and ,\nhold for all . Then, using (6) and the above relations, we obtain that for any and ,\nhold for all .\nWe first prove statement (i). Using (46), we derive that for any and ,\nwhere the last inequality is due to . In view of this and (5c) and (5d), we see that\nwhich along with (14) implies that is nonempty for any . In addition, it follows from (47) that for any and ,\nThis immediately implies (15). Hence, statement (i) holds as desired.\nWe next prove statement (ii). Under Assumption 1(c), we see that the assumptions of Lemma 1 holds with . It then follows from Lemma 1 that\nBy substituting these into (48) and using and (7), we can derive that for all , , and ,\nBy this, one can see that holds for defined in (17). Hence, holds, which completes the proof. ‚àé\nProof of Theorem 2.\nNotice that the assumptions in Theorem 1 holds. By the same arguments as for proving (48), one has that for any and ,\nIn addition, recall from Assumption 2 that\nBy substituting these into (49), we obtain that for all , , and ,\nBy this, one can see that holds for defined in (18). Hence, the conclusion of this theorem holds as desired. ‚àé\n6.3 Proof of the Main Results in Section 4.1\nProof of Lemma 3.\nFix any . By the optimality condition of (19) with , it follows that there exists such that\nwhich along with the convexity of implies that\nBy (8), the definition of in (6), and the convexity of , one has\nCombining this with (50), we obtain that\nwhere the last inequality is due to\nIn addition, we recall from Theorem 1(i) that for all ,\nUsing the definition of in (6), and the definition of in (13), we obtain that for all ,\nTaking expectation on (52) with respect to , using (53) and (54), and rearranging terms, we obtain that (21) holds as desired. ‚àé\nProof of Theorem 3.\nNotice from Theorem 1(i) that under Assumption 1(c). Thus, exists. Using (20), (21) with , and the convexity of , we obtain that for all ,\nRecall from the definition of in (6) that . In addition, by the definitions of and in (14) and (22), respectively, one has that . Combining these with (55), we obtain that for all ,\nwhere the first equality is due to the definition of in (22). Then, by this, one can see that holds for all satisfying (23). Hence, the conclusion of this theorem holds as desired. ‚àé\nThe following inequality provides an estimation of the th harmonic number:\nwhere the first inequality follows from the convexity of with (see also [he2025complexity, Lemma 2] with ).\nWe next provide a lemma that will be used to derive complexity bounds for Algorithm 1. Its proof follows similarly to that of [he2025complexity, Lemma 3].\nLemma 6.\nLet be given. Then, holds for all .\nProof.\nFix any satisfying . Then, by and the fact that is decreasing, one has . Let . It can be verified that is decreasing on . By this and , one has that\nwhere the last inequality follows from due to . Hence, the conclusion of this lemma holds. ‚àé\nProof of Lemma 4.\nBy similar arguments for proving (50) and (51), one can prove that\nBy the same arguments for proving (52), one has that\nUsing the definition of in (13), we obtain that for all ,\nTaking expectation on (57) with respect to , using (53) and (58), and rearranging terms, we obtain that (24) holds as desired. ‚àé\nProof of Theorem 4.\nNote from Theorem 1(i) that under Assumption 1(c). Thus, exists. Using (20), (24) with for all and , and the convexity of , we obtain that for all ,\nwhere the second inequality is due to (24) and the definition of in (25). By the definitions of and in (14) and (25), respectively, one has that . Then, using this, (56) and (59), we obtain that for all ,\nwhere the second inequality is because for all . In addition, using Lemma 6 with , we obtain that\nwhich along with (60) implies that holds for all satisfying (26). Hence, the conclusion of this theorem holds as desired. ‚àé\n6.4 Proof of the Main Results in Section 4.2\nLemma 7.\nProof.\nFix any . It follows from (28) that\nwhere the last relation is due to and\nIn addition, we recall from (13) and Theorem 1(i) that for all ,\nBy substituting them into (62), one can derive that for all ,\nwhere the second inequality follows from for all and . Letting , and using Assumption 1(b) and the fact that , we obtain that\nHence, (61) holds as desired. ‚àé\nLemma 8.\nProof.\nFix any . By the optimality condition of (27), there exists such that\nProof of Lemma 5.\nThe next lemma will be used to derive the complexity bounds for Algorithm 2. Its proof can be found in [he2025accelerated, Lemma 2] and is therefore omitted here.\nLemma 9.\nLet be given, , and for . Then, it holds that\nProof of Theorem 5.\nNotice from Theorem 1(i) that under Assumption 1(c). Thus, exists. By the definition of and , we obtain that the assumptions of Lemma 5 hold. Then, by (6), (16), and (29), and , one has that\nTaking expectation on both sides of (30) with respect to , we have\nSumming up this inequality over , and using (68) and (69), we obtain that for all ,\nBy the definitions of and in (14) and (31), respectively, one has that . Rearranging the terms in this inequality and substituting and for all , we obtain that\nwhere the last relation is due to (67) and Lemma 9 with . Recall that is uniformly selected from . It then follows from this and the above relation that\nBy this, one can observe that holds for all satisfying (32), which completes the proof of this theorem. ‚àé"
  },
  {
    "article": "Bertsimas et al.\nEarly Warning Index\nEarly Warning Index for Patient Deteriorations in Hospitals\nDimitris Bertsimas \\AFFSloan School of Management, Massachusetts Institute of Technology, \\[EMAIL_REMOVED], \\AUTHORYu Ma \\AFFOperations and Information Management, University of Wisconsin Madison, \\[EMAIL_REMOVED],\nKimberly Villalobos Carballo \\AFFTechnology Management and Innovation, New York University, \\[EMAIL_REMOVED],\nGagan Singh \\AFFHartford HealthCare, \\[EMAIL_REMOVED],\nMichal Laskowski \\AFFHolistic Hospital Optimization, \\[EMAIL_REMOVED],\nJeff Mather \\AFFHartford HealthCare, \\[EMAIL_REMOVED],\nDan Kombert \\AFFHartford HealthCare, \\[EMAIL_REMOVED],\nHoward Haronian \\AFFHartford HealthCare, \\[EMAIL_REMOVED],\nProblem definition: Capacity-constrained hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data‚Äîranging from vital signs and lab results to scheduling and patient flow metrics‚Äîto effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats.\nMethodology: We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI‚Äôs design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient‚Äôs risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers.\nResults and Managerial implications: Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to care giver scheduling and allocation of critical resources (i.e. ICU beds). As a result, clinicians and administrators can avert downstream complications including costly procedures or high readmission rates and improve overall patient flow.\nhealthcare operations, Electronic Health Record (EHR), multimodal learning, human in the loop, explainability, predictive machine learning, large language models.\n1 Introduction\nHealthcare operations are increasingly relying on data-driven methods to forecast important logistical and clinical events. The forecast of deteriorating patients in particular is of significant financial and operational importance across hospital systems. A successful alert algorithm can help physicians proactively treat warning signs and apply immediate urgent care to prevent patients from further severe deterioration (Alam2014), which reduces potential downstream costs of additional treatments, as well as increased risk of future readmission. These accurate planning is also instrumental in resource allocation, where successful predictions efficiently ensure specialized personnel being deployed and allow other staff members to stay focused on routine care, whereas unsuccessful predictions draw resources away from other deteriorating conditions and emergencies. From physicians‚Äô perspective, advanced notice of ad-hoc deterioration events could also help reduce unpredictability in their workspace, which is a leading cause of their burnouts. However, in practice, such systems have been largely supported by front-line healthcare workers by their qualitative expertise using a limited subset of core patient information (Jones2006), and their time can be significantly saved by using an automated, quantitative risk score prediction system.\nOur study setting is a large hospital system in the U.S., which has recently started to embed predictive models into its electronic healthcare record system. However, existing scoring systems, including the EPIC deterioration index (Byrd2023), as well as NEWS scores (Smith2019), were not adopted widely in practice by physician and nurse teams. The first predominant issue is their relatively low performance, caused usually by relying on only limited source of information from EHR and are thus not particularly personalized for individual patient profiles. These lead to frequent cases where predictions contradict clinicians‚Äô medical judgments, which are then avoided altogether. In this study, we develop a multimodal learning framework to provide an accurate deterioration forecast called Early Warning Index (EWI). EWI integrates rich patient medical history and dynamically updates medical signals into a holistic framework to update deterioration risks daily to provide guidelines for physicians. Specifically, we jointly predict emergency response team dispatch, intensive care unit (ICU) admission, and near-term mortality in the next 24 hours as an aggregated deterioration event. Secondly, development of such deterioration systems in high-stake environments such as hospitals should not be purely quantitative. Rather, important operational and clinical insights, and private information that are not observable from EHR data, but equally significant in practice, should be integrated to iteratively refine model developments, and ultimately incorporated as part of the final decision. EWI facilitates these human-in-the-loop efforts through explainable feature quantification using SHAP, which computes the contribution of each input feature on the outcome and reveals important operational insights. To the best of our knowledge, there is limited work on leveraging this diverse level of data sources and human judgements into a single, holistic framework for patient deterioration risk prediction that has been implemented in practice.\n1.1 Related Literature\n1.1.[ADDRESS_REMOVED] problems such as nurse scheduling (Legrain2024), ICU bed management (Ouyang2020), and future readmission (Mann2024). Once a patient is admitted into the hospital, multiple caregiver teams will be mobilized to optimize the patient‚Äôs journey within the hospital. Importantly, when a patient is in critical condition or in need of emergency responses, significantly more resources will be allocated for their continuous monitoring and related medical needs. These involve re-appointing nurse managers or cardiac/respiratory specialists from their existing work shift to frequently check upon deteriorated patients, as well as potentially transferring patients to ICU units or step-down units. Unfortunately, both emergency-expertise clinicians and available beds are scarce and cost-intensive resources in most healthcare settings. It is thus of high importance for the upstream [AFFILIATION_REMOVED]. This advanced knowledge of patients‚Äô risk of deterioration can assist medical administrators in allocating this critical resource (KC2012, Bertsimas2022) optimally. If not managed appropriately, resulting in flow congestions and over-capacitation could delay patients‚Äô access and quality of care ( Bertsimas2022, Kim2015, Chen2023), increase spillovers across other hospital units (Kim2024), and even increase readmission risks if some patients are forced to be either discharged prematurely or be exposed to infection risks due to prolonged stay (Chan2012).\nThere are two types of deterioration events in clinical settings: acute and stable. Acute deterioration events are urgent, arising within high-stakes environments that demand rapid assessments of extensive patient data and immediate action, such as resuscitation. For instance, emergency response teams typically have less than five minutes to react following a dispatch call (Jung2016). Current systems for detecting acute deterioration predominantly employ a ‚Äùtrack-and-trigger‚Äù method. This involves continuous bedside monitoring that prompts immediate intervention upon recognizing critical abnormalities through a rule-based system (Gao2007). Such systems force caregivers to react in real-time without the opportunity for advanced planning. On the contrary, stable deterioration refers to a more gradual, ongoing decline in a patient‚Äôs health, where physicians can typically anticipate and strategize interventions more.\nTo this end, automated machine learning models have been proposed to stratify patients into different risk groups to assist physicians‚Äô decision-making (Gao2020, Taylor2016). These early warning alert systems include the National Early Warning Score (NEWS) (Smith2013), the APACHE III prognosis system (Knaus1991), as well as the EPIC deterioration index (Byrd2023). However, variables included in the model development phase are often constrained to a single modality, or a source of data information.\n1.1.2 Multimodal and LLM Patient Representation\nThe integration of data from diverse data modalities was one of the main sources for recent artificial intelligence‚Äôs superior performance across medical domains. During routine rounds of patient status updates, nurses and doctors are often given large quantities of data ranging from medications, procedures, and lab results to diagnoses for multiple patients. The difficulty to accurately integrate all this information manually in a short time spang makes room for error and could cause physicians fail to prioritize patients requiring the most immediate attention (Na 2023). Frameworks of multimodal learning have been proposed in several works (Chen2024, Acosta2022) to unify data from tabular, time-series, vision, and language. However, despite the richness of information these novel methods provide, available predictive models still largely rely on a subset of limited medical information, often predominantly including age, gender, past medical diagnoses, and family history (Byrd2023, Smith2019). The bottleneck of pushing for more personalized decision-making calls for need of an automated data processing pipeline that can scale across hospitals. However, processing different modalities of data into a consistent representation is not trivial. Challenges in missing data (Norris2000), differences in patient record length, differences in service availabilities in different [AFFILIATION_REMOVED].\nAnother large volume of literature is dedicated to the learning of textual data, which led to the recent breakthroughs in Large Language Models (LLMs). These versatile models can be applied to a wide range of tasks. An especially useful usage of these models for healthcare data is the representation of categorical information, which is traditionally processed via methods such as one-hot encoding or top-k frequency (including only the first k categories ranked by frequency). These traditional processing is sub-optimal especially in healthcare operational settings requiring continuous daily update: consider we wish to maintain medications categories prescribed to all patients, due to the sheer volume of possible categories, if all categories are included, this data matrix would be sparse. On the other hand, maintaining only a subset of frequently prescribed medications could lose important signals of patient condition. Instead, by considering the categorical information of EHR data as textual information, we effectively retain all raw signals. We discuss this more in detail in Section 2.3.\n1.1.3 Human-in-the-loop Decision Making\nWith the increasing awareness of machine learning‚Äôs performance, a diverse pool of algorithms and models have been developed, piloted, and implemented across different operational decision areas. While these algorithms often exhibit impressive performance, there is an increasing recognition that emphasizes the important role of integrating human experts‚Äô opinions, decisions, and insights to achieve practically relevant outcomes. In particular, such human-in-the-loop practices are shown to effectively complement and guide final operational and financial decisions across contexts including ride hailing (Benjaafar2024), medication prescription (Baucum2023), investment decisions (Bianchi2024), hotel pricing (Garcia2024), and text messaging (Teeni2023). An overarching theme of this stream of literature is the recognition that humans often remain the ultimate decision makers, especially in high-stake settings. Therefore, machine learning algorithms should thus explore beyond algorithmic accuracy to examine and integrate how managers and other decision makers engage with model outputs based on real-world constraints such as resource limitations, human preferences, and organizational policies. In our specific setting, we build on these findings by incorporating human (i.e., caregiver and logistic managers) expertise in two phases. First, during model development, experts help identify negative corner cases‚Äîsituations where the model fail due to data limitations or unrecognized medical conditions. Second, at deployment, they refine alert thresholds to account for organizational constraints such as limited ICU bed capacity and the risk of alert fatigue. We detail these human-in-the-loop design considerations and their implications for operational performance in Section 3.3.\n1.2 Contributions and Structure\nWe propose a hospital-centric methodology that integrates historical medical and operational information of a patient‚Äôs admission stay to forecast near-term future deterioration risk. Our method involves two key steps that had not been previously explored or discussed: (a) an end-to-end machine learning pipeline that integrate multimodal data from a diverse source of information, (b) complementing quantitative evaluation of model performance with human-in-the-loop evaluations and model refinement.\nFor the first step, we developed an automated extraction method to gather relevant tabular, time-series and language data from an existing large scale electronic health record system from a large U.S. hospital system. An important contribution of our work is tackling traditional challenges associated with integrating data of different formats and occurrence intervals. For example, we extensively discuss why we developed large language model-based method to reformulate medication and diagnosis categories as textual data to avoid sparsifying feature representation and lose rare occurred, but critical medical information. Instead, our approach exploits known advantages of LLM‚Äôs ability to gather contextual information as well as tapping into the larger store of knowledge it was originally pretrained on (see Section 2.3).\nThe second key contribution of our work is the integration of human-in-the-loop expertise throughout the model development, evaluation, and implementation process. Compare to previous literature which largely focuses on improving quantitative performance of similar forecast models, our approach integrates physician feedback as a key metric of success. We discuss extensively and provide concrete cases where such insights provided guidance on feature and model selection that comply with both existing medical understanding and operational constraints (see Section 3.3) and discuss in depth how human decisions guided the design of the developed alert system. This stream of analysis was made possible with explainability method that visualizes the contribution of features to each prediction, which quantitatively provided insights into relevant operational factors (i.e. service load within ward), administrative conditions (i.e. day of week), and clinical factors (i.e. vital measurements).\nThe rest of the paper is organized as follows: In Section 2, we define the problem setting, present the patient cohort and the definitions of the deterioration outcomes, describe the multimodal patient representation. In Section 3, we outline our model training and evaluation methodology and detail how to adopt human-in-the-loop expertise. In Section 4, we report and compare the predictive power of multimodal combinations using three machine learning techniques on the retrospective data and discuss the operational insights we observed in the best-performing model. In Section 5, we outline the practical implementation pipeline and considerations and showcase the implemented dashboard. In Section 6, we discuss some managerial implications and limitations of the work and its impacts on operational efficiencies and patient outcomes.\n[ADDRESS_REMOVED] Model\n2.[ADDRESS_REMOVED] near-term patient deterioration risk within their hospital admission stay using EHR data. We assume each patient‚Äôs medical information is indexed by , with denoting the collection of all information that is available to us from admission to time . This information consists of three different sources of information, or modalities: tabular (denoted as ), time-series (denoted as ) and textual (denoted as ) and can be represented as . We describe in detail how to process each modality in Section 2.3 and 2.4. The granularity of t can adjust in practice depending on the use case, and our problem formulation is agnostic to its specific value. We adopt as a single day (24 hours) as this interval collects ample information update in the participating hospital and announce these predictions at 8am in the morning to most closely adhere to physicians‚Äô rounding schedule. We suppose that there is a function that aggregates all learned information up until time t. We call the patient embedding at time , and refer to it as in the remainder of the paper.\nLet denote the time-series of deterioration outcome (defined later in Section 2.5), where denotes whether a deterioration outcome has occurred during time and . Given input , we look for the best performing predictive model that outputs a probability of outcome . In our setting, this is equivalent to asking: given all gathered medical information up until the end of day of a patient‚Äôs stay, what is the probability of them developing a deterioration event in the next 24 hours.\nExisting approaches adopt a continuously updating patient-centric time scale, where a prediction is made after a designated period, starting with the time of a patient‚Äôs admission. However, in practice, daily rounds of physicians and nurses for the discussions of patient conditions are often caregiver-centric, where a designated time (usually twice a day) is used. We thus aggregate patient information ending at 11:59 pm every day and make predictions with this sharp deadline.\n2.2 Patient Cohort Selection\nA total of 18,633 patients were retrospectively identified from the existing hospital database from 2021-01-01 to 2023-04-01. We only include patients who are currently not in ICU units, as these patients are already in critically ill conditions, and life-threatening events such as emergency response team dispatch or mortality will be managed distinctively from other [AFFILIATION_REMOVED]. Patients who did not stay at the hospital overnight (length of stay of fewer than 24 hours), have an abnormal deterioration event dispatch time that is outside of their admission time window, and have missing admission dates are excluded from the patient cohort as well. Our cohort design aims to dynamically update patient risks daily throughout the admission, and thus, we consider 110,900 patient-admission-days, with each sample aggregating all previous days‚Äô information of a patient admission into a single representation of .\n2.3 Textual Representation of Categories\nOne bottleneck to utilizing medication and medical diagnoses data is the large number of categories within each source, where the majority of the categories only occur in a few patients. Traditional approaches take the most frequently occurring categories and discard the rest, thus leaving a large amount of information originally available unused. To address this challenge, we leverage recent advancements in Large Language Models (LLM) using pre-trained language models to represent time-series data (Wang2022, Huang2020, KC2012). These methods project the original input text into a fixed-sized numerical vector representation, embedding, that can be used for downstream tasks. Concretely, given as the input text that has been tokenized to tokens, we use a pre-trained function , in this case, an LLM, to obtain a tensor of dimension m times a pre-defined embedding size. This token can be further reduced to be a one-dimensional vector via averaging over the token dimension, effectively [AUTHOR_NAME_REMOVED]-trained LLM is crucial for accurate representation of the original language as well as computational efficiency for practical implementations. ClinicalBERT (Alsentzer2019) is an open-source, state-of-the-art medical LLM built on the BERT architecture and finetuned on electronic health records from the MIMIC dataset (Johnson2016). However, to avoid the computational hurdle of its relatively slower processing on a large amount of data, we instead use a distilled version of ClinicalBERT, tiny-ClinicalBERT (Rohanian2024), that tries to mimic the teacher models‚Äô behaviors with reduced computational costs and faster processing time. In addition to passing the original text information, we also incorporate contextual knowledge in the form of a prompt that informs the prediction task. The input reads, ‚ÄùWe aim to predict if the patient will experience a deterioration event in the next 24 hours (emergency response team, ICU admission, mortality). [rest of category information].‚Äù For each recorded patient-admission-day-time medication or diagnosis, we process the original text into these fixed-size embeddings of dimension 312.\n2.4 Multimodal Patient Representation\nA rich set of patient data from multiple sources and modalities is aggregated into a comprehensive patient profile. These features capture information from three main categories: tabular features, time-series features, and textual features. Specifically, tabular data include 1) demographic information (e.g., age); 2) social behavior (e.g., whether is an alcohol user); 3) patient administrative condition (e.g., whether the patient is in ICU), and 4) auxiliary operational variables which are not patient-specific (e.g., day of the week, ward utilization); time series data include: 5) vital signs (e.g., blood pressure, oxygen level), 6) lab results (e.g., potassium level); and textual data include 7) medication administered (e.g., medication name, dosage, and intake form), and 8) medical diagnosis (e.g., recorded by ICD codes). We process these features based on their native forms to optimize the extraction of their information. For static features, we encode categorical features so that they are ordered properly with respect to the outcome (i.e., alcohol user has a higher value than non-alcohol users). For time-series features whose precise numerical fluctuations signal patient conditions, we compute the daily average, peak, minimum, and last values recorded, as well as a trend from the previous day. For all features, when missing data is present, we first impute it by using the latest value recorded for that patient in the same hospital stay, and if it is missing for all admission days, we impute it with 0.\nThe final set of features extracted was inspired by discussions with the physicians regarding certain medications prescribed to severe patients, which are summarized in Appendix Table 1. Two types of medications are considered: green and yellow. Specifically, green medications refer to intravenous medications that signify a patient‚Äôs relatively sick condition. If these medications are discontinued, transitioned to oral medication, or no other medication from the same class was prescribed, this implies that the patient is improving. The inclusion of these medications‚Äô prescription changes could thus be a signal of a non-inference event (not deteriorating). Another set of yellow medications is generally used either during the procedure, in the ICU, or when the patients experience emergency responses. We assume that the use of these medications is a surrogate marker for sickness. These two groups of medications can also be used together to signify the patient‚Äôs condition. For example, if a patient had a yellow medication during the hospitalization and is now on green medication, we can hypothesize that it could be an indicator that the patient is at higher risk for deterioration.\nTo aggregate all information belonging to the same day, we further average all embeddings from the same day into a single patient-admission-day numerical vector. Encompassing all language embedding and other features, 1478 features were eventually extracted to provide a holistic representation of the entire patient profile. We remark that we do not adopt embedding alignment methods, such as contrastive learning, from these three different sources although it is possible that such approaches could improve final performance. A key to this decision is to ensure explainability at the model refinement stage to concretely identify the features and their respective raw values that had contributed to the prediction of deterioration risk, as is detailed in Section 3.3.\n2.5 Outcome Definitions\nWe make predictions for every inpatient in the hospital for the following three deterioration events: an emergency response team dispatch, an ICU admission, or mortality in the next 24 hours. Specifically, an emergency response team dispatch consists of four types of events: rapid response team (RRT), cardiac alert team (CA), anesthesia STAT (AS), and difficult airway team (DART) dispatches. The majority of these calls come from emergency [AFFILIATION_REMOVED]. When an event is not recorded, we consider the occurrence to be negative (0). The aggregated deterioration is defined as the union of all three deterioration events, where the occurrence of any suffice to trigger a deterioration alert. Rigorously, this can be expressed as follows, where l is an indication function of the occurrence of an event:\nWe remark that these outcome variables are surrogate to our true objective: patient deterioration severity, and that the identifications of them are subjective to endogeneity. In particular, admission into the ICU unit could be determined beyond patient conditions, but instead are results from availability of ICU beds, as well as human decisions that are subject to uncertainties and biases. Our predictive model, thus, should also not be viewed as an estimate of how likely emergency response teams will be dispatched the next day, or whether or not the patient should be admitted into the ICU units. Instead, these machine learning models provide guidelines to estimate the likelihood of deteriorating patient conditions which could result in these critical events.\n3 Model Development and Human-in-the-loop Refinement\n3.1 Cohort Splitting\nFollowing traditional machine learning practice, the data set is split into training/validation/test sets, where training is used to develop models, validation is used to select the best model, and test is used to evaluate the final model performance on the unseen, out-of-sample dataset. To reflect and simulate real-world development cycles where the models are trained on past data and utilized for future predictions, data split is performed chronologically. We sort the data by record date and split it into 70% training ([POSTAL_CODE_REMOVED] patient-admission-days), 15% validation ([POSTAL_CODE_REMOVED] patient-admission-days), and 15% testing sets ([POSTAL_CODE_REMOVED] patient-admission-days). Specifically, the training set includes all patient records from 2021-01-01 to 2022-08-07, the validation set includes all patients between 2022-08-13 and 2022-11-29, and the testing set includes all patients between 2022-11-29 and 2023-04-01. We note that this stratification by date and patient is to ensure that no data leakage is shared between these three groups. In comparison to the random splitting approach, under a practical production lens, having access to future dates‚Äô samples and outcomes are inherently leaking information to the current training process.\n3.2 Model Training and Evaluation\nWe apply three different types of machine learning models, random forest (Breiman2001), gradient boosted trees (Chen2016), and TabNet (Arik2021), to learn the binary prediction task of two outcomes: occurrence of inference event (1) and non-event (0). The inference process aims to solve an empirical risk minimization (ERM) problem, where given input realizations of past data with n features and their respective binary outcome variable we look for the optimal function solution within a class of cleaners that minimizes the risk given a loss function\nTo reduce this infinite dimension optimization to a tractable form, we consider instead a parametric family of learners with finite-dimension parameter . However, ERM approaches potentially lead to non-optimal out-of-sample performances, where a new sample from the true data-generating distribution could potentially have a high loss due to optimal learners‚Äô overfitting on the training data. To avoid this, we conduct regularization and hyperparameter tuning on the hold-one-out validation set to select the learner with the best generalizability performance. For tree-based models (random forest and gradient boosted trees), we select the optimal combination of the number of estimators (20, 50, 100) and maximum depth (3, 4); for TabNet, we select the optimal combination of the dimension of the decision prediction layer (16, 32), dimension of the attention layer (16, 32), and number of steps in the decision process (3, 5). The optimal combination is used to conduct a final training on the combination of training and validation sets to optimize performance.\nWe compute the accuracy metric of the area under the receiver operating curve (AUROC) reported on the test set and evaluate the sensitivity and specificity of the final learner to select the optimal threshold to modify the continuous probability output into a binary prediction. Specifically, sensitivity (recall) refers to the model‚Äôs ability to predict a deterioration event when it occurs (captures the truly severe patients), and specificity refers to the model‚Äôs ability to predict a non-deterioration event when it doesn‚Äôt occur (captures the non-severe patient). We also investigate precision (positive predicted value), which refers to the model‚Äôs accuracy of the positive predictions (predicted early warnings are indeed deteriorating patients).\n3.3 Human-in-the-loop Refinement\nIn high-stake environments such as healthcare, directly implementing purely quantitative, machine-learned algorithm can lead to unintended negative consequences. Due to healthcare setting‚Äôs heavily human-oriented and resource-constrained nature, algorithms are at large developed as support tools complementing physicians‚Äô decisions rather than autonomous decision makers. Care providers and logistic managers often have access to additional private information, as well as in-depth understanding of medical implications and the availability of existing resources to act upon these forecasts. In our setting, such expertise heavily guided model development in two phases: identifying negative corner cases, and refining alert thresholds. Once initial models are developed, we identify patients whose predictions significantly deviate from the ground truth, and use SHAP to quantify the most significant contributing factors that has resulted in the incorrect risk prediction. For example, a group of high-risk alerts were repeatedly generated for patients whom we later identified with DNR (do not resuscitate) orders. This highlights factors beyond medical conditions, in this case patient‚Äôs end-of-life preferences, were not effectively integrated in the system to prevent aggressive cares. In another instance, certain medications were interpreted as signals of acute deterioration risk, though upon physician review, they were identified as care providers‚Äô existing recognition of a patient‚Äôs declining condition. This led to the classification of green and yellow medications discussed previously. By iteratively removing, reclassifying, or adding these features into the predictive model, we align these algorithmic outputs with established existing clinical protocols and physician expectations more closely.\nAnother important expert-driven decision of the forecast model is the setting of alert thresholds. Specifically, the raw output of the predictive model is a probability indicating the degree of patient‚Äôs likelihood of developing a deteriorating event. However, such raw output cannot be used directly in practice as they are not directly related to the decision or action needed by care providers. Instead, these probabilities are stratified into three groups: low, medium, and high risks, which then correspond to differing levels of alerts to notify them regarding who should be prioritized during their routine rounding check-ins. However, the identification of these thresholds is not trivial: if set too low, the predictive model will have high false negative rates and fail to alarm care providers of patients who deteriorate without being detected. This is a particularly negative scenario as such algorithmic failure could result in patient severe conditions and even mortality. On the other hand, it is important to recognize that hospitals are resource constrained environments, and the prevalent phenomenon of false positive alarms, where alerts go off for healthy patients, is a significant factor of physician burnout. These events divert resources away from patients who may truly need them and contribute to physicians‚Äô reluctance to adopt these algorithms in the future, as they disrupt their work schedule. Thus, the decision of the thresholds of when to raise these alerts are extensively discussed with the physicians, where multiple metrics beyond performance are used to determine the final cutoffs. Among these, thresholds were implemented and tested in practice to gather physician feedback daily, and iteratively adjusted until a stable threshold was agreed upon among physicians who actively monitor the forecasts.\n3.4 Computation Resources\nData processing, feature engineering, model training, and inference are conducted in Python 3.8.11 with a parallelization strategy on a remote server with 32GB RAM Intel Xeon-P8 CPU per instance. This implies that our framework is feasible from both compute and memory for majority of the hospital [AFFILIATION_REMOVED].\n4 Quantitative and Qualitative Performance\n4.1 Accuracy of Model\nAmong the three machine learning models experimented with, we obtained the highest test AUC using the gradient boosted trees model combining all three available modalities. In comparison to the best performing single modality model, the best performing multimodal model achieves a performance improvement of 4.7%. The general trend of model performance improves with additional modalities included across different settings, with few exceptions where inclusion of additional modalities reduces performance (i.e. random forest tabular and language, in comparison to tabular data alone), which potentially implies that there are cases where increased number of features could imply more noise.\n4.2 Physician Determined Alert Threshold\nIn Figure 1, we demonstrate the different options of probability threshold cutoffs of 0.03 (blue), 0.06 (pink), and 0.12 (orange), which lead to different sensitivity, specificity, and precision performances. The most optimal tradeoff emphasizes capturing all patients with a risk of deterioration, as a missed prediction could be life-threatening for the patient. Given a selected threshold (i.e., colored pink) has a sensitivity of 0.846, a specificity of 0.557, and a precision of 0.154 and is used as the cutoff for the early warning alert (all predictions higher than 0.06 is predicted as an alert, otherwise as not an alert). This alert will then be sent to the physicians‚Äô team to request additional attention for the predicted medium-risk patients. Similarly, high-risk alerts can also be sent once they had reached a threshold of 0.12 (colored orange) given the tradeoffs from below.\n4.3 Explainability by SHAP\nThe ability to understand qualitative insights generated by machine learning models guide physicians to make better real-world medical decisions. SHAP values are considered a state-of-the-art interpretability method for quantifying feature-level contribution to predictions from black-box models. Specifically, a positive SHAP value implies that the inclusion of the feature contributes to predicting the occurrence of a deterioration event, and a negative SHAP value implies a contribution to predicting the non-occurrence of the deterioration event.\nConsistent with previous operations management literature, observations from SHAP indicate that the prediction of deterioration events is strongly correlated with operational factors. In the example indicated for an individual patient, a higher end-of-day census of the ward, which corresponds to a more congested ward facility, intuitively reduces individual-level care quality. Similarly, a high number of hospital admissions could cause potential reduced attention of care providers to individual patients due to high volume of traffic across the entire hospital facility. These factors indicate that deterioration events are far from simple demonstrations of medical complications, but rather also a combination of with hospital service quality as well.\nWe observe that each feature‚Äôs contribution to the prediction varies as their value changes in Figure 3, where each point corresponds to one sample in the test set. We choose features that are of continuous numerical, ordered categorical, and binary categorical nature. We observe that a positive future surgery date contributes positively to SHAP value, with its highest inflection point moving from negative to positive SHAP values occurring at 1. This implies that a scheduled next-day surgery, which could potentially fail or create further medical complications, increases near-term (24-hour) deterioration event risk. Similarly, placement of a DNR order (1), lower RASS measurement (less than 0), and average heart rate in the past 24 hours (above 105) all correspond to a positive SHAP value and a higher likelihood of a deterioration event occurrence. Note that the trend of SHAP value for features could be both linear (negative last RASS measurement) and nonlinear (future surgery date), demonstrating the ability of SHAP to capture rich feature relationships.\n5 Deployment in Production\n5.1 Implementation Considerations\nSeveral critical factors were considered during the development of the predictive model to ensure its efficiency and applicability in real clinical settings. Firstly, to ensure scalability for rapid prediction updates, the model is designed to be lightweight, enabling rapid processing of individual samples. To ensure data integrity, significant amounts of checks were implemented to ensure that recording delays, erroneous data entries, as well as data leakage would not negatively impact the data quality. These checks include the cleaning of data entries with different structures, excluding samples outside of the expected time range of admission, as well as ensuring that no recorded data directly corresponds to the prediction of outcome. Given the high-stakes nature of the environments in which the model would be deployed, another crucial aspect was the development of straightforward, direct visualizations. These visualizations are tailored to enable physicians to utilize the tool optimally under time pressure. Consequently, the selection of threshold cutoffs for alerts was extensively deliberated in collaboration with physicians, as also previously discussed in Section 3.3. This collaboration ensured that the binary alerts generated by the model were both clinically valid and reasonable, thereby supporting swift and accurate decision-making in critical care scenarios. Finally, model calibration was evaluated to ensure that the model output probability corresponds to the empirical event rate, as can be seen in Appendix Figure 1.\n5.2 Currently Implemented Pipeline\nThe proposed model has since been implemented and is currently under testing in the hospital system with daily updated data, which is received at 11:59 pm every day. We demonstrate the implementation pipeline in Figure 4, which largely follows the one in [18] for patient inflow predictions. The pipeline is developed by a proprietary firm that processes daily updated patient information received from the hospital system, which is used to make early warning alerts for each patient at 8am in the morning prior to the first patient status update round. These alerts are reviewed by the physicians, who gather both positive and negative use cases, and report back to the model maintenance team to investigate and update the model. However, the model‚Äôs performance could deteriorate as new data comes in and the patient covariate distribution experiences distributional shifts. To ensure model performance do not deteriorate due to distributional shift, after the first round of model training, the model is kept static until being retrained again every 6 months. As we improve patient outcomes, we expand the model deployment to satellite hospitals of the same hospital system and then beyond to other hospital systems.\n5.[ADDRESS_REMOVED] in the participating hospital and daily process around 1500 patients across the seven satellite locations. For each patient, the dashboard displays [AFFILIATION_REMOVED]. Computed risks of deterioration as well as changes of deterioration risks in comparison to the previous one and two days are also displayed. Red alerts indicate the highest-risk patient group, where the patient‚Äôs current day (the immediate next 24 hour) deterioration risk is more than 12%, or has increased over the previous day by 6%; yellow alerts indicate the medium-risk patient group, where the patient‚Äôs current day deterioration risk is more than 3%, or has increased over the previous day by 1.5%; and white alerts indicates the low-risk patient group for all remaining patients. These risk stratifications are used as triage tools for the physicians during rounding to avoid delays in identifying and prioritizing patients who need check-ins and interventions most urgently.\n6 Managerial Implications and Limitations\n6.1 Operational Benefits via Early Warning Alerts\nThe implementation of early warning systems for patient deterioration is critical and addresses the complex, multifaceted dynamics of planning and preparation across [AFFILIATION_REMOVED]. The primary goal of these systems is to provide clinicians with advanced notice, allowing them to intervene before deterioration occurs. Once a warning is triggered, specific conditions‚Äîsuch as abnormal blood measurements or signs of respiratory collapse‚Äîcan be promptly assessed to allow rapid identification of the root cause of a patient‚Äôs decline. Physicians can then take targeted actions to stabilize the patient by various clinical interventions Targeted actions can also include change in the level of care by moving the patient to ICU or step down for better monitoring and advanced interventions. By offering early intervention opportunities, these systems not only prevent the need for critical resuscitation efforts but also reduce the physical toll on patients and alleviate the significant burden on healthcare resources.\nPhysician burnout is another critical issue that could be improved by the integration of such an early warning system. Specifically, these burnouts are closely tied to the unpredictability of inpatient settings, where fluctuating patient volumes and clinical conditions make it difficult for physicians to plan their shifts efficiently. This unpredictability often leads to overtime, contributing to low workplace satisfaction and increased physician turnover. Upon starting rounds, physicians may have 20 or more patients to assess, with no real-time, objective method for determining which patients require immediate attention. Our model addresses this challenge by streamlining rounding activities and providing physicians with a quantified, objective tool to prioritize patient care. This ‚Äùsnapshot‚Äù prioritization enables quick identification of high-risk cases, reducing the need for manual reviews of vital signs and nurse notes. For low-risk patients, the tool can also aid in assessing discharge readiness‚Äîa key factor in downstream bed management and overall hospital logistics. By improving response times and decision-making accuracy, this system can help physicians plan their day more efficiently and ultimately reduce the strain that leads to burnout.\nOur model is planned to continuously adapt to the dynamic nature of patient admissions by updating every 24 hours, ensuring that predictions remain aligned with [AFFILIATION_REMOVED]. Once fully implemented, the model will support approximately 40-50 medical providers and 100-120 nurses across various [AFFILIATION_REMOVED]. In the long term, the model‚Äôs reach is anticipated to expand across seven hospitals within the same healthcare system, offering comprehensive support to preempt critical care interventions, reduce unnecessary physician burnout, and ultimately lead to proactive patient management.\n6.2 Patient Outcome Impacts\nEarly warning alerts for patient deterioration facilitate timely and precise interventions, preventing the progression of conditions that could otherwise escalate into severe complications. This capability not only improves the immediate responsiveness of healthcare teams but also reduces the need for more intensive and costly treatments in the future. By intercepting deterioration at an early stage, these systems could help maintain patient stability and prevent acute crises that typically result in high-cost care. Concrete outcomes of implementing such alert systems include measurable reductions in the length of hospital stays and ICU admission rates. Shorter hospital durations are directly correlated with reduced healthcare costs and lower patient exposure to hospital-acquired infections, which are significant contributors to morbidity. Furthermore, early stabilization of patient conditions decreases the risk of readmissions, a key indicator of quality care and an important metric for hospital performance evaluations.\n6.3 Limitations\nWe note several limitations of our study and propose potential solutions to mitigate them in future studies. First, the exclusion of patients currently in the Intensive Care Unit (ICU) from the cohort means that the model‚Äôs predictive capabilities do not extend to the subset of the hospital population that is already under critical conditions. This omission restricts the model‚Äôs effectiveness in predicting emergency responses and mortality among the most critically ill patients, who could potentially benefit the most from accurate early mortality warning systems.\nSecondly, the study could benefit from incorporating data from other modalities, such as medical images or clinical notes. The inclusion of imaging data, for example, could offer critical insights into conditions that are not obviously measured in vital signs or lab results. Similarly, although we make use of LLMs, the availability of additional clinical notes, such as nurse notes and radiology notes, could provide additional valuable information such as psychological issues, previous admission history, or other more rarely recorded information. The choice to opt out of these modalities in the current study is due to the consideration that both notes and image data are well known for having processing delays in data recording systems, which would not be readily available often for real-time alarms. In addition, clinician and other related notes contain large amounts of personally identifiable information that would require careful removal process before being able to be used by third-party data storage or process service, which often involve significant manual labors and physician reviews.\nWe also note that the operational effects of the deterioration index cannot be straightforwardly evaluated. This is largely due to the presence of many endogenous factors that are out of the control of any single model predictions but rather a combination of logistical factors within the hospital organizational structure. However, although these changes are highly subjective, by providing this clinical support tool to the physicians, inefficiencies regarding data processing and integration of multimodal information can be alleviated and provide value to hospital managers.\nLastly, we note that the existing system was developed for a 24-hour continuous update schedule, however, physician and nurse schedules could be shorter than this existing timeline. An updated release of 6-hour or 8-hour interval predictions could potentially align more closely with their shift changes.\nAuthor Contributions Y.M. led the efforts of model development, planning and performing experiments, writing code, analyzing results, and writing the manuscript, and contributed to data processing and model implementation. K.V.C. contributed to data processing, developed models, planned and performed experiments, wrote code, analyzed results, and edited the manuscript. G.S. contributed to the research, validation of the results, and edited the manuscript. J.M. collected data used in the study. M.L. wrote code, contributed to model development and analyzing results, and implementation of the model in the hospital system. H.H. supervised and led the medical team, contributed to experimental design and model evaluation, and edited the manuscript. D.B. directed the overall project, from concept and research to implementation, and edited the manuscript.\nAcknowledgments The authors would like to thank the Hartford HealthCare team for their help with implementation, feedback, and discussions. We are grateful to Jeff Mather from Hartford and Ali Haddad-Sisakht for their support in data extraction. The authors Anne Gvozdjak, Aaron Y. Zhu, and Demetrios C Kriezis for providing support on computational experiments to our work. Finally, we acknowledge the MIT SuperCloud and Lincoln Laboratory S0upercomputing Center for providing computing resources and technical consultation"
  },
  {
    "article": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing\nAbstract\nMulti-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves wall-clock speedup on coding benchmarks with minimal loss in performance. Based on Jacobi Forcing Model‚Äôs trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to higher token acceptance count per iteration and nearly wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at [URL_REMOVED]\n1 Introduction\nModern large language models (LLMs), such as GPT-5 (openai2025gpt5), Gemini-2.5 (google2025gemini2.5), and Kimi-K2 (team2025kimi), excel at complex and interactive agentic tasks. Yet, autoregressive (AR) decoding generates tokens sequentially, limiting parallelism and leading to high latency. To address this, recent work explores predicting multiple future tokens natively in transformer-based models without relying on auxiliary draft models. A popular approach is diffusion-based language models (dLLMs), which relax left-to-right generation by modeling the entire sequence jointly and decoding via full-sequence denoising (nisonoff2024unlocking; schiff2024simpleguidance; khanna2025mercury). This, in turn, enables highly parallelizable computation. However, open pretrained dLLMs (ye2025dream7b; zhu2025llada15_vrpo; nie2025large_language_diffusion_models) underperform AR models in generation quality, mainly due to their negative evidence lower bound (NELBO) training objective, a loose bound on AR‚Äôs negative log-likelihood (NLL) that is proven less efficient (cheng2025sdar; niescaling; arriola2025block).\nTo preserve the generation quality of frontier AR models, the community has adapted high-quality AR models into dLLMs for parallel decoding (jetastraSDAR; wu2025fastdllmv2efficientblockdiffusion). Concretely, they perform block-wise perturbations of pretrained data by randomly masking tokens following the dLLMs recipe, and leverage these data to posttrain AR models by modifying the attention mask to enable block-wise bidirectional attention and replacing the training objective from NLL to NELBO. This adaptation delivers limited speedup under quality constraints, primarily due to a significant pretrain-to-posttrain mismatch. Specifically, enforcing block-wise bidirectional attention conflicts with the causal prior in pretrained AR models. For instance, SDAR cheng2025sdar suffers substantial quality drops when large block sizes (e.g., 64 or 128) are adopted. Moreover, the masked data distribution during post-training deviates sharply from the natural data distribution seen during pretraining, making the adaptation difficult to learn. Consequently, AR-adapted dLLMs are costly to train as shown in Figure 1, and fail to scale speedup reliably with larger block sizes, thereby underutilizing modern AI accelerators whose abundant FLOPs could otherwise be leveraged to decode more future tokens per iteration and further reduce end-to-end latency.\nIn this work, we introduce Jacobi Forcing, a progressive distillation technique that addresses this pretrain-to-posttrain mismatch. It trains AR models on their own generated data without any modifications of causal attention. This is made possible by collecting trajectories using Jacobi Decoding‚Äîa widely adopted parallel decoding technique for AR models (song2021accelerating; santilli2023accelerating). It first randomly initializes a block of n tokens and feeds it to the AR models to iteratively update it, and eventually, the block converges to the same n tokens generated by AR decoding, forming a trajectory between the randomly initialized point and the converged point. The full sequence is generated block by block. Prior works including CLLM (kou2024cllms_consistency_large_language_models) and CEED-VLA (song2025ceed_vla_consistency_vla) design a consistency loss to map any point along the trajectory to the converged point, which in turn teaches AR models to predict multiple correct tokens in one iteration simultaneously. However, they face a similar limitation as AR-adapted dLLMs: as block size increases, the number of tokens correctly decoded per iteration remains essentially constant. Jacobi Forcing addresses this by introducing a noise-aware causal attention that teaches the model to predict the converged point within each block conditioned on previous unconverged blocks, and we show it enables more useful future tokens to emerge in each block‚Äôs trailing tails. Furthermore, Jacobi Forcing repeats this distillation procedure for the trained model and involves more noisy data with a larger block size for progressive distillation.\nWe observe Jacobi Forcing Model has a stronger capability of generating correct future tokens conditioning on noisy context, consistent with our training objective. To better utilize this characteristic, we design a rejection-recycling and multi-block decoding algorithm for further inference optimization. Rejection recycling reuses high-quality consecutive tokens discarded from past Jacobi iterations to generate candidate token sequences, enabling the decoding of more accurate tokens via verifying multiple branches in a single iteration. Multi-block decoding maintains and refines multiple blocks simultaneously, where correct tokens are decoded in subsequent blocks even when preceding blocks remain unconverged for further speedup.\nExperiments show Jacobi Forcing Model can serve as very efficient parallel decoders with up to improvement in generation speed across coding and math benchmarks. It also effectively generates higher quality draft n-grams from future tokens within each block, as observed in Section 4. Using rejection-recycling and multi-block decoding makes use of future n-grams and further boost speedup to around .\nIn summary, key contributions of this paper includes:\n-\n‚Ä¢\nWe introduce Jacobi Forcing to train AR models as fast parallel decoders, Jacobi Forcing Model, with up to generation speedup.\n-\n‚Ä¢\nWe empirically observe and qualitatively verify Jacobi Forcing Model has both higher fast-forwarded token count and a useful n-gram count in comparison with baseline models.\n-\n‚Ä¢\nWe propose rejection-recycling and multi-block decoding to make use of higher quality draft n-grams from future tokens within each block, and apply them to Jacobi Forcing Model boost generation speed nearly up to across various benchmarks.\n2 Preliminary\nThis section reviews the basics of Jacobi decoding and consistency distillation training to accelerate Jacobi decoding of AR models.\n2.1 Jacobi Decoding\nGiven a prompt and a pre-trained LLM parametrized by , the standard AR decoding under the greedy strategy produces a response sequentially as follows:\nwhere . This process requires forward passes of the LLM to generate tokens . The inherently sequential nature of AR decoding limits practical efficiency when generating long sequences. Jacobi decoding (song2021accelerating; santilli2023accelerating) addresses this bottleneck by reformulating token generation as solving a system of nonlinear equations:\nwhere . This system can be solved in parallel using Jacobi fixed-point iteration (ortega2000iterative). Starting from a randomly initialized -token sequence , the update at each iteration is:\nNotably, for LLM, the above maximization problems can be solved in parallel by using a causal attention mask, i.e., only one forward pass of the LLM is required to obtain based on . The iteration exits at some such that and we define as the fixed point. Let denote the Jacobi trajectory. It can be proven that is identical to AR decoding under greedy strategy (song2021accelerating).\nTo generate a long response of length , Jacobi decoding is applied sequentially over blocks of size until the <eos> token appears in a fixed point. Let denote the fixed point obtained for the -th block. The full output is then constructed by concatenating fixed points from consecutive blocks:\nwhere denotes the number of blocks generated before termination.\n2.2 Consistency Distillation\nDespite the promise, Jacobi decoding achieves little speedup over standard AR decoding (santilli2023accelerating; fu2024lookahead), as it rarely predicts more than one correct111By correctness, we mean alignment with the AR decoding result under a greedy sampling strategy. token within one fixed-point iteration. To address this, recent works such as CLLMs (kou2024cllms_consistency_large_language_models) propose consistency distillation, a training approach designed to accelerate convergence to the fixed point from arbitrary states on a Jacobi trajectory. The key idea is to introduce a consistency loss that encourages an LLM to predict multiple tokens simultaneously:\nwhere and denotes the KL divergence aggregated across the tokens in a block. Here, denotes sampling a block index uniformly at random, and denotes randomly sampling from the Jacobi trajectory of the -th block.\nCLLMs build upon this idea by first collecting Jacobi trajectories, obtained by running Jacobi decoding with on a set of prompts. The model is then trained with a joint objective that combines the consistency loss in Eq. [ADDRESS_REMOVED] AR loss, achieving up to a speedup over AR decoding while maintaining quality. Similar training objectives have also been adopted for inference acceleration in other domains, such as action prediction in VLA models (song2025ceed_vla_consistency_vla).\n3 Methodology\nIn this section, we first discuss the training challenges of consistency distillation with larger block sizes , and then present Jacobi Forcing, a progressive consistency distillation method designed to mitigate this bottleneck, and denote LLMs trained under this paradigm as Jacobi Forcing Model. Furthermore, by observing Jacobi Forcing Model‚Äôs trajectories under vanilla Jacobi decoding, we introduce rejection-recycling and multi-block decoding strategies to improve its efficiency.\n3.1 Jacobi Forcing\nProgressive Noise Schedule. In Jacobi decoding, we maintain strict causality within each block, where each token is updated in accordance with Eq. 3. Consider the -th block of size is been decoded at some iteration step . Assume the first tokens have been accepted, and we denote as the future token as shown in Eq. 6.\nwhere is the clean context, is the noisy222By noisy, we refer to tokens in the non-converged point along the Jacobi trajectory that that differ from those in the fixed point at the same positions. context. While the training objective in Eq. [ADDRESS_REMOVED] token prediction in this setting, it‚Äôs observed from kou2024cllms_consistency_large_language_models that predicting is hard when it‚Äôs conditioned on a long noisy context under large block sizes (e.g., ).\nTo address this challenge, we instead split a large block into smaller blocks (e.g., ) with noise ratios determined by a predefined schedule . Each denotes the fraction of noisy tokens in a block. The noise schedule follows a cyclic strategy with window size , where the noise ratio linearly increases from 0 to 1 within each window, i.e.,\nThis progressive schedule ensures that each block retains a partially clean context, thereby shortening noisy tokens dependencies. In particular, it reduces the longest span of consecutive noisy inputs for any prediction from assuming for all blocks using a random schedule to using a progressive schedule, which facilitates learning. Empirically, we find this progressive schedule to be more effective than a purely random noise schedule (Table 4).\nProgressive Distillation Loss. Let denote the point along the -th block Jacobi trajectory with several noisy tokens closest to . The training objective is to predict tokens correctly within each block, aggregating losses across blocks to reduce gradient variance and stabilize optimization. Accordingly, we introduce a new loss term, progressive consistency loss, which optimizes under the progressive noise schedule in Eq. 7:\nAR Loss. kou2024cllms_consistency_large_language_models notes that using only the consistency loss (Eq. 5) must be supplemented with an AR loss to maintain generation quality. Our preliminary experiments show that using only the consistency objective (Eq. 8) produces the same effect. This motivates our inclusion of a conventional AR loss term in the final training objective to safeguard output quality:\nwhere is a tunable weight that balances the two learning objectives.\nNoise-aware Causal Attention. In CLLM, loss from each training step is computed based on KL divergance from one block instance in Eq. 5. This learning objective is to train correct token prediction in the setting where there is only a big block (Eq. 6). Moreover, in both Eq. 5 and Eq. 8, the loss term computation involves two forward passes using a conventional causal mask since each involves a distinction sequence. As a result, it requires forward passes to compute all loss terms in Eq. [ADDRESS_REMOVED] passes to compute gradients, resulting in low training efficiency. We reduce the number of forward and backward passes from to by introducing a sequence packing technique and a block-wise sparse attention mask. We illustrate the sequence packing that interleaves and for the entire complete sequence in Figure 2 for computation, in contrast with conditioning each unconverged only on clean tokens for consistency distillation with in Figure 2.\nProgressive Distillation for Larger Block Sizes. In training Jacobi Forcing Model on trajectories from the original AR model, we find that speedup scales with training steps and saturates at large step counts, likely due to significant data distribution shifts from extensively trained models. To break this ceiling, we collect an additional round of Jacobi trajectories with progressively larger block sizes from the Jacobi Forcing Model empowered with multi-token prediction capability and further train it on newly generated trajectories. This yields a further 20% speedup with only minor performance degradation. Detailed training configurations are in Section 4.1.\n3.2 Inference Optimization\nBehavior of Jacobi Forcing Model. Jacobi Forcing Model is trained to have a stronger capability of generating correct future tokens conditioning on noisy tokens. Qualitative analysis in Figure 4 illustrates that it indeed brings the quality improvement: fixed-point segments emerge within the noisy tokens of the unconverged point. Furthermore, these segments progressively extend (e.g., the number of red tokens increases from point 1 to point 2 in Figure 4), even under noisy context, consistent with our training patterns. In this section, we focus on how to translating this qualitative observation of draft quality improvement into qualitative speedup.\nRejection Recycling. Prior work has shown that n-grams produced during Jacobi iterations can be verified in parallel and reused in subsequent iterations (fu2024lookahead). As illustrated in Figure 4, such n-gram sizes could be large in Jacobi Forcing Model. If correctly verified, many tokens can be fast-forwarded in one iteration. In particular, we initialize a fixed-size n-gram pool constructed from noisy token sequences observed at unconverged points during Jacobi decoding. During decoding, if the pool contains an n-gram whose first token matches the last accepted token of the current point, we extend this token by concatenating it with its subsequent tokens to form new candidates. These candidates are then verified in parallel by appending them along the batch dimension. At each iteration, we select the candidate that yields the largest number of newly accepted tokens. For instance, this strategy enables skipping from point 3 to point 5 in Figure 4, as the fixed-point segments in point 3 yield higher-quality candidates.\nMulti-block Decoding. In addition to high-quality n-grams in the draft, we also observe the increasing number of stationary tokens, which are correctly predicted with preceding noisy tokens and remain unaltered through subsequent iterations. Together they yield higher quality drafts. To make use of the property, we introduce multi-block decoding, a new decoding paradigm that maintains and refines up to blocks simultaneously. It marks the block closest to the effective KV cache boundary as the real-active block and all the other blocks as pseudo-active blocks. Only tokens within the real-active block are accepted and committed to KV cache. Tokens in pseudo-active blocks are only pseudo-accepted, conditioning on prior blocks; once converged, pseudo-active blocks will wait until they are promoted as the real active block, where all tokens will be verified again, but now with a higher-quality draft. A detailed description is provided in Algorithm 1 (with rejection recycling) in Appendix 1 and with an example in Figure 3. Note that both rejection recycling and multi-block decoding are lossless as they employ greedy rejection sampling for token acceptance in the real-active block (leviathan2022speculative_decoding).\n4 Experiments\n4.1 Evaluation Settings\nModels and Datasets. We evaluate Jacobi Forcing Model across coding benchmark. For coding benchmarks, we train Qwen2.5-Coder-Insutrct (hui2024qwen2coder) on OpenCodeInstruct (ahmad2025opencodeinstruct) and test on the HumanEval (chen2021evaluating), MBPP (austin2021program). On OpenCodeInstruct, we curate question instances that come with generations that pass all unit tests, from where we use 450k prompts for trajectory generation and training. For mathematical tasks, we train Qwen2.5-Math-7B-Instruct (yang2024qwen25mathtechnicalreportmathematical) on the math split of Openthought2 (guha2025openthoughtsdatarecipesreasoning) and test on GSM8K (cobbe2021gsm8k), and MATH (hendrycks2021math). On Openthought2, only mathematical prompts are considered, from where we apply the same training settings for trajectory generation and training.\nTraining Settings. All training and inference are conducted on instances equipped with 8x NVIDIA A100-80GB GPUs and 8x NVIDIA H200 GPUs. All models are trained with a learning rate of , a batch size of 4, and a max new sequence length of 2048. For Jacobi Forcing Model, we adopt a linear progressive noise schedule, initial block size at 16, window size at 16, and train for 10k steps, and a second round of training with block size at 32, window size at 8, and train for another 10k steps. Ablation studies on parameter choices are presented in Section 4.3.\nBaselines. Our main objective in this section is to compare performance and efficiency between diffusion-based parallel decoders and the AR-based parallel decoder, Jacobi Forcing Model. The dLLM baselines also have the capability of generating a single block of tokens or multiple consecutive blocks of tokens together. Specifically, we compare Jacobi Forcing Model with state-of-the-art (SOTA) dLLMs including LLaDA-7B (nie2025llada), Dream-7B (ye2025dream7b), fast-dLLM (wu2025fast_dllm) and D2F (wang2025diffusion_forcing). We also compare Jacobi Forcing Model with AR-based parallel decoder, including vanilla Jacobi decoding (santilli2023accelerating) and CLLM (kou2024cllms_consistency_large_language_models). In this work, we do not focus on speculative decoding methods, because the models themselves don‚Äôt serve as parallel decoders without supplemental architecture modifications (e.g. via additional heads) (cai2024medusa; li2024eagle; li2024eagle2; li2025eagle3) or separate draft models (leviathan2022speculative_decoding; liu2024online_speculative_decoding). In addition, to situate Jacobi Forcing Model among broader AR-acceleration techniques, we present in the Appendix B a complementary comparison with speculative decoding and consistency-distilled baselines.\n4.2 Results\nPerformance. The performance metrics are the greedy generations‚Äô strict accuracy (pass@1) on HumanEval and MBPP. Table 1 compares Jacobi Forcing Model with both dLLMs and Jacobi decoding baselines. On A100 GPUs, our results show that on both benchmarks, Jacobi Forcing Model consistently achieves competitive accuracy with a much better speedup at the same parameter scale. In particular, for structured generations like Python coding, Jacobi Forcing Model achieves speedup in comparison with the AR baseline, speedup comparing to dLLM baselines, and 2.[ADDRESS_REMOVED]-dLLM and D2F with techniques like adding block-wise KV cache, bidirectional KV cache and pipelined parallel decoding. For speedup evaluation, we run all evaluations with a block size of 128 except for Jacobi Forcing Model (MR) since MR takes extra FLOPs for multiblock decoding and parallel verification.\nMoreover, we report the speedup and problem solve rate (test@1) on GSM8K and MATH in Table 2. Across both benchmarks, the Jacobi Forcing Model substantially outperforms the AR baseline with 3.70 speedup while preserving competitive accuracy. In the MATH benchmark, Jacobi Forcing Model delivers a 150.7 TPS while even slightly improving the solve rate from 77.0% to 77.4%, highlighting its ability to achieve both high efficiency and accuracy.\nWe also present speedup comparison across different AR-based techniques with Jacobi Forcing Model on B200 in Table [ADDRESS_REMOVED]-forward count to TPS conversion rate with more compute on B200.\nOn B200, with the block size at 128 and verification size at 4 (rationale provided in Section 4.3), we apply multi-block decoding using Jacobi Forcing Model and the results are presented in Figure 3. The running window method is an optimized variant of Jacobi decoding designed for settings where many tokens are accepted per iteration. It maintains a fixed-size active block by replenishing draft tokens to the original block size as accepted tokens are committed to the KV cache. The results demonstrate that multi-block decoding with rejection recycling consistently achieves the highest number of fast-forwarded tokens per iteration, particularly in the larger block-size regime as shown in Figure 5.\n4.3 Ablation Study\nTraining Noise schedules. We evaluate three types of noise schedules: random, linear progressive, and reverse progressive. In the random schedule, the noise step for each block is sampled uniformly as during sequence packing in Jacobi Forcing Model training. The linear progressive schedule follows Eq. 7, while the reverse progressive schedule applies a linearly decreasing noise ratio from 1 to 0 within each window. Results in Table 4 show that the linear progressive schedule significantly outperforms the other two when the window size is 8. Intuitively, with , this schedule corresponds to adding noise more aggressively across blocks within each window, roughly two additional noisy tokens per future block, until the final block where all tokens are noisy.\nTraining Mask types. We train Jacobi Forcing Model on the objective in Eq. 8 with noise-conditioned mask implementation (Figure 2). An alternative implementation of the mask is to condition all blocks within a window on a clean context. In other words, for every query, it sees blocks from all preceding windows as of Figure 2]), and all blocks within its own window as of Figure 2. Intuitively, it makes token predictions in later windows and blocks easier to learn because now they are conditioned on a cleaner context. We summarize results in Table 5, where it shows noise-conditioned mask is more effective in empowering Jacobi Forcing Model with speedup while maintaining generation quality.\nInference FLOPs Utilization Analysis. Jacobi Forcing Model (MR) involves both multi-block decoding and rejection-recycling, where each technique consumes extra FLOPs for parallel drafting and parallel verification, respectively. To maximize hardware utilization, we experiment with how end-to-end decoding latency changes as the total number of decoded tokens changes. We use Jacobi decoding to run the experiments and the results are shown in Figure 5. On H200 GPUs, Jacobi decoding with block sizes up to 64 shows no latency penalty and only minor degradation at 128, particularly in the high fast-forwarding regime. The result is consistent across accepted token counts fixed at , indicating that up to 126 tokens can be decoded in parallel with shared KV without significant latency overhead. We provide a more detailed analysis in Appendix D.\nInference Configuration Search. Beyond block size, the main tunable parameters for Jacobi Forcing Model (MR) inference are verification size (entries verified in parallel with shared KV for rejection recycling), number of blocks, and initialization threshold. We observe that performance gains from additional blocks saturate at block size = 2 as later drafts degrade quickly. The initialization threshold, defined as the fraction of the first block completed before launching the next, can be optimized via grid search and shows consistently optimal performance at for block size 64 across verification sizes 2 to 8. For maximum FLOPs utilization, we use block size = 64, verification size = 4, where wall-clock speedup remains stable until parallel decoding exceeds 256 tokens. More details on inference configuration search given the FLOPs budget can be found in Appendix E.\n5 Related Work\nDiscrete Text Diffusion. dLLMs represent a new paradigm that challenges traditional autoregressive (AR) modeling by replacing left-to-right causality with iterative denoising, enabling parallel multi-token generation (li2024derivativefree; nisonoff2024unlocking; schiff2024simpleguidance). Closed-source dLLMs (e.g., Gemini Diffusion (gemini_diffusion_2025; khanna2025mercury; seed_diffusion_2025)) show huge throughput improvement while maintaining competitive code and text quality, underscoring better accelerator utilization. On the open-source side, community dLLMs with released code and weights delivered strong throughput and controllability via parallel iterative denoising, yet remaining less efficient than autoregressive decoding (ye2025dream7b; zhu2025llada15_vrpo; nie2025large_language_diffusion_models; jetastraSDAR; gong2024scaling_diffusion_language_models). Recent efforts (arriola2025block; wu2025fast_dllm; liu2025dllm_cache) further push the efficiency and scalability of dLLMs.\nJacobi Decoding. Jacobi decoding reframes AR generation as a parallel fixed-point update over all positions, with convergence linked to greedy AR, and has been instantiated using Jacobi (Gauss-Seidel) iterations (song2021accelerating; santilli2023accelerating). Building on this, follow-ups either refine the decoding procedure or train models as parallel decoders to exploit parallel: CLLMs (kou2024cllms_consistency_large_language_models) fine-tune LLMs with consistency distillation to predict multiple correct tokens per iteration and speed convergence; CEED-VLA (song2025ceed_vla_consistency_vla) brings the similar idea to robotics. Other strands adapt Jacobi to new regimes, including FastCoT (zhang2023fast) for reasoning with parallel CoT updates, Speculative Jacobi Decoding (teng2024accelerating) for sampling in AR Test-to-Image, and MSN, TR-Jacobi (wang2024make) that injects denoising training and a retrieval-augmented Jacobi strategy.\nSpeculative Decoding. Speculative decoding speeds up AR generation by letting a lightweight drafter propose several future tokens and having the target model verify them in one pass (leviathan2022speculative_decoding; chen2023accelerating). It preserves the target model‚Äôs distribution while reducing latency. Subsequent work improves proposal quality and verification efficiency: online speculative decoding (OSD) (liu2024online_speculative_decoding) adapts draft models to user query distributions via continual distillation, substantially improving token acceptance and reducing inference latency. Medusa (cai2024medusa) adds multi-head drafters to the base LM to produce verifiable token blocks; EAGLE, EAGLE-2 (li2024eagle; li2024eagle2) reuse target features for feature-level drafting, and EAGLE-3 (li2025eagle3) scales this idea with multi-layer fusion. Lookahead Decoding (fu2024lookahead), PLD (saxena2023pld; somasundaram2024pld), and REST (he2023rest) dispense with a separate drafter, instead synthesizing speculative candidates directly from context or future tokens. The self-speculative decoding paradigm shares a close connection with the Jacobi decoding adopted in this work.\n6 Conclusion\nIn this work, we propose a progressive distillation technique for training AR models as faster and more accurate parallel decoders compared to dLLMs. Unlike CLLM (kou2024cllms_consistency_large_language_models), which directly trains models to predict large blocks of tokens in parallel, our approach introduces a progressively more difficult learning objective. This is achieved through a progressive noise schedule, combined with a sequence packing strategy and a noise-aware causal mask, enabling parallel token prediction conditioned on noise. The model is further improved through iterative training, where trajectories are regenerated with progressively larger block sizes. The resulting model, Jacobi Forcing Model, achieves a 3.8 speedup while largely preserving accuracy. Analysis of its generated trajectories shows that Jacobi Forcing Model produces high-quality draft tokens toward the tail of sequences. In addition, we introduce rejection recycling and multi-block decoding, which together bring tokens accepted per iteration to as high with nearly speedup on HumanEval using on both A100 and B200 GPUs.\nEthics Statement\nAll authors have read and adhere to the ICLR Code of Ethics. This work does not involve human subjects, sensitive personal data, or experiments with the potential to cause harm. No confidential or proprietary data were used. The methods and experiments were conducted in accordance with principles of research integrity, fairness, and transparency. Potential societal impacts, including limitations and biases of large language models, are explicitly discussed in the paper. All conclusions are the sole responsibility of the authors.\nReproducibility Statement\nWe have made significant efforts to ensure the reproducibility of our results. Detailed descriptions of the models, datasets been used, as well as hyperparameter choices are included in the main text. All datasets used are publicly available, and the preprocessing steps are fully documented. Ablation studies are provided to validate robustness of results. These resources collectively allow independent researchers to verify and reproduce our work.\nUse of LLM\nDuring the preparation of this manuscript, large language model was used to refine grammar and improve clarity. The authors carefully reviewed and revised all outputs to ensure the text reflects their original ideas and take full responsibility for the final content, including all statements and conclusions.\nAppendix A Detailed Decoding Algorithm\nWe present the detailed algorithm for multi-block decoding and rejection sampling introduced in Section 3.2. Rejection recycling reuses high-quality consecutive tokens discarded in previous Jacobi iterations to construct candidate token sequences. Multi-block decoding jointly maintains and refines multiple blocks, allowing correct tokens in later blocks to be decoded even when earlier blocks remain unconverged, thereby further improving decoding throughput. These two techniques are orthogonal and can be seamlessly combined. As shown in Table 3, their combination yields an improvement of over 30 TPS compared to vanilla Jacobi decoding on a B200 GPU.\nAppendix B Further Baseline Comparisons\nThe main text focuses on comparisons between Jacobi Forcing Model and diffusion-based parallel decoders, as well as AR-based parallel decoders, under a controlled setup where AR variants share the same backbone (Qwen2.5-Coder-7B-Instruct). This appendix extends the comparison to (i) distilled discrete diffusion models and (ii) state-of-the-art speculative decoding baselines.\n‚àóHere we report the strongest checkpoints released by the authors, in principle EAGLE-3 and HASS are lossless in comparison with greedy AR checkpoints if they were trained with the Qwen2.5-7B backbone.\nDistilled dLLM baselines.\nA distilled dLLM baseline is useful for mapping Jacobi Forcing Model against contemporary training techniques for discrete diffusion models. dParallel (chen2025dparallel) performs trajectory-level consistency distillation on a discrete diffusion model to accelerate token sampling while aiming to preserve quality. We adopt the technique as the latest distilled dLLM baseline.\nAs shown in Table 6, on HumanEval, Jacobi Forcing Model (MR) attains a noticeably stronger speed‚Äìquality profile than dParallel: Jacobi Forcing Model (MR) achieves higher accuracy and achieves more than higher TPF and TPS. On GSM8K, Jacobi Forcing Model improves accuracy by absolute points with about higher TPF and TPS (GSM8K numbers are omitted from the table below for brevity). These gaps indicate that, relative to latest consistency-distilled dLLM of comparable scale, Jacobi Forcing Model occupies a more favorable point in the speed‚Äìquality trade-off space.\nSpeculative decoding and recent dLLM baselines.\nSpeculative decoding (SD) forms widely used family of AR acceleration methods. To place Jacobi Forcing Model among such approaches, this appendix includes comparisons against two recent SD methods, EAGLE-3 (li2025eagle3) and HASS (zhang2025hass), which represent stronger baselines than earlier methods such as Medusa and Medusa-2.\nThe comparison in Table 6 also includes two recent dLLM baselines, Fast-dLLM v2 (wu2025fastdllmv2) and SDAR (cheng2025sdar), in addition to the community dLLM and D2F variants discussed in the main text. Fast-dLLM v2 improves blockwise diffusion efficiency via enhanced scheduling and caching, while SDAR introduces a synergistic diffusion‚Äìautoregressive paradigm for scalable sequence generation.\nAppendix C Mapping Noise Schedule to Training Sequence for Progressive Consistency Distillation\nWe elaborate the process of mapping the noise schedule to arrive at the training sequence in Figure 2.\nFor each training sample, let the target model‚Äôs complete generation of length be . Given a training-time block size and a noise schedule (e.g., the linear progressive schedule in Eq. 2), we partition into blocks of size . The schedule is applied over a window of blocks, yielding noise ratios defined in Eq. 7. For each block, we select the point along its Jacobi trajectory whose fraction of unconverged tokens is closest to , and use that point to form the corresponding noisy block. A full illustration is shown in Figure 6.\nA complete training sequence contains both noisy and clean blocks. Clean blocks are the original partitions of , while noisy blocks are constructed as above. We interleave each noisy block with its corresponding clean block so that a single forward pass, together with the custom attention mask in Figure 4, produces teacher logits on clean blocks for the AR loss and student logits on noisy blocks for the consistency loss. Under the progressive noise schedule, the longest consecutive noisy span within any block is , which is much smaller than the naive worst case where every token in every block is noisy.\nAppendix D Understanding TPF and FLOPs Trade-off\nTo estimate how many tokens can be decoded in parallel before hitting the hardware roofline, we profile generation-only latency as a function of the total number of simultaneously decoded tokens (horizontal axis in Figure 7), sweeping several block sizes . On H200 and B200 (left and middle panels), the curves for are essentially flat as we increase the parallel token count up to tokens, and only start to grow noticeably when we push beyond that to tokens. This plateau followed by an approximately linear region is the empirical roofline: up to batched tokens the GPU has spare FLOPs and KV bandwidth, so extra tokens are almost ‚Äúfree,‚Äù whereas beyond that point the device becomes compute- or memory-bound and latency scales roughly linearly.\nOn A100 (right panel of Figure 7), the plateau is shorter: generation time is nearly constant up to parallel tokens, but increases steeply once we go beyond 128 and approaches linear scaling by 256 tokens. Taken together, these measurements suggest operating near the ‚Äúknee‚Äù of each roofline, which corresponds to parallel tokens on A100 and parallel tokens on H200/B200. This motivates our final configuration: block size with verification size on H200 and B200 ( tokens), which maximizes FLOPs utilization without hurting wall-clock performance.\nThese roofline measurements imply a FLOPs budget on each GPU: once the parallel token count approaches the hardware knee, additional tokens incur an almost linear increase in cost. Consequently, there is an explicit TPF‚ÄìFLOPs tradeoff: configurations with larger blocks and more aggressive parallelism achieve higher TPF, but the extra FLOPs consumption can saturate the hardware and even degrade wall-clock latency.\nAppendix E Inference Configuration Search\nBecause of this TPF-FLOPs trade-off, choosing an inference configuration is no longer a matter of simply maximizing block size or verification depth: the configuration must respect the FLOPs budget implied by the roofline of the target GPU. Once and (initialization threshold) are fixed as training-optimal values from a separate grid search (as discussed in Section 4.3, the remaining degrees of freedom at inference are the block size and the -gram verification size, which jointly determine how much parallel draft and verify work is done per step under a given hardware constraint.\nTo explore this space, we perform a grid search over block sizes and -gram verification sizes , measuring the achieved tokens per second for each pair on the target GPU. Since the raw grid is relatively coarse, we fit a smooth surface over the discrete measurements and use it as a surrogate for continuous hyperparameter selection. Specifically, we construct a 2D polynomial design matrix in of total degree up to , select the best degree by mean squared error, and then interpolate the fitted surface onto a dense grid using scipy.interpolate.griddata with a light Gaussian-like smoothing pass.\nThe results are shown in Figure 8, and the resulting surfaces reveal a clear optimum region: tokens-per-second peaks at moderate block sizes and medium -gram verification, with the global maximum near and . Very small blocks or -gram verification size underutilize the available FLOPs, while very larger choices push the system closer to the roofline and begin to degrade wall-clock latency. This analysis justifies the final choice of using block size and -gram size on B200, which lies near the empirical optimum under each GPU‚Äôs FLOPs budget."
  },
  {
    "article": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image\nAbstract\nWe propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1 xu2024vasa, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.\n1 Introduction\nAdvances in generating 3D head avatars are revolutionizing digital interaction, effectively bridging the gap between physical presence and virtual engagement. Vivid representations of human faces serve to enhance various applications, ranging from virtual reality and gaming to remote education and online meetings. By conveying realistic facial expressions and movements, 3D head avatars can foster a deeper sense of connection within virtual environments, making interactions more personal and engaging, and significantly improving user experience and immersion.\nMost recent research on 3D head avatars utilize parametric head representations derived from 3D scans gafni2021dynamic; grassal2022neural; zheng2022imavatar; gao2022reconstructing; khakhulin2022realistic; hong2022headnerf; zheng2023pointavatar; zielonka2023instant; xu2023avatarmav; zhao2023havatar; li2023one-shot; li2023generalizable; ye2024mimictalk; qian2024gaussianavatars; xu2024gaussian; ye2024real3d; chu2024generalizable; he2025lam. The model‚Äôs shape and motion are personalized to image or video data of a reference face, and then the avatar animation is driven by an audio or video track of what the avatar will say. Despite the impressive developments to date, significant challenges still remain. One major issue with current methods is that their output often lacks the nuanced motion and subtle expressions of real human faces, resulting in less visually compelling facial dynamics. Additionally, a vast majority of existing methods gafni2021dynamic; guo2021ad; grassal2022neural; zheng2022imavatar; gao2022reconstructing; zheng2023pointavatar; zielonka2023instant; xu2023avatarmav; zhao2023havatar; li2023efficient; ye2024mimictalk; qian2024gaussianavatars; li2024talkinggaussian; xu2024gaussian require video or multiview data of the reference face for avatar modeling, limiting their utility.\nIn this paper, we present VASA-3D, an audio-driven 3D head avatar generator that transforms a single portrait image into a lifelike 3D talking head, synchronized with any speech audio input. The head avatar is modeled with 3D Gaussian splatting kerbl20233d; qian2024gaussianavatars, which ensures multiview consistency and facilitates real-time audio-driven animation and free-view rendering. Notably, the model captures and conveys dynamic expression details with a degree of realism that markedly exceeds current state-of-the-art techniques.\nWe observe that the expression terms of parametric head representations used for 3D head avatars, such as 3DMM blanz1999morphable; amberg2008expression and FLAME li2017learning, are modeled on 3D scans of just a few hundred subjects. To model more diverse and detailed facial dynamics at minimal acquisition cost, VASA-3D instead takes advantage of 2D head videos, which are abundant online. Specifically, it employs the motion latent of VASA-1 xu2024vasa, which has been trained on data from 9.5K subjects, to capture rich facial dynamics. This motion latent, though learned on 2D data, encodes implicit 3D structure, and a key contribution of our work is in translating it to a 3D avatar. We accomplish this by first mapping it to the parameters of a FLAME head model, on which 3D Gaussians are bound as done in qian2024gaussianavatars. Although FLAME‚Äôs expression parameters are modeled on just hundreds of 4D face captures, VASA-3D addresses this limitation by next predicting dense, freeform Gaussian deformations that are conditioned on the motion latent, thereby enabling the generation of more expressive 3D dynamic heads.\nOur latent motion controlled Gaussian avatar offers great potential for 3D talking head synthesis, but customizing it to just a single portrait image poses a challenging problem. Existing methods that personalize a head avatar representation using a single image khakhulin2022realistic; li2023one-shot; li2023generalizable; ye2024real3d; chu2024generalizable; chu2024gpavatar encode facial expression via a parametric head model, thus limiting expressiveness. Our solution is to utilize a pretrained portrait video generation model, namely VASA-1 xu2024vasa, to transform the reference image into a collection of frames with varied facial expressions and head poses, and then fit the avatar to these frames. As there exist limitations with this synthetic data, as well as overfitting issues due to dense deformation, we have developed an optimization framework with various training losses designed to robustly train the model despite these problems.\nVASA-3D represents a significant step forward in 3D head avatar synthesis, capitalizing on 2D head videos to enrich its model of facial dynamics and allowing customization of this advanced model using only a single portrait image. The effectiveness and realism of VASA-3D is validated through various experiments, where it demonstrates clear superiority over recent techniques. By creating lifelike avatars that accurately reflect human expressions and facial motions, this approach paves the way for more immersive and engaging virtual experiences.\n2 Related Work\n3D Face and Head Representations. A common representation for 3D head avatars is parametric mesh-based models such as 3DMM blanz1999morphable and FLAME li2017learning. These models are built on a collection of 3D head scans, from which the principal components of shape with respect to identity and expression are used as bases for modeling head and face geometry. Though compact and efficient, these parametric models provide low-fidelity mesh representations and limited detail for facial expressions, whose bases are derived from scanned data of only hundreds of subjects.\nAn alternative approach based on neural radiance fields (NeRFs) mildenhall2021nerf does not explicitly represent geometry but instead stores the radiance field of a head in a neural network. With these radiance values, head appearance at novel views can be synthesized by volumetric rendering. This approach has led to many works that can generate highly photorealistic head avatars gafni2021dynamic; hong2022headnerf; gao2022reconstructing; zhao2023havatar. However, NeRF-based methods often require multiview images or a video of the reference head, thus restricting their usage. Moreover, rendering speeds for NeRF-based models typically do not reach the levels needed for real-time applications.\nReal-time performance with high rendering quality has been achieved through representations based on 3D Gaussians qian2024gaussianavatars; xu2024gaussian; chu2024generalizable; li2024talkinggaussian; hu2024gaussianavatar, whose positions, orientations, and densities are optimized for the reference head. By accounting for the visibility of each Gaussian and rendering only those that can be seen, real-time performance is attainable kerbl20233d. Rigging 3D Gaussians to a parametric head model allows them to be dynamically controlled through parameter manipulation. Our work utilizes this representation but controls face and head motion using VASA-1 motion latents, for which residuals to these Gaussians are incorporated to precisely model the subtle expression details that these latents capture.\n3D Head Reconstruction. 3D head avatars can be reconstructed from a reference head using multi-view correspondences qian2024gaussianavatars; xu2024gaussian. For greater practical convenience, much attention has focused on one-shot methods that require only a single head image. These methods either rely on a parametric head model as a strong prior khakhulin2022realistic; chu2024generalizable or predict a volumetric hong2022headnerf or tri-plane li2023one-shot; li2023generalizable; ye2024real3d representation for NeRF rendering. In this work, we propose to leverage the considerable recent advance in 2D talking face generation xu2024vasa to synthesize close approximations of additional views of the input face, providing further data for training.\nA collection of frames synthesized in our method may resemble monocular video input, which is used in several works for 3D head avatar reconstruction gafni2021dynamic; grassal2022neural; zheng2022imavatar; gao2022reconstructing; zheng2023pointavatar; zielonka2023instant; xu2023avatarmav; zhao2023havatar; ye2024mimictalk. However, our training data differs significantly from monocular video in two respects. One is that a broad range of head poses and facial expressions can be synthesized with our approach, much beyond than what can reasonably be captured in a video of a reference head. The other is that the images generated by VASA-1 xu2024vasa lack temporal texture consistency, which creates problems when using common training losses based on pixel-wise comparisons. We overcome this issue through judicious selection of losses that are robust to such artifacts.\nHead Avatar Animation. Head avatars are typically modeled in a way that they can be driven using parameters of parametric models like 3DMM and FLAME gafni2021dynamic; grassal2022neural; zheng2022imavatar; gao2022reconstructing; khakhulin2022realistic; hong2022headnerf; zheng2023pointavatar; zielonka2023instant; xu2023avatarmav; zhao2023havatar; li2023one-shot; li2023generalizable; ye2024mimictalk; qian2024gaussianavatars; xu2024gaussian; ye2024real3d; chu2024generalizable. Their reliance on parametric models for animation encoding and control limits the expressiveness of faces. In contrast, our method drives animation using VASA-1 motion latents, which provide a richer expression representation learned from an abundance of 2D head videos. Though the 3D Gaussian splats that represent head shape in our work are rigged to a FLAME model, we incorporate residuals conditioned on the motion latents, giving expression control to these latents.\n3 Method\nOur VASA-3D framework, illustrated in Fig. 2, is built on two main ideas: adapting the VASA-1 motion latent to 3D, and leveraging the high realism of VASA-1 xu2024vasa in 2D talking head video generation to facilitate single-shot customization of the 3D head model. We train VASA-3D models with carefully-designed losses that enhance visual quality while avoiding issues that may arise from the synthesized videos.\n3.1 VASA-3D Model\n3D Gaussian Representation. Our VASA-3D model is based on 3D Gaussians kerbl20233d equipped with Gaussian deformation fields driven by the VASA-1 motion latent. The 3D head is represented as a set of 3D Gaussians each with position , rotation , scale , color , and opacity . To ease learning of Gaussian parameters and deformation fields, we make use of priors from existing 3D head parametric models. Specifically, we use 3D Gaussians bound to a FLAME model FLAMEsiga2017, a representation introduced in GaussianAvatars qian2024gaussianavatars. Unlike in qian2024gaussianavatars and other previous works, animations will be driven by the VASA-1 latent.\nWe decompose the deformation into two parts: a Base Deformation, and another deformation we call VAS Deformation. The former is driven by the FLAME parametric model to change the geometric properties of the Gaussians including position, rotation, and scale. The latter derives the fine-grained geometric and color variations, which is crucial for expressing the motion nuances captured in VASA-1 and improving the rendering quality.\nBase Deformation. Given a motion latent produced by the VASA-1 diffusion model, we first map them to FLAME parameters using two MLPs. The first MLP converts the facial dynamics code to the expression-related FLAME parameters which includes the expression PCA coefficients, eye pose, and jaw pose, respectively. The second MLP uses to predict the FLAME pose parameters including neck rotation, global rotation, and global translation. These two mappings can be written as:\nBoth and have three fully connected layers, with hidden units per layer followed by a ReLU activation function. Additionally, a shape coefficient is jointly optimized during training and fixed during inference.\nGiven these parameters, the FLAME mesh will be rigged accordingly, which drives the changes of () for the Gaussians attached to the mesh triangles. Additional details of this process can be found in qian2024gaussianavatars.\nVAS Deformation. We further learn dense Gaussian deformation fields for our 3D head model, modulated by VASA-1 motion latents. Two MLPs are introduced to predict the deformations of Gaussians in the face and neck region, respectively. The first MLP takes Gaussians in FLAME‚Äôs facial region as well as the VASA-1 facial dynamics latent as input and predicts the full transformation . We also feed the FLAME expression parameters to the MLP so that it is aware of the current base expression. The second MLP is responsible for the FLAME neck region . It takes the Gaussians, VASA pose and FLAME pose parameters as input to predict the residuals. The VAS deformations can be expressed as:\nThese two MLPs share the same architecture as and except for the different input and output dimensions. For all input Gaussian positions , we apply sinusoidal positional encoding with mildenhall2021nerf.\nAnimation and Rendering. Once trained, VASA-3D models can be animated using VASA-1 motion latents. The driving sources can be either audios or videos. For audio input, we use VASA-1‚Äôs diffusion transformer to generate motion latents. For videos, we use VASA-1 motion encoders for latent code extraction. The animation frames are efficiently rendered with Gaussian Splatting and the whole animation and rendering pipeline can run in real time on a commodity GPU.\n3.2 Synthetic Training Data Generation\nWe leverage VASA-1 to generate video frames with a diverse set of poses and expressions from the given single image. To achieve this, one can either use real speech audios and/or face videos for data generation. For example, in most of our experiments, we randomly sample up to 10 hours of video clips from the VoxCeleb2 dataset Chung_2018 to render the training data. We extract the VASA-1 motion latent for each frame and use the VASA-[ADDRESS_REMOVED] the portrait image and synthesize the corresponding frames. The paired motion latent and video frame data will be used for VASA-3D model training, which we present next.\n3.3 Robustified Model Training\nWe train our models in an end-to-end manner where all the trainable modules, including Gaussian parameters and the MLPs for deformation, are trained together from scratch.\nChallenges. Our synthesized training data and the dense free-form deformation in our 3DGS-based head model give rise to several challenges for training.\n-\n‚Ä¢\nUnlike real videos, inconsistency of temporal texture and facial shape exists among the synthesized frames.\n-\n‚Ä¢\nLarge viewing angles are often missing from the training data, leading to difficulties in shape reconstruction.\n-\n‚Ä¢\nThe inclusion of residuals for the Gaussians can lead to overfitting to the training video frames.\nWe apply the following losses to train the model effectively without succumbing to these issues.\nReconstruction Losses. We use a combination of the structural similarity index measure (SSIM) and the color difference as the photometric loss between the generated image and the ground truth image:\nPerceptual Losses. As temporal texture inconsistency can reduce the efficacy of the photometric loss, we rely on perception-level losses that measure visual quality but are robust to temporal inconsistency among the training frames. Specifically, we apply the Learned Perceptual Image Patch Similarity (LPIPS) loss zhang2018perceptual with a pretrained VGG network Simonyan15. To further improve realism, we add multi-scale patch discriminators and apply an adversarial loss for training. We employ three discriminators with different input image scales and trained them together with our model. The perceptual loss functions can be written as:\nSDS Loss. Since the synthesized video data generally covers a limited range of poses, we apply the SDS loss poole2022dreamfusion to minimize visual artifacts in side views and to enlarge the range of valid viewing angles. Specifically, we render our model from random viewing angles for which we apply the SDS loss . Random views are uniformly sampled from azimuth angles in the range and elevation angles in the range . We use StableDiffusion v2.1 Rombach_2022_CVPR as the diffusion model in our SDS loss with classifier-free guidance factor and gradient scale . The text prompt is ‚Äòhuman portrait, realistic photography, by DSLR camera‚Äô.\nThough the dense VAS deformations help in capturing detailed expressions and improving image quality, they also require careful design of regularization to avoid overfitting to each frame‚Äôs training data. In light of this, we compute and for the rendered images of the Gaussians both after base deformation and after VAS deformation, denoted as and , respectively. This approach aims for to effectively capture facial features shared across frames to the extent possible through multi-frame consistency, while focuses on modeling the residual facial details of different frames.\nRender Consistency Loss. Although the SDS loss helps to eliminate artifacts in profile regions, we found that it also tends to smooth out the details across all regions in our case. This issue is pronounced for because the Gaussian residuals are learned for each frame. Such flexibility makes susceptible to the side effect of the SDS loss, especially for regions not well captured by the current view (hence less constrained by the ground-truth reference image). In contrast, are less prone to this issue, as these Gaussians are optimized to jointly fit the multi-frame data with different poses and thus are less affected by . Therefore, we design a loss to regularize with . Specifically, for each training iteration we render an additional pair of images with and respectively, under a new view angle significantly different from the current training view. We apply a render consistency loss between these two images and as\nwhere the stop-gradient operator prevents from being negatively affected.\nIn practice, the new view is randomly sampled with the azimuth angle in the range or and elevation angle in the range . Of the two azimuth ranges, we select the one that is farther away from the view of the training frame.\nSharpening Loss. To further increase the sharpness of the rendered results, we optionally apply a contrast-adaptive-sharpening (CAS) filter CAS to the model‚Äôs rendered images and use them to further train the model. Specifically, we apply the CAS filter to the rendered image and then apply the LPIPS loss between the sharpened image and the original image. The CAS loss is applied at the end of the training process as a lightweight finetuning step.\nThe overall loss function is a combination of the aforementioned losses:\nwhere the loss weights are omitted for brevity, and stands for other loss functions introduced in qian2024gaussianavatars such as the Gaussian position and scale losses (see qian2024gaussianavatars). More details about our losses and their implementation can be found in the supplementary material.\n4 Experiments\n4.[ADDRESS_REMOVED] experiments to analyze our method as well as the design choices. In the following experiments, we use ten portraits, including five males and five females, generated by StyleGAN2 Karras2019stylegan2 to train our models. We train the models using 4 NVIDIA A100 40G GPUs and a batch size of 4. A 512512 resolution is used for both the training data and VASA-3D rendering throughout this paper.\nInference Speed. Given an audio clip as input, the animation and video frame rendering of our VASA-3D model can run at 75fps with a preceding latency of only 65ms, evaluated on a single NVIDIA RTX 4090 GPU. A real-time demo is provided in the supplementary videos.\n4.1.[ADDRESS_REMOVED] analyze the influence of dataset size (i.e., length of training videos synthesized by VASA-1) and training time (in iterations) on result quality. For each image, we use VASA-1 to render eight training datasets of different sizes, i.e., 5min, 10min, 20min, 30min, 1h, 2h, 5h, 10h, using VASA-1 latents extracted from random video clips in VoxCeleb2 Chung_2018. We evaluate the models at varying training iteration numbers (up to 400K) on our test set, which are VASA-1 generated videos of 3min for each image.\nFigure [ADDRESS_REMOVED] set improves as the dataset size and training iterations increase. It is observed that the improvements almost plateau after the dataset size reaches 2 hours and after the number of iterations exceeds 200K. Therefore, we set the total number of iterations to 200K by default in the following experiments. We also trained our model with 20K iterations and compare it against other baselines in some experiments below. Since the training data size does not affect training time, we simply set it to 10 hours. For each portrait, a 10-hour dataset can be rendered in less than 1 hour on 4 NVIDIA A100 40G GPUs. Training with 20K/200K iterations takes about 1.8/18 hours for each model.\nSome examples of our results generated by the default setting are presented in Fig 6. Our method is shown to generate high-quality 3D head renderings with accurate audio-lip sync, vivid facial expressions, and lively head motions. Video results are provided in the supplementary materials, where readers can more fully examine the quality of VASA-3D generation.\n4.1.[ADDRESS_REMOVED] setup, and the quantitative results are presented in Table 3, where the evaluation metrics include PSNR, L1 error, SSIM, LPIPS, and lip-audio synchronization score. For the lip-sync score, we use SyncNet Chung16a to assess the alignment confidence score and feature distance .\nTable [ADDRESS_REMOVED] of our VAS Deformation, compared to a basic setting with Base Deformation only. All the image quality and lip-sync metrics are significantly improved, demonstrating the importance of the VASA-latent-driven Gaussian deformation. Some visual comparisons are presented in Fig. 4. The image quality is clearly improved by VAS deformation. More importantly, the facial expressions including subtle facial details follow the ground-truth reference frame more closely, displaying the expressive talking features with nuanced facial details that are modeled by VASA-1.\n4.1.3 Effects of Different Losses\nFig. 4 exhibits improvements with the SDS loss and render consistency loss. Due to the limited pose coverage of our training data, artifacts can be clearly observed for the results without SDS loss under side views rendered at azimuth angles. The SDS loss provides additional regularization to the model and eliminates the artifacts in side views. However, it also tends to smooth out details. Adding the render consistency loss improves the results by enhancing the rendering details. Fig. 5 shows results from training with the CAS loss, where the images are further sharpened.\nTable 3 shows numerical results with different losses. The full method without the CAS loss (i.e., the fourth row) yields the best image quality in terms of the PSNR, L1 and SSIM metrics, whereas the method with the CAS loss has the best perceptual score measured by LPIPS due to the enhanced image sharpness. The lip-sync scores remain stable with different loss functions incorporated.\n4.2 Audio-Driven Generation and Comparisons\nIn this experiment, we construct our training datasets by collecting five random audio clips from the web (two males and three females), each with a total length of 25 minutes111Our model can use either video frames or audios for training. Here we use audios since some compared methods require continuous videos and cannot utilize the 10-hour dataset in Sec. 4.1 containing short video clips.. For each audio clip, we apply VASA-[ADDRESS_REMOVED] a StyleGAN2 portrait image to generate the synthetic video data. We only use the first 20-minutes to train the models, and the remaining 5-minutes are used as the test set.\nComparison with VASA-1.\nWe compare our method to VASA-1, our training data generator, to check the quality difference of the generated talking videos. Table 2 presents the frame FID Seitzer2020FID, LipSync Chung16a confidence score and feature distance , and the facial identity similarity between test video frames and driving portrait images, calculated with ArcFace deng2019arcface. The performance gap between VASA-3D and VASA-1 is small.\nComparison with Previous 3D Talking Head Avatar Methods.\nTo our knowledge, no existing method deals with the same task as ours ‚Äì i.e., single photo to expressive, fully-animatable 3D head avatar driven by audio ‚Äì making the comparison difficult. Most audio-driven 3D talking avatars are trained on long videos, and they typically do not generate full head dynamics such as head pose.\nStill, to facilitate comparison with state-of-the-art techniques for reference purposes, we consider the following methods: ER-NERF li2023ernerf, GeneFace ye2023geneface, MimicTalk ye2024mimictalk, and TalkingGaussian li2024talkinggaussian. We employ the same video data as in our VASA-3D to train these methods and use the audios from the test set to generate videos. Table 3 shows the evaluation results of different methods. Our model surpasses the other methods on the LipSync metrics by a wide margin. Its identity similarity score is marginally lower than TalkingGaussian li2024talkinggaussian and better than others. We further conduct user studies to evaluate the rendered video quality and the overall realism of the audio-driven results. 15 participants were invited to assess: 1) the visual quality of the rendered talking head videos (audio muted), with ratings from 1 to 5, and 2) the user preference of the results from the five compared methods, judged by overall realism. Our visual quality rating was significantly higher than the other methods and the users chose the VASA-3D as the best one for 93.91% of the presented cases.\nMore comparisons.\nWe further compare our method with related works on video-driven 3D talking head animation (i.e., the face reenactment task). Note that face reenactment is not a focus our work; the goal here is to further compare VASA-3D‚Äôs rendered video quality as well as its expressiveness on facial expression against prior art. Details of this experiment can be found in the suppl. material.\n4.3 Generation with Additional Control Signal\nInheriting the capabilities of VASA-1, our VASA-3D can take additional control signals besides an audio clip, such as eye gaze direction, head distance, and emotion offset. Fig. 7 as well as our supplementary video present typical results with emotion offset control, where the generated 3D talking heads closely adhere to different emotion offsets and exhibit emotive talking styles.\n4.4 Artistic Image Experiments\nWe also tested our method on artistic-style portrait images, with some examples shown in Fig. VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image and Fig. 8. Our method can effectively handle such artistic images and produce convincing 3D videos.\n5 Conclusion\nVASA-3D offers unparalleled realism for audio-driven 3D head avatars by presenting a way to leverage the extensive expression data present in online 2D head videos. With a meticulously designed architecture and training scheme, our model can be easily customized using only a single portrait image. We believe our approach paves the way for more immersive and engaging virtual experiences with 3D head avatars.\nLimitations and Future Work.\nOur method still has several limitations. Limited by the viewing angles of the synthetic training videos, it does not model the back of heads. This issue could potentially be resolved through 3D inpainting, since the back of a head is mostly rigid. Similar to VASA-1, our method does not handle dynamic elements such as accessories. Extending VASA-3D to include the upper body is another interesting direction we will explore in future.\n6 Societal Impacts and Responsible AI Considerations\nOur research aims to support positive applications of virtual AI avatars and is not intended for creating misleading or deceptive content. However, like other related techniques, VASA-3D could potentially be misused in generating the likeness of a real person. Throughout the development of VASA-3D, responsible AI considerations were factored into all stages. To safeguard against such harm, we are training face forgery detection models that incorporate our models‚Äô outputs as part of the training data. Though VASA-3D produces visually realistic results, we have found that they are easily distinguishable from authentic videos by these models and improve the models‚Äô generalizability.\nWhile recognizing the potential for misuse, it is important to acknowledge the substantial positive impact that our research technique could eventually have. We are currently examining potential benefits, such as its application in an AI coworker and AI tutor, which can enhance latent intelligence accessibility for knowledge workers and learners. These applications highlight the significance of this research and other related investigations. We are committed to developing AI responsibly, with the goal of advancing human well-being.\nA More Training Details\nFor the SDS loss , we apply it every [ADDRESS_REMOVED]. We apply with time step in a normalized range of to , with decaying by every 2,000 iterations. The CAS loss is applied after 200K iterations, and the model is fine-tuned for an additional 20K iterations with and other losses.\nIn all our experiments, the loss weights are set as , , , , , and . These weights are empirically chosen without careful tuning.\nAs mentioned in Sec. 4.1 of the main paper, our models are trained for 200K iterations by default, excluding the CAS loss finetuning iterations. Gaussian densification and pruning start at the 10K iterations, with intervals of 2K iterations. We stop this process after 100K iterations or when the total number of Gaussians exceeds 200,000.\nB More Experimental Results\nComparison with 3D Talking Head Avatar Methods. Sec 4.[ADDRESS_REMOVED] some 3D talking head avatar models, with numerical results provided including user studies. Fig.A presents some examples. Our method is shown to generate high-quality 3D head renderings with accurate audio-lip sync, vivid facial expressions, and lively head motions, surpassing the capabilities of existing 3D talking head avatar methods.\nFig. B shows the screenshot of our user study interface. To assess the visual quality of the rendered videos, we asked the participants to assign satisfaction scores from 1 to 5. Videos were presented one at a time, with the play order of different methods randomized for each test case. We asked the participants to provide their own judgment of satisfaction when watching a talking avatar on screen. Note that individual satisfaction levels may vary; however, the averaged scores provide a fair basis for comparison as each participant rated results from all methods. To evaluate user preferences for overall realism, we display the results of all compared methods side by side and ask the participants to select the one that looks the most realistic to them. Method names remained anonymous and their orders are randomly shuffled for each test case.\nComparison with 3D Face Reenactment Methods. As mentioned in Sec 4.2, we further compare our method with related works on video-driven 3D talking head animation, i.e., the face reenactment task. Note that face reenactment is not the focus of our work; the goal here is to further check our video quality and its expressiveness on facial expression.\nSpecifically, we collect 26 portraits, each with 1-minute high-quality talking videos, from the CelebVHQ zhu2022celebvhq dataset. For each portrait, we randomly selected one frame from the video and use the VASA-1 decoder to render 10 hours of training frames from it, with VASA-1 latents extracted from random VoxCeleb2 video clips. With the collected 1-minute real talking videos as test sets, we compared our model with the following video-driven 3D head avatar methods: GAGAvatar chu2024generalizable, GPAvatar chu2024gpavatar, Real3DPortrait ye2024real3d, Voodoo3D tran2023voodoo and Portrait4D-v2 deng2024portrait4d.\nTable A shows the averaged results of these methods with different metrics. Our model outperforms all the other methods under all the metrics."
  },
  {
    "article": "Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI\nAbstract.\nLLM‚Äìbased chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM-based systems. In this vision paper, we argue that interaction-level behavior is an underexamined factor shaping the environmental impact of LLM-based systems, and we outline this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy-aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long-running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM-based systems.\n1. Introduction\nLarge Language Model (LLM)‚Äìbased chatbots have become central to how users interact with computational systems across technical, educational, and analytical domains. In software engineering, conversational agents support developers as they reason about code, interpret design alternatives, and navigate complex information spaces through natural language dialogue (richards2024you; de2024unveiling). Chat-based interfaces have also been adopted in educational and design-oriented environments, where they facilitate reflective learning, scaffold problem solving, and mediate interactions with analytical or development tools (richards2024you; ashkbous2025leveraging; becchi2025gpt; bekkar2024chatbots). These developments indicate a shift toward LLM-driven conversation as a common mode of coordination between users and digital systems.\nAt the same time, sustainability concerns have gained prominence as research documents the resource demands of large-scale models. Studies examining training and inference describe how energy use is shaped by token processing, memory utilisation, and system-level behaviours (vartziotis2024carbon; jiang2024preventing; wilkins2024hybrid; stojkovic2403towards; ding2024sustainable). Broader sustainability frameworks emphasize that environmental impact arises not only from infrastructure and hardware efficiency but also from patterns of use (van2021sustainable; falk2024challenging). Current mitigation strategies, however, focus predominantly on infrastructural and architectural levers, such as optimizing deployment environments, scheduling workloads across heterogeneous hardware, and reducing emissions through operational decisions (wilkins2024hybrid). Related work has examined model reliability, noting that unnecessary generations and repeated corrections can inflate inference activity (jiang2024preventing). Together, these efforts aim to reduce the environmental footprint of LLMs, yet they primarily address sustainability through system-level optimizations, giving less attention to how interaction practices contribute to overall impact.\nA growing body of evidence suggests that conversational interaction itself may have a meaningful effect on the computational characteristics of LLM-based systems. Chatbots encourage extended exchanges, elaborated responses, and context-rich dialogue, all of which increase the number of processed tokens and shape memory usage during inference (richards2024you; de2024unveiling; coignion2024green; ashkbous2025leveraging). Interaction norms such as immediate responsiveness and continuous multi-turn engagement reinforce real-time workloads that limit opportunities for energy-aware optimisation (wilkins2024hybrid). As chat-based interfaces become integral to software engineering and other digital practices, understanding how user-facing behaviours influence the environmental profile of these systems becomes increasingly important (jiang2024preventing; wilkins2024hybrid). This creates an opportunity to explore sustainable AI from the perspective of interaction design, complementing hardware- and model-level strategies with software-based considerations grounded in how users engage with LLM chatbots.\nThis vision paper examines four dimensions of this problem (Section 3). First, it analyses how conversational patterns influence the amount of generated text and contribute to the computational cost of inference. Second, it considers how expectations of immediacy limit opportunities for energy-aware scheduling and workload consolidation in real-time interaction. Third, it examines how user behaviour shapes the cumulative energy profile of chat-based systems over time. Fourth, it investigates how context accumulation increases memory requirements during inference and affects the efficiency of LLM-based chatbots. The paper concludes by outlining research directions for sustainable AI that focus on conversational norms and user-facing practices rather than hardware or infrastructure alone (Section 4).\n2. Background\nThis section provides background on software engineering bots, the sustainability concerns associated with LLMs, and the emerging paradox that arises when LLM-powered bots are deployed for an increasingly diverse range of purposes, including activities related to sustainability itself.\n2.1. Chatbots and LLMs in Software Engineering\nChatbots have become established tools in software engineering, supporting both technical and social aspects of development work. Early studies describe chatbots as interfaces that assist developers by connecting them to services, providing feedback, and automating structured tasks across communication platforms (wessel2022software). These systems participate in conversational channels, guide newcomers, surface relevant information, suggest code improvements, assist with defect investigation, and offer just-in-time explanations during development discussions (wessel2022software; moguel2023bots).\nWith the introduction of LLMs, chatbots have become more capable and flexible. LLM-based chatbots support tasks such as code generation, bug explanations, summarization, and design reasoning, often within a single conversational interface (abedu2024llm). Developers report that conversational interaction aligns with their problem-solving habits, enables incremental inquiry, and provides opportunities for learning through natural language explanations (ashkbous2025leveraging; richards2024you). LLM-based chatbots thus extend earlier automation by offering adaptive dialogue and personalized guidance.\nAlthough these systems enhance productivity and accessibility, they also introduce challenges, such as variable behaviour, inclusivity issues, context-handling limitations, and communication mismatches (richards2025bridging; melo2025enhancing). As LLM-based chatbots increasingly support complex workflows, understanding how they operate and how they are used becomes important for assessing their broader implications. Additionally, because these chatbots rely on LLMs for their capabilities, their adoption intersects with growing concerns about the energy requirements and environmental impact of LLM-based systems (vartziotis2024carbon; jiang2024preventing; van2021sustainable).\n2.2. LLMs and Sustainability\nSustainability concerns have become central as LLMs scale in size, capability, and usage. Energy consumption in software systems has emerged as an important environmental and societal concern. Within AI, the concept of Green AI has been defined as research that produces novel results while explicitly accounting for computational cost and encouraging reductions in resource usage whenever feasible (schwartz2020green). Applied to LLMs, this work has considered both training and inference impacts. Prior benchmarking studies have estimated the energy required to generate a single model response (samsi2023words), and carbon analyses have compared the ongoing inference cost of different categories of machine learning systems, including task specific finetuned models and more general purpose models trained for multiple tasks. Deployment cost has been characterized as the energy and carbon required to perform a fixed number of inferences, such as 1,000 model outputs (luccioni2024power).\nResearch investigating the environmental impact of AI systems indicates that large models require substantial energy for training, fine-tuning, and continuous inference (van2021sustainable; falk2024challenging; ding2024sustainable). Carbon analyses of LLM services describe how operational costs depend on the number of processed tokens, hardware utilization, memory requirements, and the duration of system operation (vartziotis2024carbon; jiang2024preventing; wilkins2024hybrid). According to Luccioni et al. (luccioni2024power), inference may have an environmental impact comparable to model training, given the computational resources required to deploy modern models at scale. Although a single inference is far less costly than training, its higher frequency can lead to significant cumulative energy use. Inference is not cost-free, and the processing of each input‚Äìoutput sequence incurs energy expenditure that accumulates with repeated usage (falk2024challenging; jiang2024preventing).\nStudies of LLM inference show that longer input sequences require more computation, including increased memory access and processing time raises energy consumption relative to short-context queries (wilkins2024hybrid; jiang2024preventing; stojkovic2403towards). These effects become particularly relevant in conversational settings where context grows with each turn. Broader sustainability frameworks in computing further note that the energy footprint of a system depends not only on architecture but also on patterns of use, including interaction frequency and runtime behaviors (van2021sustainable). These observations highlight that LLM sustainability is influenced by architectural design, hardware efficiency, and user interaction patterns.\nTo address these issues, at the hardware level, some mitigation techniques have been explored. Research describes how heterogeneous allocation across energy-efficient and performance-oriented hardware can reduce resource use when tasks are scheduled according to workload size (wilkins2024hybrid; stojkovic2403towards). Broader sustainability analyses highlight how data center characteristics, energy sources, and infrastructure decisions influence the environmental footprint of AI systems (van2021sustainable; falk2024challenging; ding2024sustainable). Energy-aware scheduling and workload-adaptive allocation have also been shown to reduce energy consumption in LLM inference, although these improvements remain constrained by model size and interaction characteristics (wilkins2024hybrid; jiang2024preventing; stojkovic2403towards). At the software level, fewer mitigation strategies have been discussed in the literature, suggesting that sustainability considerations remain concentrated primarily at the hardware and infrastructure layers rather than the interaction or application layers.\n2.3. The LLM‚ÄìSustainability Paradox\nLLM-based chatbots have been developed across multiple domains, reflecting their growing role as conversational interfaces for technical, educational, and analytical work. In software engineering, LLM-based chatbots help developers reason about code, understand design decisions, and obtain tailored explanations during problem-solving (richards2024you; ashkbous2025leveraging). In educational contexts, conversational systems support learning and reflection through dialogue-based guidance, with applications ranging from programming instruction to environmental science education (zheng2025developing; nguyen2025value). The energy footprint of LLM-based code assistants has been investigated through simulated developer interactions with GitHub Copilot, suggesting that energy consumption depends on factors such as model size, quantization, streaming, and concurrency, and that a substantial portion of generated suggestions is canceled or ignored, which introduces avoidable computation (coignion2024green). The study further indicates that higher concurrency improves efficiency and that server configuration parameters, including GPU count and model size, influence energy use and latency, which points to practical opportunities for reducing environmental impact. Conversational systems have also been incorporated into sustainability-oriented applications, including eco design frameworks that assist engineers in environmentally informed decision making (ashkbous2025leveraging) and educational settings where chatbots support students in reasoning about climate change, ecological systems, and sustainability perspectives (nguyen2025value; bekkar2024chatbots).\nAt the same time, studies examining the operational footprint of LLMs show that increases in input and output length elevate token processing, memory usage, and energy consumption during inference (wilkins2024hybrid; jiang2024preventing; vartziotis2024carbon; stojkovic2403towards). In conversational settings, multi-turn exchanges, elaborated responses, and growing context windows further expand the amount of generated text and computational work (richards2024you; de2024unveiling; coignion2024green). Sustainability frameworks emphasize that usage practices influence environmental impact, indicating that frequent or intensive interactions contribute to the overall energy profile of AI systems (van2021sustainable). The paradox therefore emerges: while LLM-based chatbots are increasingly introduced to promote sustainability awareness and environmentally responsible decision making, their own operation relies on resource-intensive computation (ding2024sustainable).\n3. Rethinking User Interaction in LLM-Based Chatbots\nThis vision paper examines overlooked sustainability issues in LLM-based chatbots, focusing on how interaction design and user behaviour influence the environmental footprint of inference. Rather than prescribing solutions, we highlight how conversational para- digms, such as expectations of immediacy, extended dialogue, and persistent context shape computational demand in ways not typically accounted for. Although chatbots provide intuitive access to information, their operational characteristics carry environmental implications that remain largely invisible to end users. We argue that interaction practices are part of the sustainability problem space, affecting token throughput, workload management, and memory usage throughout the inference lifecycle (stojkovic2403towards; ding2024sustainable). The following sections outline four analytical dimensions that illustrate how user-facing behaviour contributes to the resource demands of LLM-based systems.\nInteraction Patterns and the Cost of Output Inflation.\nEnergy measurements increasingly indicate that the amount of generated text plays a substantial role in determining the computational cost of inference. Studies examining LLM inference pipelines describe how longer outputs require more computation per token, increase runtime, and reduce throughput across hardware systems (wilkins2024hybrid; stojkovic2403towards; coignion2024green). Similar observations appear in carbon analyses of LLM-as-a-service workloads, where inference energy depends on the total number of processed tokens, including both prompt and generated output (vartziotis2024carbon). Empirical evaluations of conversational assistants show that chatbots frequently produce elaborated or verbose responses, often exceeding what is necessary for user problem solving (richards2024you; de2024unveiling; ashkbous2025leveraging). These findings suggest that extended responses, which are common in chatbot interactions, elevate operational demand even when a concise answer would suffice. The prevailing assumption that richer explanations inherently improve user experience therefore warrants reconsideration, particularly in contexts where shorter responses can adequately support user tasks.\nThe Inefficiency of Real-Time Conversational Workloads.\nChatbots are designed around immediate responsiveness, creating continuous, individualized workloads. However, research on energy-efficient inference indicates that meaningful optimization occurs when requests can be distributed across heterogeneous hardware or processed according to token thresholds (wilkins2024hybrid; stojkovic2403towards). These strategies reduce energy use by allocating small workloads to more efficient systems, yet such allocation requires temporal flexibility that real-time chat typically cannot provide (jiang2024preventing; ding2024sustainable). The emphasis on instantaneous response therefore limits opportunities for workload consolidation or deferred scheduling. Treating low-latency interaction as a fixed requirement obscures its environmental implications, and a more sustainable view would recognize that responsiveness is a design choice rather than an inherent constraint of conversational systems.\nUser Behaviour and Demand-Side Sustainability.\nSustainability concerns also emerge from user behaviour. Frameworks on sustainable software practice highlight that usage patterns, including the frequency and complexity of interactions, directly influence the energy profile of a system over time (van2021sustainable). In LLM-based chatbots, routine look-ups, repeated small queries, and follow-up questions contribute cumulatively to token processing and operational energy use (jiang2024preventing). Carbon analysis of LLM services similarly emphasizes that operational footprints are tied to the number of tokens processed during inference, indicating that elevated interaction volume increases downstream energy demand (vartziotis2024carbon). Empirical studies of code assistants further show that unnecessary generations and unused suggestions increase energy consumption, and that manually triggered or selective invocation can reduce waste (coignion2024green). These insights suggest that everyday interaction choices have measurable environmental effects. Introducing user-facing guidance or lightweight alternatives for simple tasks could support more sustainable patterns of use without reducing system utility.\nThe Burden of Context Accumulation.\nConversational agents commonly retain interaction histories to preserve coherence and continuity, which increases the number of tokens processed during inference. Existing evaluations of LLM inference show that energy consumption grows with the length of input sequences, as longer requests require additional computation and longer processing time (wilkins2024hybrid; jiang2024preventing; stojkovic2403towards). Carbon analyses of LLM workloads similarly note that operational costs scale with the total number of processed tokens, indicating that accumulated dialogue history contributes to higher computational demand (vartziotis2024carbon). Although retaining full conversational context can benefit certain tasks, maintaining long histories by default imposes additional energy use. Mechanisms such as selective summarization, shorter context windows, or user-controlled context persistence could reduce unnecessary computational overhead while preserving coherence where needed.\n4. Research Opportunities\nWe identified research opportunities focused on how conversational interaction contributes to the environmental footprint of LLM-based chatbots and how new interaction models might support more sustainable patterns of use. These directions extend beyond hardware-centric mitigation and focus on user-facing, behavioral, and socio-environmental factors that shape the development of LLM-based chatbots.\nQuantifying and Mitigating Interaction Level Environmental Costs.\nCurrent research demonstrates that the total number of processed tokens strongly influences the environmental footprint of conversational systems (vartziotis2024carbon; wilkins2024hybrid; jiang2024preventing; stojkovic2403towards). Standardized procedures for evaluating these factors remain limited. A research opportunity lies in developing metrics that characterize the cost of dialogue-level behaviors, including elaborated responses, multi-turn exchanges, and accumulated interaction history. Such work would support comparative analyses of interaction patterns and provide empirical grounding for sustainable design choices of LLM-based chatbots. This direction also connects to the need for strategies that manage conversational history more efficiently. While longer inputs require additional computation during inference (wilkins2024hybrid), the contribution of accumulated dialogue to overall energy use remains insufficiently understood. Investigations into selective retention, summarization-based compression, or user-controlled context scope could show how different approaches influence both computational demand and user comprehension.\nInteraction and Task Models that Selectively Invoke Generative Reasoning.\nMany user queries do not require full generative reasoning. Routine look ups, factual retrieval, and simple transformations could be addressed through lightweight mechanisms such as templates, retrieval components, or smaller models. This suggests a need to classify tasks according to their computational requirements and identify thresholds for mode switching. Current interaction patterns often lead to repeated exchanges as users refine or narrow their requests (becchi2025gpt; coignion2024green), which elevates token usage and contributes to operational energy cost (vartziotis2024carbon). Research is needed on interaction models that reduce token throughput without diminishing utility. Possible directions include concise response modes, structured information presentation, and hybrid mechanisms that activate generative reasoning only when needed.\nAdaptive Invocation Mechanisms that Reduce Unnecessary Generations.\nCoignion et al. (coignion2024green) show that a substantial portion of code-assistant generations are canceled or ignored, indicating that many requests do not warrant full model invocation. This points to the need for interaction and task models that trigger inference only when user intent is sufficiently clear. Predictive or interaction-aware invocation strategies, such as delaying generation until a query stabilizes or using lightweight intent-detection mechanisms could reduce unnecessary computation.\nUser Guidance and Demand-Side Moderation.\nUser behavior plays a core role in shaping cumulative energy consumption, as repeated inference and high interaction volume contribute directly to operational energy use (jiang2024preventing; coignion2024green). Sustainability frameworks in computing observe that patterns of use contribute to long term environmental outcomes (van2021sustainable). This creates opportunities to design interfaces that encourage low-impact practices, such as selecting minimal response modes for simple tasks or opting for lightweight alternatives when generative dialogue is unnecessary. Studying how users respond to such guidance, and how it influences their long-term interaction patterns, would support demand side strategies that complement infrastructural improvements.\nSustainable Context Management.\nConversational systems often retain interaction histories to preserve coherence, which increases the number of input tokens processed during inference. Existing studies show that energy consumption grows with input length and with the total number of tokens handled during inference (wilkins2024hybrid; jiang2024preventing; vartziotis2024carbon; stojkovic2403towards). Research is therefore needed on managing conversational history in ways that balance coherence and efficiency. Potential directions include selective context retention, summarization-based compression, or user-controlled mechanisms for adjusting context scope. Studies on token-level environmental costs suggest that context-aware interaction design may reduce unnecessary computation, although the effects of different strategies on user comprehension and task performance remain insufficiently understood.\nUnderstanding Higher-Order and Systemic Effects.\nThe environmental footprint of LLM-based chatbots extends beyond immediate computational costs. Large-scale deployments depend on data center infrastructure that affects water use, electronic waste, and the extraction of critical minerals (falk2024challenging; ding2024sustainable). Interaction patterns define demand for this infrastructure, as repeated inference and sustained usage contribute to cumulative operational impact (jiang2024preventing). Further research is needed to examine how increased chatbot adoption influences user behavior, induces rebound effects, or redirects tasks previously conducted through lighter tools toward generative systems. Evidence from sustainability-oriented chatbot deployments indicates that conversational systems can influence decision making (nguyen2025value; bekkar2024chatbots), suggesting that indirect environmental consequences warrant further investigation.\nOverall, these research opportunities indicate that sustainable AI requires attention not only to hardware and infrastructure but also to interaction practices that shape inference workloads. By investigating task differentiation, response design, user behavior, context management, and systemic effects, future work can extend sustainability efforts into the software layer where users engage directly with LLM-based chatbots. However, these directions introduce trade-offs that warrant consideration, since reduced verbosity, relaxed immediacy, or constrained context may influence how users experience coherence and responsiveness. Interaction-level adjustments contribute to lower resource consumption, yet their effect remains bounded by infrastructural conditions that shape the environmental profile of LLM systems. A balanced view, therefore, recognizes both the value of conversational modifications and the practical limits imposed by large-scale model deployment.\n5. Conclusion\nThis vision paper explored how the sustainability of LLM-based chatbots can be observed not only by aspects related to model architecture and deployment infrastructure but also by interaction design and user behavior. While ongoing work emphasizes improvements in hardware efficiency, deployment infrastructure, and system-level optimization, our analysis indicates that conversational practices also influence the environmental footprint of inference in meaningful ways. Extended responses, expectations of immediacy, and persistent context contribute to computational demand in ways that are often overlooked. The increasing use of LLM-based chatbots across software engineering activities amplifies the importance of these issues. As chatbots become integral to development workflows, the software engineering community will need sustainable perspectives for designing and deploying these systems.\nBuilding on this motivation, we proposed research opportunities that foreground interaction-level considerations as part of sustainable AI. These include developing metrics for the environmental cost of dialogue behaviors, designing response models that reduce token demand, identifying which tasks require generative reasoning, supporting more sustainable user behaviors, and managing conversational history in ways that balance coherence and efficiency. These directions indicate that sustainability in LLM-based chatbot development requires attention to how users interact with chat-based systems and how software design choices influence long-term resource consumption."
  },
  {
    "article": "ART: Articulated Reconstruction Transformer\nAbstract\nWe introduce ART, Articulated Reconstruction Transformer‚Äîa category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.\n1 Introduction\nArticulated objects are ubiquitous in daily lives and central to human‚Äìscene interactions [38]. Accurately constructing their digital replicas is important for VR/AR, robotics, and embodied AI [53, 27, 68, 9, 28, 25, 10]. While recent 3D generation and reconstruction methods have significantly advanced the automatic creation of static assets [48, 43, 16, 65, 61, 54, 34], articulated objects remain challenging as they require recovering both geometry and underlying kinematic structure. Today, building such models still demands extensive expert effort, making the process labor-intensive and hard to scale, and ultimately limiting the accessibility and realism of articulated content for large-scale interactive environments.\nTo automate this process, we tackle the challenging problem of image-based articulated object reconstruction: recovering a complete 3D representation for an articulated object, including geometry, texture, and its underlying articulation structure. We specifically focus on a practical, yet difficult setting: reconstructing articulated objects in a feed-forward manner from only a sparse set of multi-state RGB images. This setup is important for scalability, as dense multi-view, multi-state capture is often infeasible in real-world scenarios. However, the sparsity of inputs poses a significant challenge, requiring the inference of complex 3D shape, material, and articulation structure from limited visual cues, a task where existing articulated object reconstruction methods typically fall short.\nExisting approaches to image-based articulated object reconstruction can be categorized into per-object optimization and feed-forward learning, both of which are ill-suited to our target setting. Per-object optimization methods [21, 36, 15, 62, 42] achieve high-fidelity reconstructions but are impractically slow due to the lengthy test-time optimization; they also depend on dense observations (often views) and fragile cross-state matching, making them unsuitable for sparse inputs. In contrast, while feed-forward models [20, 4, 37, 13] offer fast inference, they are typically trained on limited datasets (e.g., PartNet-Mobility [63], restricting them to a few categories and limiting generalization to diverse, unseen objects.\nTo address this gap, we introduce the Articulated Reconstruction Transformer (ART), a category-agnostic, feed-forward model that reconstructs complete articulated 3D objects from sparse, multi-state RGB images. Our key insight is that articulated objects can be effectively represented as a collection of rigid parts, with articulation defining their kinematic relationships. Accordingly, ART formulates the reconstruction of articulated objects as a part-based prediction task.\nInspired by the success of large-scale static reconstruction models [16], ART adopts a transformer architecture that maps sparse image inputs to a set of learnable part slots, each trained to capture one object part. From each slot, ART jointly decodes a unified part representation‚Äî3D geometry, texture, and explicit articulation parameters (e.g., motion type, axis, and pivot). Training with per-part supervision on a large-scale, diverse dataset yields a transferable prior, allowing ART to function as a single, unified model across categories. The part-based output is physically interpretable and directly exportable to standard simulation formats (e.g., URDF), producing simulation-ready assets.\nThrough comprehensive experiments, we show that ART significantly outperforms both optimization-based and feed-forward baselines. Our method sets a new state-of-the-art for articulated object reconstruction from sparse image inputs, demonstrating the potential of large-scale, part-based feed-forward models for this challenging task.\nIn summary, our main contributions are as follows:\n-\n‚Ä¢\nWe tackle image-based articulated object reconstruction from sparse-view, multi-state inputs by formulating it as a part-level prediction of geometry, texture, and articulation properties.\n-\n‚Ä¢\nWe propose ART, a category-agnostic feed-forward transformer trained on large-scale articulated object datasets, capable of inferring not only per-part geometry/texture, but also kinematically consistent articulation structures.\n-\n‚Ä¢\nWe demonstrate that ART significantly outperforms both optimization-based and feed-forward baselines, establishing a new state-of-the-art for holistic articulated object reconstruction.\n2 Related Work\n2.1 Articulation structure understanding\nA large body of work studies articulation understanding across diverse input modalities, including RGB [59] and RGB-D [39, 1, 18] images, point clouds [12, 40, 60], videos [41, 50] and 3D meshes [51]. Within this space, one line of work identifies movable parts to reveal the potential degrees of freedom of the object [20, 56, 50] from the given input. Another complementary line directly estimates articulation parameters‚Äîfrom high-level kinematic graphs to low-level joint directions, pivots, and motion angles [21, 31, 64]‚Äîwith recent advances adopting generative or diffusion-based formulations to improve robustness and accuracy [37, 26]. However, most prior methods treat articulation in isolation from geometry and appearance: they recover motion structure but do not reconstruct photorealistic shape and texture suitable for simulation or rendering. In contrast, our method jointly predicts articulation structure and part-level geometry/texture, producing a unified, simulation-ready representation.\n2.[ADDRESS_REMOVED] reconstruction\nBeyond understanding alone, another line of research targets reconstruction of articulated objects‚Äîi.e., recovering geometry, texture, and articulation structure from observations. Methods typically fall into two categories. The first is the per-object optimization methods. Many approaches formulate reconstruction as inverse rendering with neural radiance fields [45] or 3D gaussian splatting [24]. These approaches iteratively optimize geometry and appearance over multi-view, multi-state sequences while inferring articulation [42, 62, 36, 9, 57, 46]. Although often yielding high fidelity, these pipelines require very dense viewpoints or carefully staged sequences, entail lengthy per-instance optimization, and rely on fragile cross-state correspondences, making them sensitive to occlusion and initialization.\nThe alternative is the feed-forward prediction models. To improve scalability, these models infer articulation without test-time optimization. For instance, SINGAPO [37] predicts a kinematic graph and retrieves parts to assemble full assets, while other approaches [4, 6, 44, 15] reconstruct articulated objects directly from single- or multi-stage image inputs. Nonetheless, data scarcity often restricts such models to a small set of categories. More recent work [14] leverages strong pre-trained generative prior to expand generalization, but is designed for multi-stage image generation and is typically combined with an optimization-based refinement for final reconstruction. By contrast, our approach differs remains purely feed-forward at inference from sparse RGB inputs, directly predicting a part-based 3D geometry/texture and explicit articulation parameters, and is trained on a substantially larger and more diverse dataset.\n2.3 Feed-forward 3D reconstruction\nThe availability of large-scale 3D datasets [8, 7] has enabled powerful feed-forward reconstruction systems that combine scalable transformers architectures with differentiable rendering supervision [16, 65, 29, 22, 34]. Recent work extends this approach to part-based reconstruction/generation [3, 35], indicating that structured outputs can be produced in a single forward pass. Building on these insights, ART exploits a transformer backbone for articulated 3D reconstruction: it decomposes an object into consistent components and predicts part-level geometry and texture along with explicit articulation parameters (motion type, axis, pivot/limits) for each dynamic part, yielding a unified representation that is both photorealistic and kinematically interpretable.\n3 Articulated Reconstruction Transformer\n3.1 Problem Formulation\nThe input is a multi-view and multi-state image set , where is the number of camera views and stands for the number of articulation stages (states). During training, each view has known intrinsics and extrinsics to cast sampling rays, and the object is normalized to a bounding sphere with radius .\nWe aim to reconstruct an articulated object as a set of parts. Let denote the number of parts (including a static base part). For each part , we predict a unified representation :\nwhere encodes geometry/texture, and denotes articulation parameters. The detailed definitions of these articulation parameters are given below (omitting the part index for simplicity):\n-\n‚Ä¢\ndenotes an axis-aligned bounding box in the canonical object frame, defined by its center position and the side lengths along each axis.\n-\n‚Ä¢\nis the motion type. The base part is static by definition; all other parts are either prismatic (translational) or revolute (rotational).\n-\n‚Ä¢\nis the joint axis direction in the canonical object frame, represented as a unit-length 3D vector.\n-\n‚Ä¢\nis a point on the joint axis in canonical frame. For a revolute joint, it is essentially the hinge pivot. For a prismatic joint, is defined and predicted but is not required in inference since the direction of the axis suffices to interpret the part motion.\n-\n‚Ä¢\nrepresents the normalized motion value (‚Äúdynamics‚Äù) for each input stage: angles in radians for revolute motion, or translations in object-scale units for prismatic motion. During training, aligns rendered images to the observed state ; at inference, it can be used to control the articulated configuration.\nFor , following transformer-based reconstruction models [16, 61, 34], we represent each part‚Äôs geometry and texture with a hexa-plane [34, 2] parameterization (details in Section 3.2 and Section 3.3). Concretely, we denote:\nwhere each predicted plane stores the features that will be queried during volume rendering process. All parts in the model are predicted in a shared canonical object frame (the rest state). Given an articulation configuration (rotational angle or translation), part can be posed by a rigid transform that (i) rotates about axis when , or (ii) translates along direction when . The base part uses the identity transform. During training, we enforce consistency between the posed prediction and the observed stage using the predicted per-state dynamics.\nWe allocate learnable part slots in the network and predict active parts. At inference time, similar to previous methods [36, 62, 42], we assume the part count is known. In practice, existing available VLMs [5, 17, 67] can also provide accurate part-count estimates.\nAn important design choice is to use a canonical rest state frame for articulation parameters. This rest state is a predefined pose configuration for each object instance (e.g., all drawers closed, microwave shut), set during the data construction. In contrast to parameterizing motion relative to the first observed frame‚Äîwhich is sequence-dependent and thus inconsistent‚Äîthis canonicalization ensures identical ground truth across different sequences of the same object for both part bounding boxes and underlying geometry/texture. This results in more stable training and substantially faster convergence, an important benefit given the limited availability of articulated 3D data.\n3.2 Model Architecture\nGiven the multi-view, multi-stage inputs described above, ART maps images into a shared token space, routes these token features to a fixed set of learnable part slots, and decodes for each part both the geometry/texture plane representations and articulation structure parameters.\nEncoding image tokens. Each image is first divided into non-overlapping patches and projected by a small MLP [49] into a sequence of image tokens. To disambiguate tokens across views and articulation stages, we augment them with three types of side information.\nStage embeddings. We add a learnable embedding to every token originating from stage , enabling the network to separate information from different articulation states.\nViewpoint information. For each input image we compute the Pl√ºcker ray representation [19, 55] using known camera intrinsics and extrinsics, denoted as , where is the unit ray direction and is the camera origin.\nHigh-level semantics. We concatenate features from a pretrained DINOv2 encoder [47]; the image inputs to DINOv2 are resized to account for different patch sizes. These features provide rich semantics [33, 11, 70, 66], which is especially helpful under sparse views and limited training data.\nAfter the concatenation of the above features with patch embeddings, we obtain the final token sequence from the input images, serving as the conditional input to the transformer layers.\nLearnable part slots. We introduce learnable part slots in the model. Each slot has a set of tokens trained to predict part information given the articulated object observation inputs. One slot is reserved for the base part, and the remaining slots model movable parts. During training, slot tokens are updated via attention with image tokens [58]. With part number given, we simply keep the first slots to obtain the final prediction in both training and inference.\nTransformer layers. Inputs to the transformer are part tokens and image tokens (Figure 2). Stacked layers update part tokens while continuously progressing on information in image tokens.\nWe employ two complementary layer types. In the self-attention layers, we concatenate image and part tokens and apply a single attention operation over the entire set. This promotes global context sharing across views, stages, and parts, which is beneficial for long-range correspondence and enforcing inter-part consistency. In the cross-attention layers, image tokens act as queries and part tokens provide keys and values, explicitly routing visual information to a compact set of part slots and mitigating inter-part interference‚Äîan important factor to further improve performance.\nMost LRM-style models [16, 65, 61, 34] adopt only self-attention layer. In our setting this led to slower training and unstable specialization of part slots, so we replace of layers with cross-attention, for two reasons: (1) Token efficiency. Multi-stage, multi-part inputs introduce many more tokens than single-object settings; cross-attention uses a smaller effective attention window and is more efficient. (2) Convergence and accuracy. Interleaving cross-attention accelerates convergence and improves final quality by encouraging distinct roles for image vs. part tokens, focusing the model on their interactions, and enabling it to learn stronger reconstruction priors.\nDecoding part properties. As shown in Fig. 2, we split the final part tokens into two branches and use separate MLP heads to predict (i) the hexa-plane representation for geometry/texture and (ii) the articulation vector . We partition along the channel axis into and remap these raw outputs to the final properties:\nwhere is the sigmoid and is the radius of the normalized bounding sphere. For the motion type, produces two logits (i.e., prismatic vs. revolute), which is determined by softmax during training and argmax at inference. By convention, the first part slot is reserved for the static base with motion type fixed to . The two-way classification is only applied to the remaining movable slots.\n3.[ADDRESS_REMOVED]\nAs shown in Fig. 2, during each training iteration we render the articulated object both per part and as a composite of all parts to generate the final supervision image. Our renderer follows signed-distance-function (SDF) volume rendering [69] to allow the model to learn both underlying geometry and appearance. Specifically for each dynamic part, we transform sampling rays into the object coordinate space of the corresponding state. Implementation details for rendering static and dynamic parts and for composing all parts via volumetric rendering are provided in Sec B in the supplementary.\n3.4 Training Scheme\nTraining objectives. Our loss combines rendering objectives with direct supervision on articulation parameters. ‚Äî All rendering losses are computed on per-part renderings rather than the final composite. Empirically, supervising only the composite image hinders learning in occluded regions and biases geometry/texture near part boundaries. For each part , view , and stage , we apply mean-squared error on RGB and masks, and a perceptual loss [71] on RGB. For articulation parameters, we use cross-entropy for motion-type classification and MSE for the remaining parameters .\nPre-training stage. Articulated-object datasets are inherently less diverse and scalable than static 3D corpora [8, 7]. To learn a strong prior over geometry, texture, and part decomposition, we introduce an optional pre-training stage. For pre-training, we curate k static 3D objects with part decomposition from a collection of 3D-artist generated assets that we licensed for AI training from a commercial source. And we further filter the assets whose native glTF/GLB hierarchy contains at most parts (as defined by their mesh-based composition). During pre-training, the model is optimized only with rendering losses (, ) and MSE on part bounding-box centers/sizes; articulation parameters (motion type, axis, dynamics) are not applicable. Empirically, this stage consistently boosts downstream performance across all metrics.\nCoarse-to-fine articulation training stage. Following static pre-training, we fine-tune ART on our articulated dataset using a coarse-to-fine curriculum designed to gradually increase the network ability. First, we gradually sharpen the rendered surfaces by linearly increasing the reciprocal of the standard derivation of the SDF [34]. Second, we employ resolution annealing: training begins at a resolution for rendering-based objectives, and the supervision resolution is later increased to to encourage finer geometric predictions.\n3.[ADDRESS_REMOVED] Datasets\nOur model is trained with multi-view, multi-state RGB images from dynamic articulated sequences generated from a large and diverse collection of articulated 3D object assets. Complete details of the data pipeline (asset collection/construction and per-asset sequence generation) and statistics are given in the supplementary material. Below we briefly introduce the names and characteristics of each collection used. In total, we aggregate articulated objects from three primary sources:\nPartNet-mobility. We utilize several common indoor categories from the PartNet-Mobility benchmark [63], including buckets, microwaves, and a range of furniture classes.\nProcedural dataset. To enhance diversity and realism, we generated a new articulated-object dataset using a procedural generation method motivated by [23]. It includes high-quality articulated models across six categories, with rich variation in shape and texture.\nStorageFurniture dataset. We specifically focus on this category because it is ubiquitous in real-world environments and widely used as a training domain in prior work [4, 37, 20]. Our collection is procedurally generated from the PartNet-Mobility storage-furniture category: using a compositional assembly system, we replace parts of source objects with alternative geometries from other assets, yielding a large and diverse set of realistic storage-furniture models.\nFigure 3 shows random samples from the latter two collections. For every articulated asset described above, we automatically synthesize multiple articulated sequences, which are used for training and evaluation.\n4 Experiments\nEvaluation datasets. We evaluate on two distinct test sets.\nStorageFurniture Test Set: For comparisons to feed-forward methods, we use the held-out split of our large-scale StorageFurniture dataset, comprising objects, enabling robust large-scale evaluation.\nPartNet-mobility Test Set: For comparisons to optimization-based methods, we use a held-out set of articulated sequences from PartNet-mobility [63], aligning with the evaluation protocol of previous optimization methods on small sets due to their long inference times.\nBaseline methods. We compare against two classes of methods. For feed-forward models, we select URDFormer [4] and SINGAPO [37], two recent state-of-the-art methods for articulated reconstruction with a particular focus on part-level articulation prediction for storage-furniture objects. For optimization-based methods, we include PARIS [36], DTA [62] and ArtGS [42], which recover geometry, texture, and articulation structure from two-state, multi-view inputs.\nEvaluation metrics. To provide a holistic evaluation, we assess geometry, texture, and part-level accuracy.\n-\n‚Ä¢\nImage-level metrics: We report PSNR and LPIPS [71] on renderings from novel test viewpoints.\n-\n‚Ä¢\nGeometry-level metrics: We compute Chamfer Distance (CD) and F-Score between the predicted and ground-truth meshes.\n-\n‚Ä¢\nPart-level metrics: Following the evaluation protocol in Liu et al. [37], we match predicted parts and ground-truth parts via Hungarian algorithm, then report (i) part-level prediction accuracy using distance of generalized Intersection over Union (1-gIoU) [52] and (ii) Euclidean distance between part centroids. For both, lower is better.\nImplementation details. We train two versions of ART: a multi-view model () for comparison with optimization methods and a monocular model () for fair comparison with feed-forward baselines. We fix (‚Äústart & end‚Äù) states. All training and inference images are resized to to balance compute. The transformer has attention blocks with a cross-/self-attention ratio, heads, and a -dimensional embedding. It outputs hexa-plane features that are later upsampled to the spatial resolution of . We set the maximum part count to . Training uses AdamW with . The multi-view model trains for days on H100 GPUs; the monocular model trains for days.\n4.1 Results\nComparison with feed-forward baselines. We first compare the monocular version of ART to URDFormer and SINGAPO. Table 1 reports quantitative results: ART outperforms both baselines by a large margin across all metrics. In particular, it achieves substantially better part-level prediction, as evidenced by lower and ‚Äîand more accurate overall geometry, as indicated by a reduced Chamfer Distance. These gains are also evident qualitatively in Figure 5, which contrasts our predicted part bounding boxes with SINGAPO and highlights ART ‚Äôs enhanced ability to jointly reconstruct part structure and geometry.\nComparison with optimization-based baselines. Next, we evaluate the multi-view ART against per-object optimization baselines on the PartNet-mobility test set, using sparse inputs (4 views across 2 states) for all methods; DTA additionally receives depth maps, as required. As shown in Table 2, ART achieves state-of-the-art results on image-level metrics, outperforming all optimization-based competitors by a clear margin. While DTA reports comparable geometry metrics, this is expected given its depth supervision; nonetheless, its lower PSNR and higher LPIPS scores indicate poor appearance recovery. This disparity is rooted in sparsity: optimization approaches rely on dense cross-state correspondences, which are fragile and difficult to establish from few views. As shown by the failure example in Figure 6, due to the lack of robust correspondences, ArtGS yields fragmented, noisy geometry, whereas ART ‚Äîbenefiting from a strong learned prior‚Äîreconstructs coherent, high-fidelity textured meshes.\nQualitative results. We present qualitative results in Figure 1 and Figure 4. Our core idea is to cast articulated object reconstruction as a part-based prediction problem, decomposing objects into a set of rigid parts. As shown in Figure 1, ART successfully reconstructs diverse object categories and yields clear part decompositions, visualized by rendering each predicted part with a consistent color.\nFigure 4 provides a closer look at our method‚Äôs outputs, showing reconstructions for the start/end states provided in the image inputs. The Predicted Parts column visualizes the inferred structure by overlaying per-part bounding boxes for both states, along with recovered articulation structures (e.g., axes for movable parts). Together, these visualizations demonstrate that ART reconstructs high-fidelity geometry and texture while accurately recovering the articulation structure that drives object motion.\nReal-world images results. We further present a real-world example in Fig. 7. The capture uses only approximate camera poses and no background masking. Despite no real-image training, ART recovers the correct articulation structure with plausible geometry and texture.\n4.[ADDRESS_REMOVED] a detailed ablation study (Table 3) to validate our key design choices.All model variants are evaluated on the overall held-out test set of PartNet-mobility [63], which includes more than multi-view, multi-state sequences. Our reference is the base model w/o pre-train, which is the multi-view () ART model trained from scratch on only the articulated object dataset. We then measure the impact of adding the pre-training stage (last row) and of removing key components (first four rows) relative to this base model.\nPre-training stage helps. By adding the pre-training stage, the full ART shows a substantial improvement over the version without pre-training, with gains across geometry, texture, and articulation metrics. This confirms that a robust prior on geometry and part decomposition learned from large-scale 3D data is highly beneficial for part-based articulated reconstruction.\nMonocular vs. multi-view inputs. We compare the base multi-view model with a variant trained using only a single view as input. This change produces a clear drop in quality across all metrics, as expected, confirming that multiple views are crucial for resolving ambiguities and achieving high-fidelity, geometrically accurate articulated reconstruction.\nRest-state formulation. Removing the rest-state formulation and predicting parts relative to the first observed frame leads to a severe quality drop (PSNR ) and higher part-level errors ( ). The decline is unsurprising: because the test set contains multiple sequences of the same object, the first-frame scheme induces a pose-dependent ‚Äúcanonical‚Äù per sequence (e.g., one starting closed, another open), effectively treating the same object as different identities. This ambiguity hampers learning and underscores the benefit of a consistent, predefined rest state.\nDefined Part Order. Without forcing a pre-defined part ordering in dataset construction, performance degrades most severely across the board (PSNR , ). This is expected: Lacking a consistent ordering forces the network to learn both reconstruction and part matching, which is especially difficult for objects with many similar dynamic parts (e.g., cabinets with many drawers). We observe slot collapse‚Äîmultiple slots predicting parts in the same location‚Äîdirectly reflected in the part-level metrics. This confirms the benefit of enforcing a consistent part ordering.\nPer-part vs. composite rendering losses. Finally, eliminating per-part rendering losses and supervising only the composited image consistently harms performance. Per-part supervision is essential for learning occluded content; otherwise, the result is smeared geometry and inferior texture quality.\n5 Discussions and Conclusion\nLimitations. Our method assumes a known part count for the target object and relies on pre-calibrated camera poses. Future work should include learning a pose-free variant (self-calibrated cameras) with larger datasets and integrating part-count estimation directly into the model.\nConclusion. In this work, we propose ART, a feed-forward model to reconstruct complete 3D articulated objects from sparse, multi-state images. By casting reconstruction as a part-based prediction problem, ART jointly decodes geometry, texture, and articulation structure for each part. Experiments across a broad range of articulated objects demonstrate that ART consistently outperforms strong feed-forward and optimization-based baselines.\nAcknowledgments. We thank Yawar Siddiqui, Ruocheng Wang and Qiao Gu for the comments and fruitful discussions, and Ka Chen, David Clabaugh and Samir Aroudj for the help on dataset collecting and constructions.\nReferences\n- Abbatematteo et al. [2019] Ben Abbatematteo, Stefanie Tellex, and George Konidaris. Learning to generalize kinematic models to novel objects. In Proceedings of the 3rd Conference on Robot Learning, 2019.\n- Cao and Johnson [2023] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130‚Äì141, 2023.\n- Chen et al. [2025] Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: Autogressive 3d part generation and discovery. arXiv preprint arXiv:2507.[POSTAL_CODE_REMOVED], 2025.\n- Chen et al. [2024] Zoey Chen, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Karthikeya Vemuri, Alan Wu, Dieter Fox, and Abhishek Gupta. Urdformer: A pipeline for constructing articulated simulation environments from real-world images. arXiv preprint arXiv:2405.[POSTAL_CODE_REMOVED], 2024.\n- Comanici et al. [2025] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.[POSTAL_CODE_REMOVED], 2025.\n- Dai et al. [2024] Tianyuan Dai, Josiah Wong, Yunfan Jiang, Chen Wang, Cem Gokmen, Ruohan Zhang, Jiajun Wu, and Li Fei-Fei. Automated creation of digital cousins for robust policy learning. arXiv preprint arXiv:2410.[POSTAL_CODE_REMOVED], 2024.\n- Deitke et al. [2023a] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:[POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2023a.\n- Deitke et al. [2023b] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2023b.\n- Deng et al. [2024] Jianning Deng, Kartic Subr, and Hakan Bilen. Articulate your nerf: Unsupervised articulated object modeling via conditional view synthesis. Advances in Neural Information Processing Systems, 37:119717‚Äì119741, 2024.\n- Deng et al. [2025] Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and Jiajun Wu. Anymate: A dataset and baselines for learning 3d object rigging. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 1‚Äì10, 2025.\n- El Banani et al. [2024] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2024.\n- Fu et al. [2024] Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato, and Takeshi Oishi. Capt: Category-level articulation estimation from a single point cloud using transformer. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 751‚Äì757. IEEE, 2024.\n- Gao et al. [2025a] Daoyi Gao, Yawar Siddiqui, Lei Li, and Angela Dai. Meshart: Generating articulated meshes with structure-guided transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 618‚Äì627, 2025a.\n- Gao et al. [2025b] Mingju Gao, Yike Pan, Huan-ang Gao, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, Li Yi, and Hao Zhao. Partrm: Modeling part-level dynamics with large cross-state reconstruction model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 7004‚Äì7014, 2025b.\n- Heppert et al. [2023] Nick Heppert, Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Rares Andrei Ambrus, Jeannette Bohg, Abhinav Valada, and Thomas Kollar. Carto: Category and joint agnostic reconstruction of articulated objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2023.\n- Hong et al. [2023] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.[POSTAL_CODE_REMOVED], 2023.\n- Hurst et al. [2024] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.[POSTAL_CODE_REMOVED], 2024.\n- Jain et al. [2022] Ajinkya Jain, Stephen Giguere, Rudolf Lioutikov, and Scott Niekum. Distributional depth-based estimation of object articulation models. In Conference on Robot Learning, pages 1611‚Äì1621. PMLR, 2022.\n- Jia [2020] Yan-Bin Jia. Pl√ºcker coordinates for lines in the space. Problem Solver Techniques for Applied Computer Science, Com-S-477/577 Course Handout, 3, 2020.\n- Jiang et al. [2022a] Hanxiao Jiang, Yongsen Mao, Manolis Savva, and Angel X Chang. Opd: Single-view 3d openable part detection. In European Conference on Computer Vision, pages 410‚Äì426. Springer, 2022a.\n- Jiang et al. [2022b] Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu. Ditto: Building digital twins of articulated objects from interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5616‚Äì5626, 2022b.\n- Jin et al. [2024] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: A large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.[POSTAL_CODE_REMOVED], 2024.\n- Joshi et al. [2025] Abhishek Joshi, Beining Han, Jack Nugent, Max Gonzalez Saez-Diez, Yiming Zuo, Jonathan Liu, Hongyu Wen, Stamatis Alexandropoulos, Karhan Kayan, Anna Calveri, Tao Sun, Gaowen Liu, Yi Shao, Alexander Raistrick, and Jia Deng. Procedural generation of articulated simulation-ready assets, 2025.\n- Kerbl et al. [2023] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139‚Äì1, 2023.\n- Kim et al. [2025] Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, and Hanbyul Joo. Parahome: Parameterizing everyday home activities towards 3d generative modeling of human-object interactions. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1816‚Äì1828, 2025.\n- Lei et al. [2023] Jiahui Lei, Congyue Deng, William B Shen, Leonidas J Guibas, and Kostas Daniilidis. Nap: Neural 3d articulated object prior. Advances in Neural Information Processing Systems, 36:[POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2023.\n- Li et al. [2021] Chengshu Li, Fei Xia, Roberto Mart√≠n-Mart√≠n, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, et al. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. arXiv preprint arXiv:2108.[POSTAL_CODE_REMOVED], 2021.\n- Li et al. [2024a] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart√≠n-Mart√≠n, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, et al. Behavior-1k: A human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. arXiv preprint arXiv:2403.[POSTAL_CODE_REMOVED], 2024a.\n- Li et al. [2023a] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.[POSTAL_CODE_REMOVED], 2023a.\n- Li et al. [2022] Ruilong Li, Matthew Tancik, and Angjoo Kanazawa. Nerfacc: A general nerf acceleration toolbox. arXiv preprint arXiv:2210.[POSTAL_CODE_REMOVED], 2022.\n- Li et al. [2020] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, and Shuran Song. Category-level articulated object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3706‚Äì3715, 2020.\n- Li et al. [2023b] Zhaoshuo Li, Thomas M√ºller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8456‚Äì8465, 2023b.\n- Li et al. [2024b] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, and Jiajun Wu. Learning the 3d fauna of the web. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9752‚Äì9762, 2024b.\n- Li et al. [2025] Zhengqin Li, Dilin Wang, Ka Chen, Zhaoyang Lv, Thu Nguyen-Phuoc, Milim Lee, Jia-Bin Huang, Lei Xiao, Yufeng Zhu, Carl S Marshall, et al. Lirm: Large inverse rendering model for progressive reconstruction of shape, materials and view-dependent radiance fields. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 505‚Äì517, 2025.\n- Lin et al. [2025] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers. arXiv preprint arXiv:2506.[POSTAL_CODE_REMOVED], 2025.\n- Liu et al. [2023a] Jiayi Liu, Ali Mahdavi-Amiri, and Manolis Savva. Paris: Part-level reconstruction and motion analysis for articulated objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 352‚Äì363, 2023a.\n- Liu et al. [2024] Jiayi Liu, Denys Iliash, Angel X Chang, Manolis Savva, and Ali Mahdavi-Amiri. Singapo: Single image controlled generation of articulated parts in objects. arXiv preprint arXiv:2410.[POSTAL_CODE_REMOVED], 2024.\n- Liu et al. [2025a] Jiayi Liu, Manolis Savva, and Ali Mahdavi-Amiri. Survey on modeling of human-made articulated objects. In Computer Graphics Forum, page e70092. Wiley Online Library, 2025a.\n- Liu et al. [2022] Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu Lu. Toward real-world category-level articulation pose estimation. IEEE Transactions on Image Processing, 31:1072‚Äì1083, 2022.\n- Liu et al. [2023b] Liu Liu, Jianming Du, Hao Wu, Xun Yang, Zhenguang Liu, Richang Hong, and Meng Wang. Category-level articulated object 9d pose estimation via reinforcement learning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 728‚Äì736, 2023b.\n- Liu et al. [2020] Qihao Liu, Weichao Qiu, Weiyao Wang, Gregory D Hager, and Alan L Yuille. Nothing but geometric constraints: A model-free method for articulated object pose estimation. arXiv preprint arXiv:2012.[POSTAL_CODE_REMOVED], 2020.\n- Liu et al. [2025b] Yu Liu, Baoxiong Jia, Ruijie Lu, Junfeng Ni, Song-Chun Zhu, and Siyuan Huang. Building interactable replicas of complex articulated objects via gaussian splatting. In The Thirteenth International Conference on Learning Representations, 2025b.\n- Liu et al. [2023c] Ying-Tian Liu, Yuan-Chen Guo, Vikram Voleti, Ruizhi Shao, Chia-Hao Chen, Guan Luo, Zixin Zou, Chen Wang, Christian Laforte, Yan-Pei Cao, et al. Threestudio: A modular framework for diffusion-guided 3d generation. cg. cs. tsinghua. edu. cn, 2023c.\n- Mandi et al. [2024] Zhao Mandi, Yijia Weng, Dominik Bauer, and Shuran Song. Real2code: Reconstruct articulated objects via code generation. arXiv preprint arXiv:2406.[POSTAL_CODE_REMOVED], 2024.\n- Mildenhall et al. [2021] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99‚Äì106, 2021.\n- Mu et al. [2021] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning disentangled signed distance functions for articulated shape representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2021.\n- Oquab et al. [2023] Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.[POSTAL_CODE_REMOVED], 2023.\n- Poole et al. [2022] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.[POSTAL_CODE_REMOVED], 2022.\n- Popescu et al. [2009] Marius-Constantin Popescu, Valentina E Balas, Liliana Perescu-Popescu, and Nikos Mastorakis. Multilayer perceptron and neural networks. WSEAS Transactions on Circuits and Systems, 8(7):579‚Äì588, 2009.\n- Qian et al. [2022] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen, and David F Fouhey. Understanding 3d object articulation in internet videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1599‚Äì1609, 2022.\n- Qiu et al. [2025] Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, and Chuang Gan. Articulate anymesh: Open-vocabulary 3d articulated objects modeling. arXiv preprint arXiv:2502.[POSTAL_CODE_REMOVED], 2025.\n- Rezatofighi et al. [2019] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 658‚Äì666, 2019.\n- Shen et al. [2021] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart√≠n-Mart√≠n, Linxi Fan, Guanzhi Wang, Claudia P√©rez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7520‚Äì7527. IEEE, 2021.\n- Siddiqui et al. [2024] Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, et al. Meta 3d assetgen: Text-to-mesh generation with high-quality geometry, texture, and pbr materials. Advances in Neural Information Processing Systems, 37:9532‚Äì9564, 2024.\n- Sitzmann et al. [2021] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:[POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2021.\n- Sun et al. [2024] Xiaohao Sun, Hanxiao Jiang, Manolis Savva, and Angel Chang. Opdmulti: Openable part detection for multiple objects. In 2024 International Conference on 3D Vision (3DV), pages 169‚Äì178. IEEE, 2024.\n- Swaminathan et al. [2024] Archana Swaminathan, Anubhav Gupta, Kamal Gupta, Shishira R Maiya, Vatsal Agarwal, and Abhinav Shrivastava. Leia: Latent view-invariant embeddings for implicit 3d articulation. In European Conference on Computer Vision, pages 210‚Äì227. Springer, 2024.\n- Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n- Wang et al. [2024] Ruiqi Wang, Akshay Gadi Patil, Fenggen Yu, and Hao Zhang. Active coarse-to-fine segmentation of moveable parts from real images. In European Conference on Computer Vision, pages 111‚Äì127. Springer, 2024.\n- Wang et al. [2019] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, and Kai Xu. Shape2motion: Joint analysis of motion parts and attributes from 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8876‚Äì8884, 2019.\n- Wei et al. [2024] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality meshes. arXiv preprint arXiv:2404.[POSTAL_CODE_REMOVED], 2024.\n- Weng et al. [2024] Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, and Stan Birchfield. Neural implicit representation for building digital twins of unknown articulated objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3141‚Äì3150, 2024.\n- Xiang et al. [2020] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2020.\n- Xu et al. [2022] Xianghao Xu, Yifan Ruan, Srinath Sridhar, and Daniel Ritchie. Unsupervised kinematic motion detection for part-segmented 3d shape collections. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1‚Äì9, 2022.\n- Xu et al. [2024] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In European Conference on Computer Vision, pages 1‚Äì20. Springer, 2024.\n- Xu et al. [2025] Zhen Xu, Zhengqin Li, Zhao Dong, Xiaowei Zhou, Richard Newcombe, and Zhaoyang Lv. 4dgt: Learning a 4d gaussian transformer using real-world monocular videos. arXiv preprint arXiv:2506.[POSTAL_CODE_REMOVED], 2025.\n- Yang et al. [2025] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.[POSTAL_CODE_REMOVED], 2025.\n- Yang et al. [2023] Gengshan Yang, Chaoyang Wang, N Dinesh Reddy, and Deva Ramanan. Reconstructing animatable categories from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2023.\n- Yariv et al. [2021] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Advances in neural information processing systems, 34:4805‚Äì4815, 2021.\n- Zhang et al. [2023] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:[POSTAL_CODE_REMOVED]‚Äì[POSTAL_CODE_REMOVED], 2023.\n- Zhang et al. [2018] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586‚Äì595, 2018.\nSupplementary Material\nAppendix A More results\nWe provide additional results in the supplementary index.html.\nVideo results. We provide both fixed-view and rotational-view renderings of the reconstructed articulated objects, with the dynamic parts moving according to the predicted articulation structure.\nExport into simulator. As discussed in the main text, our part-based output can be directly converted to URDF. Combining the exported URDF with each part‚Äôs textured mesh yields simulation-ready assets. We showcase several interaction scenes in the MuJoCo simulator‚Äîeach featuring a humanoid robot and an articulated object‚Äîand include the corresponding videos in index.html.\nAppendix B Rendering Articulated Object\nAt each training iteration, we render the articulated object both per part and in a compositional manner to obtain the final image for supervision. The rendering process queries hexa-plane feature outputs from the model to compute the per-pixel RGB and uses the predicted articulation to correctly place each dynamic part.\nOur rendering pipeline follows the signed distance function (SDF) volume rendering [69]. For a camera ray defined by origin and unit direction , we first intersect the ray with the part‚Äôs axis-aligned bounding box (in canonical space) and sample 3D points along the valid ray segment. Each sampled world-space point is mapped to the part‚Äôs normalized local coordinates , which are then used to query the hexa-plane features:\nHere , are the model‚Äôs hexa-plane representation output from Eq. 2; , can be obtained analogously. These features are concatenated to form the feature vector for the spatial point . Following the architecture in [61, 34], we use two small MLPs to predict SDF value and RGB color , respectively. In particular, the SDF value is computed as:\nwhere the bias term is a prior defined in the part‚Äôs local space, to initialize the shape as a sphere to stabilize the training [69, 34]. The SDF value can be converted further to volume density using the Laplace CDF [69]:\nwhere is the standard deviation that controls the sharpness of the underlying surface. As noted in the main text, is linearly annealed over training to progressively sharpen surfaces. With and evaluated at ray samples, we compute per-part color, opacity (mask), and depth via the usual transmittance accumulation [45, 69]. Normals are obtained by numerically differentiating the SDF at each sample [32].\nTo obtain the composited image over all parts, we merge the sampled points from all parts along each ray, sort their orderings by the sampled ray distance, and apply standard alpha compositing to compute the rendered images. We employ Nerfacc [30] to accelerate point sampling, ray distance sorting, and compositing.\nDynamic parts rendering. The equations above describe static per-part rendering in each part‚Äôs canonical space (from the model-predicted bounding-box region). For dynamic parts at stage , instead of physically moving the part‚Äôs volume and rebuilding an oriented bounding box, we transform the camera rays into the part‚Äôs instantaneous local frame (i.e., move the rays inversely), keeping the axis-aligned bounding box unchanged.\nRecall that denotes the motion type, the unit joint axis, a point on that axis, and the normalized motion value at stage . For a point in the part‚Äôs canonical space, the stage- rigid transform for different motion types can be defined as:\nPrismatic:\nRevolute:\nwhere denotes the rotation by angle (radians) about axis .\nNow, instead of actually transforming the bounding box (point), we‚Äôll inversely transform each ray to reach the equivalent rendering results. It‚Äôs straightforward that for prismatic parts, after transformation:\nand for revolute parts, similarly:\nWe then proceed exactly as in the static case: intersect the transformed ray with bounding box , sample points, query hexa-planes, and composite to obtain the desired renderings. The computed values from the queried features are further assigned to the original ray sampled points, which are in the current world coordinate space, for volume rendering composition.\nAppendix C Dataset Construction\nA key advantage of ART is its training data: we substantially increase both the quantity and diversity of articulated assets. Prior feed-forward models [37, 20] typically rely on subsets of PartNet-Mobility [63], which offer limited geometric diversity and often unrealistic textures.\nIn this work, we combine three articulated-object data sources to construct our training dataset, increasing diversity and fidelity in geometry, texture, and articulation complexity. Beyond the basic information in the main text, further details on these sources are provided below.\nPartNet-mobility. As previously mentioned, PartNet-Mobility provides common indoor articulated categories‚Äîbucket, dishwasher, door, laptop, microwave, oven, refrigerator, and storage furniture. In total, we collect over objects from this dataset. However, many assets have unrealistic textures and low-quality surface geometry, motivating our exploration of more diverse and realistic articulated object sources.\nProcedural dataset. To scale both quantity and diversity, and inspired by Infinigen-Sim [23], we adopt procedural generation to author articulated assets in Blender (with TA support). The resulting dataset includes high-quality articulated models across six categories: laptop, dishwasher, beverage refrigerator, cabinet with drawers, bucket, and microwave oven. 3D geometry is provided in GLB/OBJ and articulation in URDF. Each category is governed by procedural rules over shape, appearance, and articulation; for each, we generate several hundred variants with randomized shapes, sizes, and materials. In principle, this pipeline can produce an unlimited number of articulated objects.\nStorageFurniture dataset. We recognize that the Storage-Furniture category in PartNet-Mobility spans many commonly used articulated assets, a property also noted by prior methods [4, 37] that primarily train on this category. Based on this, we construct a StorageFurniture dataset by recombining parts from the PartNet-Mobility storage-furniture class.\nFor a given object, we use its articulation tree (kinematic structure) to procedurally create new instances via compositional part assembly: original parts are replaced with geometries from other objects (rescaled as needed), followed by UV-map correction and material randomization. This yields a large, realistic, and varied set of assets‚Äîover articulated models for training.\nThese three data sources collectively provide a large pool of articulated objects. Using each asset‚Äôs articulation definition, we further generate random per-part trajectories, resulting in a large dataset of articulated object sequences.\nAppendix D More Implementation Details\nDuring sequence data construction, we explicitly set the order of the parts following a certain rule: the static base part is the first, and the remaining dynamic parts are ordered from low to high, front to back and left to right. This consistent ground-truth ordering greatly improves training stability and convergence speed. We also define the rest state at this stage as the configuration where all dynamic parts are ‚Äúclosed.‚Äù"
  },
  {
    "article": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models\nAbstract\nAchieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ‚Äútame‚Äù this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and enables cross-task generalization‚Äîachieving 20.8% success on unseen tasks without task-specific demonstrations training (vs. 0% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.\n1 Introduction\nHow do humans develop manipulation skills? We do not simply watch an expert perform a task once and then flawlessly replicate it. Instead, we learn through practice: attempting the task repeatedly, making mistakes, receiving feedback from the environment, and gradually refining our movements through continued experience. This process of learning by doing, rather than merely learning by watching, is fundamental to how intelligent agents acquire robust and adaptable capabilities in the real world.\nEmbodied intelligence, the integration of AI technology with robots, has seen remarkable progress in recent years. Propelled by the capabilities of Large Language Models (LLMs), control policies are rapidly evolving beyond traditional methods toward general Vision-Language-Action (VLA) models [kim2024openvla, black2024pi0, kim2025fine], which process multimodal inputs to produce a sequence of actions for completing a given task. By leveraging the rich semantic priors from LLMs, VLAs demonstrate impressive contextual understanding compared to their predecessors. However, despite these advances, current VLA training remains fundamentally misaligned with the human learning principle described above: they are trained exclusively through Supervised Fine-Tuning (SFT) on fixed demonstration datasets, learning to imitate expert behavior but lacking mechanism to improve through environmental interaction.\nThis paradigm of static imitation learning entails two fundamental limitations. (1) High labor cost. As shown in Fig. 1(a), adapting VLA models to new tasks requires collecting hundreds of demonstrations for supervised fine-tuning (SFT). This cost multiplies linearly with tasks, making it infeasible to scale VLAs to truly general-purpose robots. (2) Brittle memorization. VLAs optimized through behavior cloning merely imitate demonstrations and struggle to generalize beyond training distribution. They lack the ability to recover from execution deviations, where a single misstep often leads to complete task failure. These limitations represent a fundamental misalignment with how adaptive intelligence should operate. We believe that enabling continuous learning from deployment experience is essential for achieving truly general-purpose vision-language-action models.\nIn this work, we propose EVOLVE-VLA (Efficient VLA Online Learning Via Experience), a test-time training framework that fundamentally shifts how VLAs learn and adapt. As illustrated in Fig. 1(b), instead of requiring hundreds of expert demonstrations, our method needs only minimal supervision, a few demonstrations or even none, for lightweight initialization via SFT. The key innovation lies in what happens after this initial pre-training: rather than freezing the policy, we deploy it directly in the target environment where it continues to learn autonomously through active interaction. The VLA explores the environment, receives feedback, and refines its behavior via online reinforcement learning, mirroring the trial-and-error process through which humans develop manipulation skills.\nThis paradigm shift addresses both limitations: (1) it dramatically reduces labor costs by replacing extensive demonstrations with autonomous learning, and (2) it enables genuine adaptation rather than memorization, producing policies that recover from errors and discover novel strategies. For example, Fig. 1(e) shows our model developing error correction capabilities absent from training demonstrations. Beyond improving seen tasks, this approach enables cross-task generalization through self-directed exploration.\nWhile prior works like SimpleVLA-RL [li2025simplevla] have explored RL for VLA models, they rely on oracle reward functions (e.g., binary success signals) unavailable at test time. The central challenge of practical TTT is replacing the oracle with autonomous feedback. We introduce a learned progress estimator as reward, with the policy optimized via GRPO [shao2024deepseekmath]. Unlike sparse success signals, progress-based rewards provide dense, continuous feedback crucial for sample-efficient learning. However, practical progress estimators are inherently noisy [zhang2025rewind, gvl, sontakke2023roboclip, ma2023liv], and errors accumulated over long horizons can mislead the policy.\nOur core technical challenge is therefore not to build a perfect estimator, but to successfully ‚Äútame‚Äù this noisy reward signal to make learning possible. To achieve this, we introduce two key technical contributions. First, we design an accumulative progress estimation mechanism with interval-based sampling, which aggregates and smooths noisy point-wise estimates into a stable, reliable signal. Second, we propose a progressive horizon extension strategy that optimizes the policy with progressively increasing exploration horizon, making the model more resilient to estimation errors by allowing it to first master simpler sub-tasks. This combined approach not only mitigates the impact of estimation noise but also allows the VLA to effectively utilize the dense, albeit imperfect, reward.\nOur framework enables VLA models to perform test-time training using self-generated environmental feedback without oracle rewards. We validate EVOLVE-VLA on the LIBERO benchmark, achieving substantial gains: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and cross-task transfer (0% ‚Üí 20.8% on unseen tasks through autonomous adaptation). Qualitative analysis reveals emergent capabilities absent from demonstrations, including error recovery and novel strategies. These results validate that test-time training represents a paradigm shift toward adaptive embodied agents‚Äîa critical step toward truly general-purpose VLA systems. Our contributions include:\n-\n‚Ä¢\nWe propose EVOLVE-VLA, a test-time training framework that enables VLAs to continuously adapt through autonomous interaction, addressing the brittleness and scalability limitations of static SFT.\n-\n‚Ä¢\nWe tackle the central challenge of absence of oracle rewards by introducing a learned progress estimator. Critically, we develop techniques to ‚Äútame‚Äù inherently noisy reward signals, making practical test-time training feasible.\n-\n‚Ä¢\nWe introduce two key innovations: (1) an accumulative progress estimation mechanism that smooths noisy estimates into stable signals, and (2) a progressive horizon extension strategy enabling gradual policy evolution, proving effective for long-horizon tasks.\n-\n‚Ä¢\nWe demonstrate strong results: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and pioneering zero-shot cross-task generalization (0% ‚Üí 20.8%) through test-time adaptation alone. Our analysis reveals emergent skills like error recovery arising from autonomous exploration.\n2 Related Work\nVision-Language-Action Models. Recent advances in Vision-Language-Action (VLA) models [zhao2025cot, black2024pi0, ding2024quar, kim2024openvla, qu2025spatialvla, wen2025tinyvla, kim2025fine, wen2025diffusionvla] aim to equip embodied agents with the ability to perceive, reason, and act upon multimodal inputs. Early works like RT [brohan2022rt] and Octo [team2024octo] investigate how to connect the power of large models with the interactive nature of embodied environments, paving the way toward generalist robot manipulation. OpenVLA [kim2024openvla] presents an open-source VLA model fine-tuned across multiple manipulation tasks, aiming to standardize evaluation and promote reproducible research. OpenVLA-OFT [kim2025fine] further proposes parallel decoding, action chunking, and a continuous action representation to improve performance. [black2024pi0] introduces a VLA flow model by a continuous flow-based architecture. The approach demonstrates strong generalization across diverse robot manipulation tasks and sets a new direction for flow-based embodied reasoning.\nSome works focus on improving the efficiency of VLA model. TinyVLA [wen2025tinyvla] designs a lightweight VLA for robotic manipulation, which employs parameter sharing and distillation to retain performance under limited data. Recent works [bi2025vla, yang2025bitla, guo2025omnivla] also investigate how to involve tactile modality in VLA models. However, previous methods rely heavily on imitation learning with numerous manual-collected data, leading to labor-cost and poor generalization models, especially when meeting the new tasks and environments.\nRL Fine-Tuning for VLA Models. With the recent advances of RL post-training in LLMs [touvron2023llama, brown2020language] and MLLMs [llava, wang2024qwen2vl], some studies have begun to explore RL post-training for VLA models. For example, iRe-VLA [guo2025improving] explores how online RL can enhance pretrained VLA models by allowing continual improvement through interaction. VLA-RL [lu2025vla] introduces a trajectory-level RL formulation for VLA training. OctoNav [gao2025octonav] investigates how GRPO-like RL training can improve VLA reasoning ability in embodied navigation. SimpleVLA-RL [li2025simplevla] and [chen2025pirl] explore RL fine-tuning for autoregressive and flow-based VLAs, respectively. RL4VLA [liu2025can] systematically studies different RL policies and the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Although these works have explored RL post-training strategies for VLA models, they still assume access to Ground-Truth (GT) information during the RL training phase, such as whether a trajectory succeeds or fails. However, at test time, such GT supervision signals are unavailable. To address this, we propose a test-time training framework that enables the model to adapt without relying on GT feedback.\nConcurrent Work: . Concurrent to our work, Physical Intelligence recently released [pistar06], a vision-language-action model that learns from autonomous experience using their Recap method (RL with Experience & Corrections via Advantage-conditioned Policies). Our work shares a similar motivation and spirit with in addressing a fundamental limitation of VLA models trained purely on demonstration data: the inability to handle compounding errors and improve from deployment experience.\nThe concurrent emergence of both works from academia and industry highlights a growing recognition that experience-based reinforcement learning is essential for VLA models to move beyond behavior cloning. Both approaches demonstrate that achieving reliable and robust performance requires learning from the robot‚Äôs own experience rather than solely imitating expert demonstrations. We submitted EVOLVE-VLAbefore the release of , representing pioneering academic work in this direction. To foster further research and democratize access to this paradigm, we commit to releasing our full training and inference codebase upon publication.\n3 Method\n3.1 Task Definition\nWe formulate the robotic manipulation task as a Markov Decision Process (MDP) , where is the state space, is the action space, represents transition dynamics, is the reward function, and is the discount factor. At timestep , the state consists of visual observation , proprioceptive state , and task instruction .\nA VLA policy maps states to action distributions. Following modern VLA architectures [black2024pi0, kim2024openvla], we adopt action tokenization where continuous robot actions are discretized into tokens. The policy autoregressively generates action token sequences with probability . A trajectory is generated through closed-loop interaction: the policy outputs actions, the environment transitions based on physical dynamics, and updated observations feed back into the policy until task completion or maximum horizon .\n3.[ADDRESS_REMOVED]-time Training Framework\nDuring deployment, a VLA model pretrained via SFT on expert demonstrations encounters novel scenarios that differ from its training distribution. Traditional SFT models, which learn purely through imitation, lack the mechanism to adapt to these out-of-distribution states. Our goal is to enable the VLA to continue learning at test-time by leveraging online interaction with the environment.\nTest-time training (TTT) requires two key components: (1) the ability to actively interact with the environment to generate diverse rollouts, and (2) a feedback signal to evaluate and improve these rollouts. We achieve this through online reinforcement learning, where the policy is iteratively refined based on rewards obtained from environment interaction. Fig. 2 shows the overview of our TTT framework.\n3.2.1 Online Reinforcement Learning\nInteractive Rollout Generation. For a given task, we generate multiple diverse trajectories by sampling from the policy‚Äôs action token distribution with temperature . Specifically, starting from initial state , at each timestep , the policy outputs action token probabilities and samples an action from the distribution. This action is executed in the environment, producing a new state . This closed-loop interaction continues until the estimated task progress exceeds a threshold (indicating completion) or the maximum horizon is reached, yielding a trajectory . By sampling trajectories with different random seeds, we explore diverse solution strategies.\nEnvironment Feedback. Each trajectory receives a reward that evaluates its quality. This reward signal, which we detail in ¬ß3.2.2, serves as the supervisory feedback guiding policy improvement. Unlike SFT which only learns from successful demonstrations, the reward signal provides differential feedback, distinguishing better trajectories from worse ones and enabling the model to discover and reinforce effective behaviors through trial and error.\nPolicy Update. We employ Group Relative Policy Optimization (GRPO) [shao2024deepseekmath] to update the policy. GRPO normalizes trajectory rewards within each batch to compute advantages and applies PPO-style clipping for stable updates, without requiring a separate value network.\n3.2.[ADDRESS_REMOVED]-time training is the absence of oracle reward signals (e.g., ground-truth success indicators from simulators) that are available during training in simulator but unavailable at deployment. We address this by learning a reward function based on task progress: an estimate of how much of the task has been completed.\nTask Progress as Reward Function. Progress-based rewards offer several advantages over binary success signals. First, they are dense: progress can be estimated at any point during execution, providing continuous feedback even for failed attempts. This density is crucial for sample-efficient learning, especially in long-horizon tasks where successful rollouts may be rare initially. Second, progress is a more general, grounded concept than task-specific metrics or black-box reward scores, making it applicable across diverse manipulation tasks.\nTask Progress as Termination Condition. Beyond providing rewards, task progress estimation also determines when to terminate rollouts. When estimated progress exceeds a predefined threshold, the rollout stops as the task is deemed complete; otherwise, execution continues until maximum horizon . This dual-purpose usage imposes stringent requirements on the estimator: it must be (1) computationally efficient, as it is queried frequently (every steps) to detect completion in real-time, and (2) temporally smooth and consistent, as erratic estimates can cause premature termination (stopping promising trajectories early) or delayed termination (wasting computation on completed tasks). While noisy rewards can be mitigated through averaging during policy learning, a single erroneous termination decision can truncate an entire trajectory. Therefore, stabilizing the progress signal is essential not just for learning efficiency, but for correct rollout execution.\nVanilla Progress Estimation. We employ a foundation critic model, VLAC [zhai2025vlac], which takes two images and task instruction as input and output a critic value. A positive value indicates how much the second image progresses the task compared to the first image. A negative value vice versa. Specifically, given a trajectory , we compute the reward as , where and are the initial and final observations of the trajectory, and is the task instruction. The estimated reward is then normalized to to serve as the trajectory reward for GRPO.\n3.3 Accumulative Progress Estimation\nWhile the progress critic provides dense feedback, we observe that it can be noisy and inconsistent, especially for long-horizon tasks involving multiple sub-goals. A single frame-pair comparison may be misled by superficial visual changes or fail to capture intermediate progress. As discussed in ¬ß3.2.2, this noisy estimation can negatively affect both reward feedback and rollout termination.\nTo address these challenges, we introduce an accumulative progress estimation mechanism. Our key insight is inspired by a slow-fast philosophy: instead of comparing the final state to the very beginning (which becomes unreliable for long trajectories), we maintain milestone frames at regular intervals and compute progress incrementally.\nInterval-Based Milestone Sampling. We define a sampling interval (e.g., 64 timesteps). During rollout, we maintain a list of milestone frames that captures the trajectory‚Äôs evolution at a coarse granularity. These milestones serve as reference points for measuring progress.\nIncremental Progress Computation. At a finer granularity (every steps, where ), we query the critic to estimate progress relative to the most recent milestone. Specifically, at timestep , we compute:\nwhere represents the incremental progress from the last milestone to the current state. When reaches a new milestone (), we append to and store in the critic history.\nAccumulative Value Aggregation.\nGiven a sequence of incremental critic values\ncollected at milestones, we accumulate them into a progress value that estimates task completion percentage:\nwhere indexes the milestones. This recursive formulation applies a diminishing returns principle: positive progress advances the value toward 100 by a fraction of the remaining distance, while negative critics decrease the value proportionally. Critically, adjustments scale with (the remaining gap to completion) prevents both overshooting from overly optimistic critics and catastrophic collapse from pessimistic ones.\nThe full mechanism is shown in Algorithm 1. It effectively smooths the noisy critic: by comparing to recent milestones rather than the distant initial state, we reduce the impact of long-term drift; by applying proportional adjustments rather than raw critic values, we create a more stable learning signal; and by accumulating progress incrementally with diminishing returns, we smooth out local fluctuations. Such a smoothed reward provides more reliable feedback for the reinforcement optimization.\nIn addition, this mechanism is also computationally efficient. Recall that since the progress need to called frequently for determining rollout termination, at timestep , a naive multi-frame approach would require critic calls to evaluate all pairwise comparisons, whereas our method requires only a single call‚Äîcomparing the current frame to the nearest milestone.\n3.4 Progressive Horizon Extension\nLong-horizon tasks present a fundamental challenge for test-time training: early in training, the policy is far from proficient and successful task completion is rare, making credit assignment difficult with noisy reward signals. Simply allowing free exploration until the maximum horizon leads to low-quality trajectories that provide weak learning signals. Even with our accumulative progress estimation, optimizing over very long horizons from the start can lead to unstable learning dynamics.\nTo address this, we adopt a progressive horizon extension strategy. We divide the training process into stages, where each stage operates with a maximum rollout horizon . As training progresses through stages, we gradually increase , allowing the policy to first master shorter sub-goals before tackling the complete task. In early stages, the agent focuses on immediate objectives and fundamental manipulation behaviors where the reward signal is cleaner and more direct. As the horizon extends in later stages, the policy learns to chain these behaviors together and reason over longer temporal dependencies, ultimately optimizing complete task execution.\nThis schedule provides several benefits. First, shorter horizons naturally reduce the accumulation of noise in progress estimation, as fewer milestone comparisons are needed. Second, early success on simpler sub-goals provides positive learning signals that would be absent when optimizing full-length trajectories from scratch. Third, the staged progression allows the policy to build compositional skills, where early stages establish robust primitives, while later stages learn to orchestrate them.\nImportantly, progressive learning and accumulative progress estimation are complementary mechanisms. The progressive curriculum addresses temporal credit assignment, i.e., determining when and what to learn, by controlling the optimization scope. Accumulative estimation addresses noisy rewards, i.e., stabilizing the feedback signal, by aggregating incremental progress. Together, they enable robust test-time training on long-horizon manipulation tasks where both challenges are present.\n4 Experiments\nBenchmark. We evaluate our method on the LIBERO benchmark [liu2023libero], a widely used simulation benchmark for lifelong learning in robotic manipulation. LIBERO focuses on language-guided manipulation tasks across diverse object types, task specifications, and environments. It consists of four task suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. Each suite contains 10 tasks, with 50 expert demonstrations per task. We report the average Success Rate (SR) across 50 trials for each task, following the evaluation protocol in previous work [kim2024openvla, li2025simplevla].\nBase Model. We apply our method to OpenVLA-OFT [kim2025fine], a state-of-the-art autoregressive VLA model that achieves high performance and inference efficiency. Following prior work [li2025simplevla], we adopt the action chunking and parallel decoding designs, while disabling the continuous action regression head, i.e., use discrete action tokens instead. This would enable action generation compatible with the optimization of reinforcement learning.\nReward Model. For test-time training, we employ a foundation critic model VLAC [zhai2025vlac] as the progress estimator. VLAC takes two images and a language instruction as input and outputs a critic value indicating how much the second image represents progress toward task completion compared to the first image. This foundation model has been pre-trained on large-scale robotic manipulation datasets, demonstrating its ability to estimate task progress across diverse tasks and environments.\n4.1 Main Results\nTab. 1 presents our main results on the LIBERO benchmark, comparing our TTT framework against state-of-the-art VLA models. We apply TTT to the OpenVLA-OFT model (pre-trained with full trajectory demonstrations), enabling it to continue learning during deployment.\nSignificant Performance Gains. Our TTT framework achieves substantial improvements across all four LIBERO task suites. On average, we observe a +6.5% absolute gain in success rate, elevating the baseline from 89.2% to 95.8%. The improvements are consistent across diverse task types: +4.1% on LIBERO-Spatial, +7.3% on LIBERO-Object, +6.0% on LIBERO-Goal, and most notably, +8.6% on LIBERO-Long. The substantial gain on LIBERO-Long is particularly significant, as this suite contains the most challenging long-horizon tasks with complex multi-step procedures. With TTT, our method achieves 95.8% average success rate, surpassing models like (94.2%) and matching UniVLA (95.2%), demonstrating that test-time adaptation can be as effective as collecting and training on large amounts of additional demonstration data.\nChallenge of Naive Reward Modeling. We also compare with SimpleVLA, which initially employs the binary outcome reward from the simulator, i.e., oracle reward. We then replace the oracle reward with our progress estimator, and use a simple threshold-based approach to convert progress estimates into binary outcome rewards. This version of SimpleVLA achieves only 87.7% on LIBERO-Long, a modest +1.9% improvement over the SFT-only baseline (85.8%). The limited gain highlights a critical challenge: directly using a noisy progress estimator to generate binary rewards for online RL is insufficient. In contrast, our accumulative progress estimation mechanism that smooths noisy signals and provides dense, stable feedback achieves 94.4% (+8.6%), demonstrating the importance of properly ‚Äútaming‚Äù the reward model.\n4.[ADDRESS_REMOVED] of collecting extensive demonstration data. To evaluate TTT‚Äôs effectiveness in low-data scenarios, we experiment with a more challenging setting: only one demonstration per task for SFT pre-training111All 1-trajectory SFT models are reused from the SimpleVLA-RL released checkpoints [li2025simplevla]., followed by test-time training.\nAs shown in Tab. 2, the 1-shot SFT baseline (OpenVLA-OFT) achieves only 43.6% average success rate, indicating that a single demonstration is insufficient for learning robust manipulation policies. However, applying our TTT framework yields substantial improvements, achieving 61.3% average success rate, which is a remarkable +17.7% absolute gain. The improvements are consistent across all task suites: +8.3% on LIBERO-Spatial, +29.9% on LIBERO-Object, +7.5% on LIBERO-Goal, and +22.0% on LIBERO-Long. These gains validate our core claim: test-time training can effectively alleviate the data collection burden by enabling learning from self-generated experiences rather than relying solely on extensive expert demonstrations.\n4.[ADDRESS_REMOVED] Zero-Shot Cross-Task Generalization\nAn intriguing capability enabled by our TTT framework is cross-task generalization through online learning. To explore this, we conduct a preliminary experiment: we take a VLA model pre-trained exclusively on LIBERO-Long tasks (50 demonstrations per task) and directly deploy it on LIBERO-Object tasks without fine-tuning on task-specific demonstrations.\nWhen deployed directly, the LIBERO-Long pre-trained policy achieves 0% success rate on LIBERO-Object, as expected. Although, conceptually, the two task suites may share some common motion primitives, the behavior cloning paradigm strongly hinders generalization. Remarkably, however, by applying our TTT framework with progress-based feedback, the policy adapts purely through autonomous exploration and reaches 20.8% success rate on LIBERO-Object. While this performance remains modest compared to task-specific SFT baselines (which achieve 40.1% with a single demonstration and 96.6% with 50 demonstrations), the ability to break 0 success rate without finetuning on task-specific human demonstrations represents a qualitatively different capability. To the best of our knowledge, no prior VLA training method has demonstrated such cross-task transfer through test-time adaptation alone. This preliminary result suggests that TTT, when paired with a foundation-level progress estimator like VLAC, can enable VLAs to generalize across task distributions through self-directed learning.\n4.4 Ablation Studies\nAccumulative Progress Estimation.\nTab. 3 validates the effectiveness and efficiency of our accumulative progress estimation mechanism with various frame sampling strategies. F-Score is computed based on a balanced validation set (100 success cases, 100 failure cases) assessing task progress estimation performance. The baseline that directly uses 2-frame critic values without accumulation achieves 88.3% success rate with [ADDRESS_REMOVED] calls but suffers from low F-score (0.04), indicating unreliable progress estimation. When incorporating accumulative progress estimation, the sampling strategy for millstone frames matters. The uniform sampling variants improve F-score over baseline but at the cost of significantly more reward calls (96 and 224 respectively), with diminishing or even negative returns in success rate, suggesting that naive dense sampling introduces noise without proper temporal structure. In contrast, our method achieves the best performance (91.3% SR, 0.20 F-score) while maintaining computational efficiency with only [ADDRESS_REMOVED] calls, demonstrating that interval-based sampling (with a sliding ) combined with accumulative aggregation is both more effective and more efficient than naive uniform approaches.\nProgressive Horizon Extension.\nTab. 4 demonstrates the importance of progressive horizon extension for long-horizon tasks. Starting from the SFT baseline (85.8%), we examine three TTT variants. First, using binary outcome rewards (thresholding the progress estimator) yields only 87.7% (+1.9%), confirming that converting dense progress into sparse signals loses valuable learning information. Second, applying dense rewards from our accumulative progress estimator without progressive horizon achieves 91.3% (+5.5%), showing the benefit of dense feedback. Finally, adding progressive horizon extension, i.e., gradually increasing the maximum rollout length during training, reaches 94.4% (+8.6%), providing an additional 3.1% gain. This validates our strategy: by initially constraining exploration to shorter horizons and progressively extending them, the policy learns more stable sub-task skills before tackling full-length trajectories, making it more resilient to estimation errors in long-horizon tasks.\n4.[ADDRESS_REMOVED]-time training shapes policy behavior, we analyze representative rollout trajectories after TTT in Fig. 3. First, the policy develops error recovery capabilities: when initial grasp attempts fail, the SFT-only policy usually continue with the pre-programmed motion and fails, whereas after TTT the policy autonomously re-attempts grasping (top row). Second, the policy adapts to pick an object but accidentally changed the object state, then it adjusts its motion to fit the new config rather than rigidly following memorized patterns (middle row). Third, the policy discovers alternative manipulation strategies not present in demonstrations. For instance, grasping a pot by its body instead of the handle (bottom row). These improvements indicate that progress-based feedback enables the policy to generalize beyond trajectory-level imitation to goal-oriented manipulation and explore diverse solutions.\nDespite the improvements, we observe failure cases that reveal a fundamental challenge: misalignment between the environment‚Äôs rule-based success criterion and the semantic task completion assessed by our progress estimator. This mismatch manifests in two ways as shown in Fig. 4. First, in some cases the policy brings the scene very close to the goal state, leading the progress estimator to assign high rewards (near-completion signal), yet the environment‚Äôs coordinate-based rules still judge the task as unsuccessful. This creates a form of ‚Äúreward hacking‚Äù where the policy optimizes for high progress scores without meeting the strict environmental criteria. Second, the opposite occurs: the environment judges tasks as successful based on coordinate rules despite semantic incompleteness. For instance, in Fig. 4, a book placement task where the environment reports success because the book‚Äôs coordinates satisfy the spatial constraints, yet semantically the book is not properly placed inside the shelf. These misalignments highlight the inherent difficulty in aligning rule-based simulation criteria with semantic task understanding, suggesting that future work should explore improved calibration between progress estimators and environment oracles.\n[ADDRESS_REMOVED]-time training for VLA models is feasible by addressing noisy progress estimation through accumulative estimation and progressive horizon extension. This foundational step enables VLAs to learn from experience rather than merely imitating demonstrations, unlocking several promising directions for future research. First, developing more robust reward models would significantly enhance the framework‚Äôs capabilities. While our accumulative mechanism effectively handles noisy estimates, future reward models with better semantic alignment to environment success criteria could reduce the mismatch between progress estimation and rule-based success signals. Additionally, improving zero-shot capability would eliminate the need for in-context examples, enabling truly zero-shot cross-task generalization. While our current approach shows promising cross-task transfer (LIBERO-Long LIBERO-Object), the reward model still benefits from task-specific context; future reward models trained on more diverse manipulation data, with better generalization capability could enable seamless adaptation to entirely novel tasks without any task-specific examples, even for the reward model.\nSecond, extending test-time training to real-world robotic deployment presents both opportunities and challenges. The long training times required for online RL can be prohibitive in physical environments, where data collection is inherently slower than simulation. Future work can explore techniques to accelerate real-world training, such as sim-to-real transfer for reward models, parallel robot deployment for distributed data collection, or more sample-efficient online learning algorithms. Equally important is ensuring safety during exploration: the uncontrolled policy behavior in early training stages could damage the robot or environment. Developing safety mechanisms‚Äîsuch as action constraints, safety critics, or human oversight protocols‚Äîwould be crucial for enabling safe autonomous learning in physical environments. Third, exploring more sophisticated exploration strategies and curriculum designs could further improve sample efficiency and enable adaptation to even more complex, long-horizon manipulation tasks.\n6 Conclusion\nWe introduced EVOLVE-VLA, a test-time training framework that enables VLA models to continuously adapt through environment interaction, addressing the fundamental limitations of static SFT. Inspired by how humans develop manipulation skills through practice and trial-and-error, our approach shifts VLAs from rigid trajectory memorization toward genuine adaptive learning. By replacing impractical oracle rewards with a learned progress estimator and introducing two key technical contributions: (1) accumulative progress estimation and (2) progressive horizon extension, we demonstrate that VLAs can effectively learn from inherently noisy, self-generated feedback signals. Our experiments on the LIBERO benchmark validate this approach, achieving +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and enabling cross-task generalization (0% ‚Üí 20.8%) without task-specific demonstration training. Beyond these quantitative gains, we observe emergent capabilities like error recovery that arise purely from autonomous exploration. We believe this work represents an essential step on the path toward truly general-purpose VLA systems that can continuously learn and improve in real-world deployment."
  },
  {
    "article": "Enhancing Visual Sentiment Analysis via Semiotic Isotopy-Guided Dataset Construction\nAbstract\nVisual Sentiment Analysis (VSA) is a challenging task due to the vast diversity of emotionally salient images and the inherent difficulty of acquiring sufficient data to capture this variability comprehensively. Key obstacles include building large-scale VSA datasets and developing effective methodologies that enable algorithms to identify emotionally significant elements within an image. These challenges are reflected in the limited generalization performance of VSA algorithms and models when trained and tested across different datasets. Starting from a pool of existing data collections, our approach enables the creation of a new larger dataset that not only contains a wider variety of images than the original ones, but also permits training new models with improved capability to focus on emotionally relevant combinations of image elements. This is achieved through the integration of the semiotic isotopy concept within the dataset creation process, providing deeper insights into the emotional content of images. Empirical evaluations show that models trained on a dataset generated with our method consistently outperform those trained on the original data collections, achieving superior generalization across major VSA benchmarks.\nI Introduction\nVisual Sentiment Analysis (VSA) is a multidisciplinary field that combines computational methods with theoretical insights from psychology, semiotics, and affective science to understand and classify the emotional content of images. VSA is applied in various fields including marketing [VSOBorth2013], social media analysis [You2015], artistic and cultural analysis of image creation and use [Peng2016], movie and media industry [Campos2017].\nIn healthcare, VSA contributes to the early diagnosis of mental disorders [Zhou2020], supports the curation of emotionally positive imagery to enhance hospital environments [Campos2017], and helps monitor patients‚Äô emotional well-being.\nVSA research requires the availability of high-quality image datasets labeled with the emotions aroused in the observers. The creation of such datasets faces the complex challenge of managing the vast range of visual element combinations that can evoke different emotions. In fact, the emotional impact of an image is influenced by numerous factors, including semantics, depicted subjects, colors, brightness, and many other features and their possible interactions. This complexity makes it difficult to build a dataset covering all possible emotionally relevant visual combinations. As a result, when such incomplete datasets are used to train modern VSA systems based on machine learning, some combinations may not be correctly recognized. Furthermore, the limited selection of images, sometimes focused on specific content like art or particular domains, introduces a bias that negatively affects VSA algorithms‚Äô ability to correctly classify images from different contexts. Dataset biases often lead algorithms to focus on distracting elements that are not related to the emotional impact conveyed by a certain image. For instance, if within a dataset cars frequently appear in images evoking a positive sentiment, a model trained on such a dataset might erroneously associate all car images with positive emotions regardless of the context wherein they are used.\nThese limitations reduce the accuracy and generalization capability of machine learning algorithms, hindering their performance on different datasets. Such difficulties are clearly evident in the performance drop of models tested on datasets different from those used for training.\nThe most common approach to address these challenges is to construct increasingly larger datasets, based on the assumption that a greater number of images will naturally enhance the diversity of content and provide a broader range of emotionally evocative combinations. However, the construction of such large-scale datasets is incompatible with manual selection and annotation, hence requiring automated construction methods [VSOBorth2013] [t4s2017]. This in turn decreases the quality of emotional labels. In addition, most of the datasets built so far use heuristic approaches to increase diversity, without grounding the selection process on theoretically sound principles. More recent research efforts have proposed alternative methodologies inspired by findings from cognitive science, aiming to guide automatic image selection and annotation through conceptually grounded principles [VippSent_2025_WACV]. Models trained on datasets constructed using these techniques consistently outperform previous approaches in classification accuracy.\nIn this paper, we propose a new methodology, referred to as Selecta, for automatically constructing large, accurately annotated VSA datasets containing a wide variety of emotionally relevant patterns by leveraging multiple VSA models trained on a pool of pre-existing data collections to guide the selection and labeling of images crawled from the internet.\nSelecta relies on two basic ideas: i) exploiting the availability of a pool of VSA models trained on different datasets to filter out images containing spurious or distracting patterns, and ii) leveraging the semiotic concept of isotopy to include in the dataset images with emotional patterns that were not present in the original collections. Filtering is achieved by crawling the internet and retaining only images for which all, or the majority, of the pre-trained VSA models provide the same result. The inclusion of novel emotionally-relevant patterns unknown to the pre-trained models relies on the isotopy concept, which describes how elements present in a text or an image produce meaning In semiotics, isotopy refers to the repetition of semantically coherent elements that enables a text or image to communicate its message unambiguously to the observer. The central idea behind Selecta is to leverage this principle to construct an augmented large-scale dataset with reliable emotional labeling, thereby making it possible to train machine learning models focusing on genuinely relevant emotional features.\nTo validate our methodology, we focused on the classification of images into emotionally positive, negative and neutral classes, and used this approach to construct a new large dataset starting from an ensemble of VSA base models pre-trained on existing datasets, and comparing the performance of the original models with that of a model trained on Selecta111With a slight notation abuse in the following we use the term Selecta to indicate both the dataset and the methodology used to build it, the exact meaning being always clear from the context.. We then ran extensive experiments showing that the models trained on Selecta achieve better performance when tested on all major VSA datasets, outperforming both the algorithms trained on the original data collections and those trained on the most popular currently available datasets.\nThe rest of this work is organized as follows. In Section II, we review the main VSA datasets available today. In Section III, we outline the theoretical foundations of Selecta and explain how they can be applied in practice. Section IV introduces a set of simulations designed to demonstrate the effectiveness of Selecta in a simplified and controlled setting. In Section V, we describe the actual construction of Selecta dataset. In Section VI, we describe the experiments we ran to validate our methodology. Finally, in Section VII, we draw our conclusions and outline directions for future work.\nII State of the art\nDeveloping a dataset for Visual Sentiment Analysis (VSA) involves significant challenges, including selecting reliable annotation methods and maintaining adequate quality and size [Ortis2020]. Existing datasets can be divided into two main categories: those designed to train Artificial Intelligence models and those intended for psychological studies on human emotional responses.\nThe latter, which we refer to as PC datasets, rely on continuous classifications grounded in psycho-cognitive theories and organize emotions within the Dimensional Emotion Space (DES) [dimensionalscherer2009], where emotions are represented as points in continuous spaces defined by valence, arousal, and dominance, and are annotated in a highly controlled manner by experts in emotional psychology. Although PC datasets provide highly accurate annotations, their limited number of images constrains their use for training AI models. The main examples are IAPS [Iaps], which includes 1,200 images divided into three classes (positive, negative, neutral); the IAPS-a variant, with 395 images based on Mikels‚Äô eight emotions; and GAPED [gaped], which contains 730 images labeled as positive, negative, or neutral.\nIn this work, we focus on datasets designed to train VSA AI models. These kind of datasets requires a large number of images to cover the wide range of emotionally relevant possibilities and to ensure that the models are exposed to as much emotionally relevant information as possible during training. These datasets are mostly labeled with classes corresponding to discrete emotion categories, known as Categorical Emotion States (CES) [dimensionalscherer2009]. The granularity of these classifications can vary, ranging from simple binary categories (e.g., positive vs. negative) to more varied sets, such as Paul Ekman‚Äôs six basic emotions [Ekman1992], or the eight primary emotions identified by Joseph Mikels [mikels2005emotional]. Mikels‚Äô emotions can be easily mapped to positive or negative categories, while Ekman‚Äôs emotions include surprise, which may encompass both positive and negative elements. Some datasets go beyond basic emotions by incorporating a broader set of categories [emotic].\nOne of the main challenges in building a VSA dataset for AI training, is finding the best compromise between highly accurate labeling and the necessity of annotating a large number of images. To tackle this challenge, VSA datasets are usually created using three distinct labeling approaches: manual, hybrid, and automated. Among the manually labeled datasets, we mention Emotion6 [Emotion6], which contains 1,980 images labeled by 15 annotators based on Ekman‚Äôs emotional model, and FI [FIYou2016BuildingAL], including 23,308 images annotated by five individuals per image, also following Ekman‚Äôs framework. Another manually labeled dataset is Emotic, consisting of 18,360 images divided into 26 emotional categories, including the six basic emotions defined by Ekman and 20 additional emotional classes.\nA key limitation of these datasets is the use of uncontrolled online platforms for data collection, which introduces potential individual biases in emotion classification. To mitigate this issue, some datasets adopt a hybrid approach consisting of an initial phase in which images are retrieved using emotionally salient queries, followed by a validation phase in which the emotional content is verified through interviews with human subjects. Examples of hybrid-labeled datasets include Flickr (90,139 images) [katsurai2016] and Instagram (65,493 images) [katsurai2016]. The images in these datasets are labeled according to three categories: positive, negative, and neutral emotions. A further example of this category is Emoset [yang2023L], which includes 118,102 images labeled according to Mikels‚Äô eight emotions, with annotations provided by five individuals per image. The hybrid labeling approach adopted in [yang2023L] led to improvements over manual labeling, both in the accuracy of models trained on this dataset and in their generalization capability on external data. Since Emoset relies on interviews with multiple human subjects, the method lacks scalability.\nEventually, several large-scale datasets have been created by using fully automated approaches, which offer the advantage of enabling the annotation of vast amounts of data with minimal human intervention. Notable examples of this category include VSO [VSOBorth2013] and T4S [t4s2017]. The Visual Sentiment Ontology (VSO) dataset comprises 500,000 images labeled with emotional adjective-noun pairs (ANPs) based on frequency of usage. Images were downloaded using queries composed of an emotion-related adjective and a commonly associated noun, and were labeled according to the emotional valence of the adjective. The dataset includes two classes: positive and negative emotions. While this approach enables the construction of large-scale datasets, it often lacks labeling accuracy [VSOBorth2013]. T4S contains 1.5 million images sourced from Twitter, and annotated by analyzing the accompanying text.\nAutomatic dataset annotation significantly increases the volume of the dataset at the cost of lower labeling quality. A method to improve image labeling accuracy while using fully automatic annotation has been proposed in [VippSent_2025_WACV] (VippSent dataset), by relying on the application of techniques derived from cognitive science and semiotic studies. VippSent was built using an automatic labeling system based on the principles of visual semiotics and the mechanisms of emotional meaning in images. The dataset is divided into three classes: positive, negative, and neutral. VippSent was constructed by downloading images of the same subject associated with opposite and neutral adjectives, in order to encourage the classifiers to go beyond the subjects depicted by the images, to focus on the visual features that contribute to conveying emotions. The classes of emotionally charged images (both positive and negative) were enriched with artistic representations of emotional concepts.\nIn general, VSA algorithms exhibit limited cross-testing capabilities, with significant drops in accuracy when evaluated on datasets other than those used for training. Table I presents the cross-dataset performance of models trained on the main VSA datasets. The table includes the main datasets among those that include both positive and negative classes, covering datasets with three classes, including a neutral class, as well as those based on Mikels‚Äô eight emotions [mikels2005emotional], which can be grouped into positive (Joy, Trust, Anticipation, Surprise) and negative (Sadness, Disgust, Anger, Fear) categories. This selection allowed us to perform cross-testing by training algorithms on each dataset and evaluating them on the others. To test algorithms trained on two-class datasets against three-class datasets, we removed the neutral class from the test set. Conversely, to test algorithms originally trained on three-class datasets against two-class datasets, we trained the algorithms using only two classes by excluding the neutral class. As shown in Table I, the best performances in terms of generalization capability are achieved by models trained on sufficiently large datasets with accurate labeling techniques. This suggests that the limitations in the generalization capability are determined by an insufficient variety of emotionally relevant images contained in the training datasets or by unreliable labeling.\nIII Methodology\nIn this section, we describe the Selecta methodology for the construction of VSA datasets. We start by introducing the semiotic principles that Selecta relies on, then we explain how such principles can be used to construct a VSA dataset with enhanced diversity and generality. Eventually, we provide some insights into the way the construction of Selecta helps to reduce bias and to improve the generalization capability of the dataset.\nIII-A Isotopy in semiotic theory\nThe isotopy concept, introduced by A. J. Greimas [greimas1987meaning], explains how meanings emerge from a text or an image, and it is also applicable to emotional meanings [greimas1993semiotics]. Isotopies are semantic patterns created by the recurrence of terms or visual elements that converge toward a shared meaning. Their interplay ensures coherence and allows the text or image to convey specific emotional or semantic significance.\nIn the semiotic analysis of a text, the basic building blocks are lexemes, words with autonomous meaning, such as dog, cat or man. Lexemes can be attributed with semes, which are semantic properties that characterize them. For example, the lexeme dog can be associated with semes like animal, quadruped, or mammal. When a lexeme is placed in a context and combined with other lexemes, additional semes can emerge from their interaction. For instance, in the expression dog barks, semes such as anger, fear, or communication are not strictly inherent to the lexemes but can be activated in context through their combination. If we expand the expression to dog barks at the wind, further semes may appear, such as futility or frustration, which arise from the interplay of lexemes within the text. Isotopy refers to a phenomenon where different expressions in a text share the same semes, thereby creating a homogeneous meaning. For example, in the sentence: ‚ÄúThe dog barked, he slammed the door, and shouted,‚Äù the expressions dog barked, slammed the door, and shouted share the seme anger. This recurrence generates an isotopy of anger, providing emotional coherence to the sentence. Notably, the lexeme anger does not appear explicitly, but the meaning emerges from the homogeneity of the semes in the text.\nIsotopy was initially studied with reference to written texts but has since been extended to images through visual semiotics. In this way, isotopy can also explain how meaning emerges from images. For example, in an image depicting a sunset on a tropical beach and a man drinking a cocktail, we can recognize semes such as relax and vacation. However, the individual visual lexemes, such as the sun, the sky, the beach, or the cocktail, evoke these semes only partially or indirectly. For instance, in a different context, such as during a storm, the beach would not evoke relax or vacation at all. The overall meaning emerges from the interaction and coherence among the various visual elements, just as it does in written texts [floch2000visual].\nThe principle of isotopy is activated whenever there is an intention to convey a message, whether through verbal language or visual means. In most cases, this process occurs unconsciously and concerns the compositional choices made by the enunciator in constructing the textual or visual message [greimas1987meaning]. When generating a message by taking a photograph, for instance, scenes whose elements are coherent with one another are chosen in order to converge toward the intended meaning [sontag1977]. In the case of positively connoted events, specific colors and lighting conditions tend to be favored; conversely, negative events are typically represented through different color palettes and visual solutions [boltanski1993souffrance].\nIsotopic mechanisms are recurrent in the vast majority of images produced and disseminated in communicative contexts, such as digital platforms, and can be leveraged to improve the effectiveness of algorithmic-based classification systems. In fact, isotopy can help explain the difficulty that models trained on one dataset face when classifying images from another dataset. The fact that emotions can be determined by the convergence of multiple elements, rather than by the mere presence or absence of individual ones, makes it harder to classify images coming from different contexts. On the other hand, understanding the isotopy mechanism can help improve the capabilities of VSA algorithms.\nIII-B Exploitation of isotopy for automatic dataset construction\nIn this work, we rely on the isotopy principle to address the limited generalization capability of VSA models trained on existing datasets.\nThe isotopy principle suggests that the emotional meaning of an image emerges from the combination of elements and their semantic value, which together convey meaning to the viewer. In contrast, AI models tend to rely on the presence of individual visual elements to classify images into different categories, which limits their ability to classify images based on abstract concepts such as emotions [geirhos2019imagenet][zhang2019interpreting].\nOur hypothesis is that VSA models tend to exhibit a bias in emotion classification caused by the distribution of visual elements in the training data. Emotions are abstract concepts that cannot be expressed through unique and easily detectable visual elements, but rather through a multitude of possible combinations. A concrete lexeme such as ‚Äùdog‚Äù corresponds visually into a single visual lexeme, characterized by clearly shared elements: muzzle, tail, legs. An abstract lexeme such as ‚Äùlove‚Äù can have a variety of visual representations, such as two lovers kissing or a heart symbol, which emerge not from individual elements alone but from specific contextual combinations. This broad and heterogeneous variety of visual forms used to represent abstract concepts such as emotions is at the root of classification biases. If in a given dataset certain emotional classes contain images with recurring elements, such as specific colors or subjects, the model may associate those elements with that emotion, even when they appear in images with a different emotional value, thus producing a classification bias. The type of images present in different datasets influences the presence of such biases: for instance, artistic photography tends to associate certain colors or subjects with particular emotions [machajdik2010affective], whereas social media shows a more heterogeneous distribution. On social media, many everyday situations are represented only in emotionally positive images, while their negative representations are underrepresented [masciantonio2025positivity]. Taken together, these biases limit the generalization capability of models across datasets different from those used in training.\nTo mitigate such biases, it is essential to build large and diverse datasets in which different elements appear across various emotional classes, so that the model learns to recognize emotions from complex patterns that genuinely evoke them, rather than from recurring individual visual elements. It is also necessary to ensure reliable automatic image labeling, in order to expand the dataset without introducing bias or noise.\nIII-C Pipeline of Selecta\nFigure 1 illustrates the procedure we are proposing to build a coherent and balanced emotional dataset based on the principle of isotopy. The process unfolds as follows. We assume to have access to a large source of unlabeled images, as well as to labeled data collections that offer a limited and potentially incomplete characterization of images conveying emotions belonging to predefined classes (positive, negative, and neutral emotions in our experiments). As a first step, the original datasets are used to train an ensemble of base VSA models. These models are then employed to independently annotate the unlabeled image collection. The resulting independent labels are compared to assess whether a consensus exists among the base classifiers. Different consensus strategies can be applied. For small values of , unanimous agreement may be appropriate, whereas for larger , a predefined majority might suffice. Only images for which a consensus is reached are retained to form the new dataset. Their labels are automatically determined by the agreed-upon classification from the ensemble of base models.\nIn building the dataset and conducting the simulations, we chose to use three datasets for training the algorithms employed in image selection, specifically three datasets that classify images into three emotion classes: positive, negative, and neutral, a categorization widely used in psychology [Russell1980]. However, in principle, our methodology is applicable to any number of datasets with any number of emotion classes, provided that the images in the different datasets are classified according to the same classes.\nThe filtering and labeling methodology described in Figure 1 leverages two fundamental principles of isotopy. The first principle is that the emotional meaning of an image arises from the coherence among multiple elements belonging to the same isotopy [greimas1987meaning, floch2000visual], rather than from the presence of a single element. Our filtering system increases the likelihood of selecting images that contain concordant elements capable of evoking a specific emotion, while simultaneously reducing the influence of biases linked to individual visual components.\nThe second principle is that elements within an isotopy tend to recur in regular combinations across images expressing the same emotional polarity. This pattern emerges because image creators - either consciously or not [sontag1977] - tend to favor compositions that include as many reinforcing elements as possible to support the intended emotional message. As a result, statistically, if an image contains certain elements that form an emotional isotopy, it is highly probable that it also includes others that are consistent with it. Consequently, images that are classified consistently by multiple classifiers are not only likely to contain a coherent emotional isotopy, but also contribute new, representative elements of the same isotopic chain, that were not present in the original datasets, thereby enriching the resulting dataset.\nIII-D Isotopy-based analysis\nIn this section, we provide some insights about the mechanism whereby Selecta contributes to reducing bias and improving the generalization capability of the dataset.\nIt is well established that deep learning models classify images by identifying specific visual patterns learned during training [geirhos2019imagenet, zhang2019interpreting]. We hypothesize that these patterns can be roughly grouped into three categories based on their relationship with the emotional content of the image they belong to.\nPattern A\nEmotionally relevant visual patterns, that is, sets of visual lexemes that, when combined within an image, generate an overall meaning (sememe) that is part of an emotional isotopy and thus evokes a specific emotion in the viewer. For example, combining the visual lexeme of a house with that of fire coming out of a window can evoke concepts such as destruction and danger, conveying a negative emotional value to the image.\nPattern B\nThese are visual lexemes that, although appearing frequently in a specific emotional class, do not generate a typical emotional effect when taken in isolation. When combined with other coherent elements (Pattern A), however, they can contribute to conveying a clear emotional meaning. If during training these patterns appear only within a single emotional class, they may result in a classification bias. For example, a skull is a visual lexeme commonly found in images labeled as negative. When paired with other elements such as war scenes, weapons, or destruction, it contributes to the formation of a negative isotopy. However, a skull can also appear in positive contexts - for instance, a child dressed up for Halloween. If a model has been trained to associate the skull solely with negativity, it may induce a bias. In such cases, the classifier might overlook other positive cues in the image - such as the festive atmosphere created by the child‚Äôs smile or the presence of candies - and incorrectly classify the image as one conveying a negative emotion.\nPattern C\nThese are visual lexemes that appear incidentally within images of a given emotional class but do not inherently carry an emotional meaning. For example, if in a dataset chairs happen to occur only in positive images, the model may incorrectly learn to associate the presence of a chair with a positive emotion, leading to a classification bias.\nThe Selecta filtering procedure, based on consensus among models trained on different datasets, leverages the isotopy principle to enrich the dataset with Pattern A elements. In fact, images that receive consistent classifications across the base models are more likely to contain type A patterns, as it is unlikely that type B - and, even more, type C - patterns would lead to an agreement among the base classifiers. As more images are processed, type B elements tend to distribute more evenly across emotional classes, often balanced by stronger type A patterns. This reduces their biasing effect and enhances generalization. Meanwhile, the influence of type C patterns is minimized, as random elements are unlikely to appear consistently within a single class across different datasets.\nLastly, images containing type A patterns often include additional visual elements that reinforce the dominant isotopy, thereby enhancing the emotional richness and semantic depth of Selecta dataset. This enrichment is enabled by the statistically recurrent combinations that characterize elements within an emotional isotopy.\nIn the rest of the paper, we exploit the concepts discussed in this section for the construction of a three-class VSA dataset containing images conveying positive, negative, and neutral emotions.\nIV Simulations\nTo validate our methodology, we conducted some simulations to observe the behavior of a classifier based on Selecta dataset within a simplified environment. Thanks to this simplified model, we are able to simulate mechanisms analogous to isotopy, and verify that the consensus-based filtering strategy can effectively leverage isotopic structures to construct an augmented dataset and train a classifier with improved generalization capabilities.\nTo start with, an image (and its content) is abstractly modeled as a multiset of symbols drawn from a basic alphabet, where a multiset (also known as a bag) is a non-ordered collection of symbols. In contrast to ordinary sets, in a multiset elements can appear multiple times, and the number of occurrences (called the multiplicity) matters. In our simulations, symbols represent a simplified version of the visual lexemes that make up an image. For example, each symbol can correspond to a meaningful shape, such as the silhouette of a ‚Äùdog‚Äù, or to a color, a certain brightness level, or any other visual element contributing to the overall meaning of the image. More formally, given an alphabet , an image is modeled as a non-ordered bag of symbols drawn from :\nwhere the number of symbols contained in , , may vary from one image to the other.\nIn semiotics, lexemes possess a form, given by the visual elements that compose them, and a semantic content, that is, the meaning they evoke in the viewer [barthes1964]. In our multiset model, each symbol is assigned an emotional polarity: positive, negative, or neutral. The overall emotional label of the image is determined based on the emotional categories of the symbols contained in it. If neutral symbols constitute an absolute majority, the image is classified as neutral (). Otherwise, it is classified as positive () if positive symbols outnumber negative ones, and negative () if the opposite is true. In case of a tie between positive and negative symbols, the negative label prevails. Formally:\nThis mechanism provides a simplified representation of the tendency of emotional stimuli to prevail over neutral ones, and of negative stimuli to prevail over positive ones [vuilleumier2005how].\nThe simplified representation of an image as a multiset of symbols is consistent with how semiotics analyzes image signification. Visual semiotics examines the meaning of images through the grammar of visual language, where visual elements play the role of words whose combination produces the overall sense of the image [GrammaticaVisivakress2006reading]. In this context, meaning, including the emotional component, emerges from the overall visual context rather than from individual elements [barthes1964]. A limitation of the simplified image model used in our simulations is that the spatial position of elements is not taken into account, even though it can be crucial in some cases. However, according to the principle of isotopy, it is rare for the emotional impact of an image to depend solely on the arrangement of objects, as other coherent elements are often present that reinforce the overall emotional effect.\nWe also adopted a simplified mechanism to simulate a classifier trained on a dataset of images (multisets in our case). Specifically, we implemented our classifiers based on the Intersection over Union (IoU) similarity metric, adapted to work on multisets. Given two images and , we define their Intersection over Union (IoU) similarity as:\nwhere and denote the sets of unique symbols contained, respectively, in the multisets and , and indicates the cardinality of a set.\nWith the above definitions, let be a set of images, and a base classifier based on . Given a test image , the classifier computes IoU for all the ‚Äôs in and assigns to the same label of the most similar image in . Ties are solved by looking at the second most similar image and so on. The construction of Selecta dataset, then, proceeds as illustrated in Figure 1.\nThe goal of the simulations is to observe in a simplified and controlled environment how the Selecta dataset emerges from the original datasets ‚Äôs, how the presence of isotopies influences the composition of Selecta dataset, and to what extent a classifier based on Selecta improves the performance of the base classifiers relying on the original datasets ‚Äôs.\nIn particular, we aim to verify two fundamental hypotheses. The first hypothesis is that Selecta effectively enforces the selection of images containing well-defined isotopies (Pattern A in Section III-D), while reducing the impact of individual visual elements that may introduce classification bias (Patterns B and C in Section III-D). The second hypothesis concerns the ability of the Selecta to exploit the recurrence of elements that characterize images belonging to the same emotional class, as hypothesized by the principle of isotopy. In a context where the isotopy principle holds, Selecta should enable the selection of images containing new visually and emotionally relevant elements within the constructed dataset and increase the generalization capacity of algorithms trained on it.\nIV-A First simulation\nThe first simulation is a very simple one. We considered a case in which the number of original datasets used to build the original classifiers is equal to 3. Given the limited number of classifiers, we adopted a simple consensus mechanism, whereby Selecta retains only the images for which all three classifiers produce the same result. We ran the simulations with an alphabet of 100 symbols: 34 were assigned a neutral label, 33 a positive label, and 33 a negative one.\nAs a first step we generated the training sets222In fact in our simulations there is no training, given that the classifiers operate according to a kind of Nearest Neighbor (NN) strategy, based on IoU similarities. used by the three base classifiers. Let us indicate these sets by , and , and the corresponding classifiers by , and . All the images (multisets) composing the original datasets were generated independently. A multiset is generated by drawing symbols at random from the alphabet. Image labels are assigned using the majority-based criterion shown in (1). We run several simulations considering different multiset cardinalities, to evaluate the effect that the complexity (here approximated by the number of symbols contained in a multiset) of the images has on the effectiveness of Selecta. For each original dataset we generated 100 multisets with positive labels, 100 with negative labels and 100 with neutral labels. We did so by a trial and error procedure - that is by generating the multisets at random until we found the desired numbers of multisets of the various classes.\nThen, we created the Selecta dataset, hereafter denoted as . This was done by applying the filtering procedure described in the previous section. Specifically, we randomly generated new multisets following the same procedure used for constructing the datasets , , and . Each multiset was classified using , , and , retaining only those for which all three classifiers produced the same output. Note that the labels of the multisets in were assigned based on the unanimous predictions of , , and , rather than being computed by relying on the emotional labels of the symbols composing the multisets333This strategy mimics a real-world scenario in which the images Selecta dataset consists of are collected from the internet and labeled automatically.. As for the size of , it contains 600 multisets per class.\nThe total number of multisets in Selecta dataset is larger than that of the datasets used to train , , and , since one of the main goals of Selecta is the automatic generation of a large dataset starting from small ones.\nAs a last step we assessed the performance of the classifier based on Selecta (say, ), and compared them with the performance of , , and . We also built a new classifier obtained by merging the original datasets , and (in the following we indicate such a classifier as ). The tests were carried out by generating 10.000 multisets per class and classifying them with the 5 available classifiers. The simulations were repeated using multisets of different lengths: one with 13, 14, and 15 symbols; another with 15, 16, and 17; a third with 19, 20, and 21; and finally one with 29, 30, and 31 symbols.\nThe results obtained by running 20 complete simulations and averaging the outcomes are reported in Table II.\nThe simulations show that the classifier based on Selecta performs similarly to the baseline classifiers, but performs slightly worse than a classifier based on their union. The overall performance of all classifiers in this scenario remains low. A possible explanation for these poor performance is that the random association of symbols to multisets does not reflect the way images are actually created in real life, where visual elements are not combined randomly but follow compositional mechanisms related to meaning-making. Moreover, in a real-world scenario, it is very likely that Selecta includes new elements that were not present in the original datasets. Indeed, in our theoretical assumptions, the main advantage of Selecta lies in its ability to exploit the mechanisms of isotopy to include more varied isotopic combinations within the augmented dataset. For these reasons, we carried out a second simulation, with the aim of modeling a scenario that takes into account the principles governing the construction of visual content.\nIV-B Second simulation\nThe second simulation was designed to highlight how the presence of isotopies permits to bring new knowledge (in terms of patterns that were not present in the original dataset) into the Selecta dataset.\nTo start with, now some symbols may appear in the multisets used to construct that were not present in , , or . Moreover, the final test set includes additional symbols that are absent not only from , , and , but also from . This setup mirrors real-world scenarios in which a classifier may encounter previously unseen visual lexemes. Specifically, we randomly partitioned the alphabet into three groups: 40% of the symbols were used to construct the original datasets , , and ; an additional 30% was used to build Selecta; and the test set encompassed the entire set of available symbols.\nA second modification was designed to introduce a simplified version of the isotopy mechanism, whereby the creators of visual content tend to include elements that align with the intended emotional polarity of the image [barthes1964][floch2000visual].\nTo do so, if a multiset already contains emotionally concordant symbols, additional symbols with the same emotional polarity are more likely to be added. Specifically, during the random construction of the multisets, the polarity of the first 10 symbols is checked, using the same method applied to the full multisets. At this point, the multiset is further completed by adding two elements with the same detected polarity to the multiset, while the remaining elements are sampled as usual. This mechanism serves as a minimal approximation of a visual creator‚Äôs tendency toward emotional coherence. All the rest remained as in the first simulation.\nThe results of the second group of simulations are shown in Table III. Each simulation was repeated twenty times, and the reported values represent the average across all 20 runs.\nIn the new setting, the presence of the isotopy mechanism leads to improved performance across all classifiers. Most notably, Selecta now significantly outperforms both the individual base classifiers and the classifier based on the union of the three original datasets. Simulation results also show that the superior performance of Selecta is not due to the size of the dataset. In the first simulation, in fact, Selecta‚Äôs performance is lower than that obtained by relying on the union of the three initial datasets. In the second simulation, however, performance is much better, despite the size of the dataset being the same. As discussed later, the good performance instead appear to be related to the number of combinations contained in Selecta dataset and to the more balanced distribution of patterns, which is particularly good in the second simulation.\nThe simulations allow us to verify the hypotheses behind our methodology, which are difficult to validate on real images due to the complexity of real data analysis.\nFirst, we hypothesized that the construction of Selecta would enable the dataset to capture numerous emotionally determinant combinations (Pattern A). In the simulations, Pattern A corresponds to symbol combinations that define the emotional content of an image - achieving an absolute majority for neutral images, or a relative majority for positive and negative ones. Every correctly classified image must contain at least one such combination, and may include several. All subsets of concordant elements sufficient to establish the majority are considered Pattern A. Table IV reports the number of these combinations for each classifier - that is, the emotionally relevant and sufficient combinations found among images correctly assigned to the training classes. The results in Table IV refer to strings of length 13, 14, and 15 symbols.\nThe results show that Selecta generates a dataset with a significantly larger number of pattern A. In fact, the dataset produced with Selecta is twice the size of the three datasets combined, and the number of pattern A actually observed is considerably higher, even when normalized by dataset size. This difference is more pronounced in simulation 2 when the mechanism of isotopy is simulated more precisely.\nWe also argued that Selecta should promote a more balanced distribution of Pattern B across all the classes, to prevent that their presence introduces a classification bias. In our simulation we identify type B patterns as triplets and quartets of symbols belonging to the same emotional class. In fact, triplets and quadruplets have an impact on classification equivalent to that of pattern B. Although they are not sufficient on their own to determine the emotion of an image, in some cases they are long enough to influence the classification based on IoU and may introduce classification errors and bias. For this reason, we verified whether the dataset constructed with Selecta favors the distribution of these patterns more evenly across all classes. Table IV reports the percentage of triplets and quadruplets present in all classes. Specifically, the table shows the percentage of concordant triplets and quadruplets shared across all three classes, with respect to the total number of all concordant triplets and quadruplets present in the multi-sets of the training set. The results show that, in the Selecta dataset, a higher proportion of the same triplets and, to a lesser extent, quadruplets appear in all three classes rather than in only one or two, confirming a more balanced overall distribution of pattern B.\nFinally, comparing the first and the second simulation, we can conclude that the good performances of Selecta are indeed linked to the exploitation of the semiotic mechanism. In fact, in the second simulation, the performance of the classifiers based on Selecta are significantly superior to both those of the individual classifier and those of the classifier based on the union of the original datasets, while this was not the case in the first simulation.\nV Creation of an isotopy-based dataset\nTo definitively validate the Selecta methodology, we applied it to the creation of a large tripolar image dataset, starting from three smaller datasets. We then trained a VSA classifier on the new dataset and verified that it outperforms the classifiers trained individually on each of the three original datasets, as well as a classifier trained on their union. Our experiments were conducted using three existing VSA datasets: VippSent [VippSent_2025_WACV], Flickr [katsurai2016], and Instagram [katsurai2016], each organized into positive, negative, and neutral classes.\nWe trained three architectures on each of the three datasets: a CLIP Image Encoder [radford2021clip] followed by a multilayer perceptron (MLP), ResNeXt101 32x8d [xie2017resnext] pretrained on ImageNet, and Swin-B [liu2021swin] pretrained on ImageNet-22k. Among these, we selected CLIP+MLP for subsequent experiments, as it achieved superior performance across all datasets (see Table V). The three CLIP-based models, each trained on one of the original datasets, were finally used as filters to select images based on the emotions they evoke.\nTo build the Selecta dataset, we downloaded two million images from the ImageNet [imagenet], COCO [cocolin2014microsoft], and OpenImages [openimages2020open] datasets, which were used as the image pool to be filtered. These datasets include a wide variety of images, including social media content, artistic imagery, and journalistic photography, allowing us to cover a broad spectrum of visual cases. We then labeled the images using the three base models. Regarding the consensus strategy, only the images classified in the same way by the three models were retained and labeled accordingly (3-out-of-3 consensus). The result was a large dataset of approximately 400,000 images. Images in the positive class have a high probability of containing positive visual isotopies, those in the negative class are likely to contain negative visual isotopies, while those in the neutral class are characterized by the absence of emotionally salient visual patterns. In this way, we built a very rich dataset, maintaining label reliability and introducing new information through the principle of isotopy. Figure 2 shows three images from the Selecta dataset that contain combinations of elements not present in the three original datasets. The absence of these combinations in the original datasets was verified using the YOLO object-detection software [yolo8improved_yolov8_small_objects]. The examples show the images of two elements, one often associated with negative emotions, a knife, and one usually associated with positive emotions, a stuffed animal. These two elements are distributed across all three classes in Selecta, reducing the likelihood of potential biases.\nFinally, we compared the performance of the models trained on the new dataset with those trained on the three original datasets and on their union. All tests were conducted on datasets used in psychological studies, due to the high reliability of their annotations.\nFor training, the datasets were split into training, validation, and test sets using a 70:15:15 ratio. To increase variability during training, we applied data augmentation techniques such as JPEG compression with random quality between 60 and 95, and horizontal and vertical flips, while avoiding color transformations in order to preserve the sentiment conveyed by the images. The models were trained for up to 100 epochs, selecting the best-performing version. We used the Adam optimizer with an initial learning rate of 0.0001, adjusted using a linear scheduler, and a batch size of 64. Images were resized to 336 336. These settings were maintained for all the experiments, except for those performed on networks other than CLIP, for which the size was set to 224 224.\nVI Experimental validation\nIn this section, we compare the performance of models trained on Selecta with those trained on the original datasets and their union. We evaluate the performance on both tripolar and bipolar datasets.\nVI-A Comparison with base classifiers\nFor our tests, we trained three models: CLIP [radford2021clip] with an additional MLP classifier, ResNet-101 [he2016resnet], and Swin-T [liu2021swin]. All models were trained both on Selecta and on the three original datasets (VippSent, Flickr, and Instagram).\nCross testing was conducted on eight of the most widely used datasets in the literature: four psychological datasets with a neutral class (IAPS [Iaps], GAPED [gaped], OASIS [OasisKurdiLozanoBanaji2017], NAPS [NapsMarchewkaZurawskiJednorogGrabowska2014]) and four VSA benchmarks including only two classes (FI [FIYou2016BuildingAL], Emotion6 [Emotion6], Emotic [emotic], VSO [VSOBorth2013]). Differently from the comparison shown in Table I, in this experiment on the two-class datasets we applied the models without any modification, leaving the neutral class empty in the test with the expected outcome that no images would be assigned to it. This allowed us to also assess the effectiveness of the models in the classification of neutral images, even in the two-class test (positive and negative).\nAs it can be seen from Table V, the model trained on Selecta consistently outperforms the other models, with gains ranging from 1% to 8%. In the table, the Own Test Set row reports the performance obtained on the test set of the dataset used for training, while the other rows report cross-testing results. The table reports results obtained by training the models on three classes. For tests on 2-class datasets, images assigned to the neutral class are considered as errors.\nWe further compared the results of models trained on Selecta with those trained on the union of the Flickr, VippSent, and Instagram datasets. The goal is to verify whether, even when merging the three original datasets, Selecta is able to maintain superior generalization capabilities. Similarly, we trained three architectures (CLIP, Swin Transformer, and ResNet-101) both on Selecta and on the union of the original datasets, comparing their performance on the main Visual Sentiment Analysis datasets, with either two classes (positive, negative) or three classes (positive, negative, neutral). Since the models were designed for three classes, in this case as well an empty neutral class was added in the tests on two-class datasets to enable evaluation.\nThe results we obtained, shown in Table V, indicate that Selecta consistently achieves better performance across all test sets, in line with the trends predicted by the simulations.\nVI-B Comparison of results across different emotional classes\nAs a further experiment, we compared the performance of classifiers trained on Selecta with those trained on the union of the three original datasets across the three emotional classes. We focused on CLIP models, given their significantly superior performance. These models were tested on IAPS and GAPED, selected for the reliability of their labels, with the aim of assessing how the improvements obtained with Selecta are distributed across the different emotional classes.\nThe results in Table VI show a clear improvement in the neutral class. This observation is consistent both with the predictions of the isotopy mechanism theorized by semiotics and with the considerations outlined in the methodology. Positive and negative images, in fact, contain recurring visual elements that enable good performance in both cases. Neutrality, on the other hand, is characterized by more heterogeneous elements and by a lower recurrence of typical visual patterns, since it emerges from the absence of emotional isotopies. For this reason, the neutral class is more difficult to classify: in fact, the lack of typical visual forms increases the likelihood of biases caused by Patterns C and B.\nThe model trained on Selecta, having been exposed to a wider variety of isotopic combinations and to a better distribution of Pattern B across the three classes, proves to be more resistant to these classification biases. For this reason, it distinguishes the neutral class better than models trained on the original datasets.\nVI-C Two-class experiments\nAs a further validation, we compared Selecta with Emoset (containing only positive and negative images), to verify whether Selecta‚Äôs generalization capabilities are confirmed even in the absence of the neutral class. To this end, we trained the models both on Selecta and Emoset by excluding the neutral class. As before, we trained three architectures: CLIP+MLP, Swin-B, and ResNet-101. Tests were carried out on the positive and negative images of the same datasets used in the previous experiments.\nThe results we got are shown in Table VII. Even in this case, models trained on Selecta achieve superior performance across all test sets, further proving the validity of our approach.\nVI-D 3-out-of-3 vs 2-out-of-[ADDRESS_REMOVED] we run aimed at comparing two different consensus strategies: a more selective one, based on full agreement among the three base algorithms, and a looser one, for which an agreement is reached when at least two out of three based classifiers give the same result. In the first case, the selection process filtered 400,000 images, 600,000 in the second case.\nUpon inspection of the results in Table VIII, we see that despite the smaller number of images, the dataset built on a stricter consensus strategy achieves better performance across all four psychological datasets. This result suggests that the selection of images based on isotopy mechanisms has a greater impact on model performance than dataset size alone.\nVII Conclusion\nWe proposed a new methodology for constructing a VSA dataset with improved generalization capabilities. The methodology is grounded on the analysis of the typical challenges in VSA, particularly the difficulty of classifying images according to abstract labels such as emotions, which lack univocal visual correspondences. To address this issue, we drew on concepts from semiotics, including isotopy and emotional signification, and introduced a dataset design strategy enriched with emotionally relevant visual combinations. This research shows that the theoretical tools developed within the field of visual semiotics can be employed to improve performance in image classification, particularly in complex tasks such as emotion classification. Moreover, it demonstrates that a careful and targeted data selection procedure in VSA, in our case backed by semiotic principles, proved to be a key factor in significantly enhancing model performance, as also highlighted in other areas of visual computing [geirhos2019imagenet].\nVII-A Limitations and Future Work\nA limitation of the present work concerns the use of only three emotional classes: positive, negative, and neutral. This choice was adopted for both practical and theoretical reasons.\nFrom a practical perspective, there is a need to include a neutral class, which is a fundamental element for real-world VSA applications, in order to avoid forcing images that do not evoke emotions into an emotional category, and a sufficiently broad and accurate neutral class is available only in three-class datasets. A further constraint is represented by the availability of reference datasets: to apply our methodology, at least three reliable datasets classified according to the same emotional classes are required. Datasets with a larger number of categories are often too small or based on heterogeneous annotation schemes, such as Ekman‚Äôs six basic emotions [Ekman1992], Mikels‚Äô model [mikels2005emotional], or other classifications.\nFrom a theoretical perspective, classifications with a larger number of categories are generally limited to the basic emotions of Ekman and Mikels, while studies in psychology show that images can also evoke more complex feelings, such as jealousy or nostalgia, which remain excluded from such schemes [Ekman1992, mikels2005emotional, panksepp1998affective, Damasio2003, Barrett2017ConstructedEmotion]. The three-class structure, already adopted in psychology [Russell1980], better reflects this complexity by including both basic emotions and more articulated feelings. Moreover, it aligns with the categories used in semiotics to analyze the affective aspects of a text: euphoria (positive), dysphoria (negative), and aphoria (neutral)[greimas1993semiotics]. Having said that, however, the Selecta methodology can also be applied to datasets with different emotional classifications, such as Ekman‚Äôs and Mikels‚Äô, provided that a sufficient number of reliable datasets based on the same type of emotional classes are available.\nAnother promising direction for future research concerns the analysis of multimodal content, particularly the integrated composition of images and text. Extending the principle of isotopy to both modalities could enable the application of Selecta to textual content analysis as well.\nFinally, there is a broader limitation that affects the entire VSA field. Emotional responses to images are shaped by individual, cultural, and contextual factors. Moreover, many images are inherently ambiguous or may acquire different emotional meanings depending on the context in which they are viewed [barthes1964]. Consequently, visual sentiment analysis typically seeks to identify the emotion perceived by the majority of observers, while acknowledging that such responses are not universal. A potential direction for future work would be the development of a dataset that explicitly includes a class for emotionally ambiguous images."
  },
  {
    "article": "Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models\nAbstract\nVision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations.\nIn this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves 2.4 speedup and 3.3 reduction in energy, significantly outperforming state-of-the-art accelerator in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at [URL_REMOVED]\nI Introduction\nVision-Language Models (VLMs) [li2024llava, liu2023llava] have emerged as a cornerstone of multimodal AI, enabling joint reasoning over visual and textual data. By integrating advances from computer vision and natural language processing, VLMs excel at tasks such as video captioning [yang2023vid2seq, zhang2024video], visual question answering [sinha2024guiding_vlm_sel_for_vqa, das2025vlm_continual_vqa], and cross-modal retrieval [li2022blip]. Following a similar trajectory to Large Language Models (LLMs) [brown2020language, devlin2018bert], modern VLMs have rapidly scaled in size and data, resulting in notable accuracy gains. However, this scaling significantly increases compute and memory demands, posing challenges for deployment, especially on edge devices [sharshar2025vision].\nFortunately, video-based inputs offer a key opportunity: high visual redundancy [dynamicllava, voco, prumerge, fastv, wang2025corematching]. As shown in Fig. 1(a), adjacent frames often share similar backgrounds and foreground objects. Since VLMs tokenize each frame independently [li2024llava, zhang2024video], many tokens across or within frames are redundant. This has motivated techniques such as token pruning [sah2024token_pruning_vit, wang2025corematching] and token merging [bolya2023token] to reduce computation. However, most prior work focuses on algorithmic strategies without considering hardware alignment. For instance, Token Merging [bolya2023token] introduces a ToMe module that increases runtime by up to 36.8% [yoo2024adaptiv].\nRecent designs such as AdapTiV [yoo2024adaptiv] and CMC [song2024cmc] address these inefficiencies at the hardware level. AdapTiV implements a simplified ToMe module in hardware, while CMC leverages video-codec-inspired compression (e.g., H.264 [wiegand2003overview]) via an external codec block. However, both approaches largely translate existing algorithms without embracing full hardware-algorithm co-design. First, both targeted for Vision Transformers (ViTs) [dosovitskiy2020image], focus only on visual redundancy and overlook the cross-modal nature of VLMs. CMC‚Äôs codec ignores language inputs, and AdapTiV only supports static images, missing video-language interactions. Second, both operate at global token-level granularity, which is inefficient for both algorithm and hardware due to high overhead and poor locality. To enable efficient VLM deployment, a more holistic co-design approach is needed, one that leverages cross-modal redundancy while aligning with hardware-friendly processing granularity.\nIn this study, we propose a novel architecture, Focus, to accelerate VLM inference by performing streaming concentration, a multilevel compression technique that removes visual and cross-modal redundancy in a streaming-friendly, on-chip processing fashion.\nFrom the algorithmic perspective, Focus performs redundancy concentration at three levels of granularity. First, it leverages semantic understanding to retain only visual regions relevant to the textual prompt. Prior work [rao2021dynamicvit, bolya2023token, song2024cmc, yoo2024adaptiv] relies on static metrics like token magnitude, which fail to capture prompt-conditioned semantics in VLMs. As shown in Fig. 1(a), attention may shift from a foreground object (e.g., a dog) to a background element (e.g., a flower), depending on the question (see details in Sec. III-A). To address this, Focus introduces a prompt-aware importance analyzer that dynamically prunes visual tokens based on cross-modal attention, improving both accuracy and efficiency.\nSecond, as illustrated in Fig. 1(b), Focus groups retained tokens into spatiotemporal blocks, using the last token (e.g., token ) as the key for localized similarity comparisons. The key token is compared with others in its block. This is applied across the video, treating each token in turn as a key. This technique resembles a 3D convolutional sweep that progressively concentrates similarity through localized matching. By operating within small spatial-temporal windows, Focus avoids global comparisons, making the process compute-efficient and highly streamable.\nThird, Focus explores redundancy at the vector level. Due to video motion, a token may align with multiple shifted tokens in adjacent frames. As shown in Fig. 1(c), token may share features with parts of tokens and . Relying on a single best-matched token may lose information. Instead, Focus performs vector-wise comparisons, allowing each vector to match multiple candidates and capture richer sub-token similarity. By integrating these three levels, Focus achieves up to 83% (80% on average) computational sparsity through multilevel concentration, significantly outperforming CMC and AdapTiV, which typically reach only 40‚Äì50%, under similar accuracy.\nFrom the architectural perspective, Focus is designed to efficiently support multilevel concentration through tight alignment with General Matrix Multiplication (GEMM) tiling. As shown in Fig. 1(d), its vector- and block-level operations naturally align with the tiling strategies widely adopted in systolic-array‚Äìbased accelerators such as TPUs [jouppi2023tpuv4] and GPUs [choquette2021nvidia]. GEMM tiling addresses on-chip memory constraints by dividing matrices into small, independently processed tiles. Each tile is handled by the PE array in isolation, enabling efficient, in-place vector-level similarity detection and compression. By eliminating redundancy locally within each tile, Focus minimizes data movement and reduces both compute cost and DRAM traffic. In contrast to global token-wise methods that rely on costly off-chip access, Focus achieves fine-grained, on-chip processing in a hardware-efficient manner.\nAt the block level, Focus draws inspiration from CNN accelerators [chen2016eyeriss, du2015shidiannao], using a sliding window to stream and process output tokens directly from the compute core (e.g., systolic array), maximizing locality and sustaining high throughput. To handle the non-contiguous nature of VLM tokens within a block, we adopt a convolution-style layout that preserves streaming flow while ensuring alignment for block-wise matching. At the semantic-level, which corresponds to the token level, Focus integrates into the attention layer to identify and retain the most relevant tokens based on cross-modal attention scores. Through dedicated scheduling, it performs token selection in a streaming fashion, enabling compression prior to memory write-back without stalling GEMM execution.\nFocus operates as a standalone module, similar to pooling or activation, without interfering with the core computation pipeline. Its modularity enables broad applicability and scalability while maintaining high compression efficiency. By co-optimizing the algorithm and architecture, Focus achieves up to 5.0 reduction in computation and 4.5 reduction in memory footprint for VLM inference. Occupying only 2.7% of the systolic array area, it is lightweight and well-suited for edge deployment. Our contributions are as follows:\n-\n‚Ä¢\nWe propose multilevel concentration, a hardware-oriented redundancy removal paradigm that eliminates semantic-, block-, and vector-level redundancy in VLMs.\n-\n‚Ä¢\nWe develop a co-designed streaming concentration architecture that aligns with tiling-based execution and memory access patterns with minimal hardware overhead.\n-\n‚Ä¢\nTo the best of our knowledge, Focus is the first architecture tailored for VLMs, delivering 2.60/2.35 performance and 2.98/3.29 energy efficiency gains over AdapTiV and CMC, respectively.\nII Background\nII-A Vision-Language Models\nThe success of Large Language Models (LLMs), such as GPT [achiam2023gpt] and LLaMA [touvron2023llama], has driven remarkable progress across a broad range of applications. Building upon this foundation, Vision-Language Models (VLMs) extend the capabilities of LLMs to multimodal inputs, enabling joint reasoning over visual and textual information. This multimodal capability significantly broadens their utility in tasks such as video captioning [maeda2024vlm_based_caption_eval], visual question answering (VQA) [das2025vlm_continual_vqa, sinha2024guiding_vlm_sel_for_vqa], and interactive multimodal assistants [liu2023llava, bordes2024introduction]. With superior adaptability and generalization in open-world visual scenarios, VLMs are emerging as a transformative technology with far-reaching impact in both academic [alayrac2022flamingo, lin2024vila, girdhar2023imagebind] and industrial [achiam2023gpt, team2024gemini, aws2023titan_multimodal_embeddings] domains.\nModern VLMs consist of a vision encoder and a Large Language Model (LLM) that jointly process visual and textual inputs. In video-based VLMs, videos are sampled into frames, divided into patches, and tokenized by the vision encoder into embeddings, which are projected into the LLM‚Äôs word embedding space for multimodal fusion. These visual tokens are concatenated with text prompts and processed by the LLM to generate text outputs.\nThe LLM with Transformer [vaswani2017attention, brown2020language] model architecture dominates both model size and computation. For example, in LLaVA-OneVision-72B [li2024llava], it accounts for 99.35% of parameters and 98.98% of operations. Moreover, visual tokens typically make up 98%‚Äì99% of total inputs; in LLaVA-OneVision on the VideoMME dataset [fu2025video], each sample averages 6,272 visual tokens versus only 109 text tokens. Therefore, optimizing LLM efficiency is crucial for accelerating VLMs.\nII-B Efficiency Optimizations for VLM\nEfficient Algorithms. Video-based VLMs generate a large number of tokens, placing heavy demands on compute and memory. To mitigate this, various token pruning techniques have been proposed [dynamicllava, voco, fastv, liu2025keyframe, wang2025corematching]. For instance, Prumerge [prumerge] uses sparse attention scores between the class token and visual tokens to discard less important ones, while FrameFusion [fu2024framefusion] merges temporally redundant tokens across frames.\nThese methods show that only a small subset of tokens is needed to preserve performance. However, they often incur runtime overhead for importance estimation and produce irregular sparsity patterns that limit GPU utilization.\nHardware Accelerators. Dedicated VLM accelerators are still rare, though Vision Transformer (ViT) accelerators [song2024cmc, yoo2024adaptiv, dong2023heatvit] offer transferable insights. AdapTiV [yoo2024adaptiv] merges nearby tokens using lightweight similarity checks based on sign bits, while CMC [song2024cmc] leverages video codec hardware to detect inter-frame redundancy. These designs offload token selection to specialized logic, reducing overhead and enabling efficient sparsity utilization, but are limited to coarse-grained, token-level pruning.\nIn contrast, Focus captures both coarse- and fine-grained redundancy through a multi-level concentration strategy. This broadens the scope of efficiency gains and enables hardware-friendly sparsity, as detailed in the following sections.\nIII Motivation\nThis section presents our motivation from both algorithmic and architectural perspectives across three levels:\n(1) Token (semantic) level prunes irrelevant tokens based on language context through semantic concentration;\n(2) Block level (similarity scope) detects local spatiotemporal redundancy within adjacent regions using block-wise comparison;\n(3) Vector level (similarity granularity) captures fine-grained sub-token redundancy via vector-wise similarity.\nIII-A Algorithm: Multilevel Concentration\nSemantic Attention Shifts with the Prompt. In Vision-Language Models (VLMs), token importance is inherently tied to the input prompt. Prior pruning methods often rely on static heuristics such as saliency or token magnitude, which fail to capture prompt-specific semantic intent.\nTo illustrate this, we extract cross-modal attention maps averaged from all layers of the Llava-Onevision-7B [li2024llava] model under two different prompts, as shown in Fig. 2(a). When asked ‚ÄúWhat is the type of the dog?‚Äù, attention concentrates on the dog; when asked ‚ÄúWhat is the color of the flower?‚Äù, attention shifts to the lower-left corner where the flowers reside. These examples highlight that semantically relevant tokens vary greatly with the question, and static importance metrics are inadequate.\nOur semantic concentration module leverages cross-modal attention to prune uninformative tokens early, improving efficiency without degrading accuracy.\nFine-grained Granularity Enhances Redundancy Detection. Global token-level matching is often too coarse to capture redundancy arising from motion, deformation, or soft spatial shifts. As illustrated in Fig. 1(c), a token in one frame may partially overlap with several neighboring tokens in the next frame, rendering single-token matching ineffective.\nTo better capture such partial alignments, we divide token embeddings into vectors and perform similarity comparisons at the vector level. We extract all layers‚Äô input from Llava-OneVision [li2024llava] model with the MLVU [zhou2025mlvu] dataset. Fig. 2(b) shows the average cosine similarity distribution across all layers, along with the variation range among layers. On average, over 64% of 8-dimensional vectors exceed a cosine similarity threshold of 0.9, compared to only 18% for 3584-dimensional vectors, indicating that finer granularity reveals substantially more redundancy. This enables higher sparsity without degrading accuracy. As shown in Fig. 2(c), our vector-level method achieves 82.8% sparsity on Llava-Video [zhang2024video] with the VideoMME [fu2025video] dataset, outperforming both CMC and AdapTiV, and exceeding our token-wise variant by 9.8%. This translates to a 1.6 reduction in computation, while slightly improving accuracy.\nBlock- and vector-level strategies are complementary: block granularity defines where comparisons are applied (e.g., within spatiotemporal windows), while vector granularity determines how fine those comparisons are conducted. Together, they yield structured sparsity that aligns naturally with GEMM tiling, enabling efficient and accurate compression in hardware.\nIII-B Architecture: Hardware-Oriented Design\nWhile many efforts aim to improve VLM efficiency [bolya2023token, fu2024framefusion, liu2025nvila], most focus on algorithmic techniques while overlooking hardware constraints. In high-throughput systems, algorithm and architecture must be co-designed, otherwise, even efficient algorithms can suffer from memory bottlenecks or poor data locality. As shown in Fig. 3, our design bridges this gap through a vector-wise compression strategy that improves both accuracy and system efficiency.\nLimitations of Global Token-Wise Methods. Prior designs like CMC [song2024cmc] adopt global, token-wise compression by offloading redundancy removal to a codec unit after writing full token outputs to DRAM (Fig. 3a). This incurs high bandwidth usage and sacrifices data locality. AdapTiV [yoo2024adaptiv] integrates token merging into hardware, but still relies on coarse token-pair operations and must transfer uncompressed tokens before processing. If prior designs were required to perform compression before writing back to DRAM, they would need an additional large buffer; for example, CMC uses up to 1.4MB. Token-wise methods also require full-token readiness before redundancy detection, limiting streaming.\nMoreover, these approaches overlook sub-token redundancy and operate at the full GEMM level, which misaligns with the execution model of systolic arrays that process small, regular GEMM tiles. Their global execution prevents fine-grained scheduling and increases memory pressure. As shown in Sec. VII-F, CMC achieves 46% sparsity but still incurs 79% of dense DRAM traffic, whereas Focus reaches 81% sparsity with only 21% of the bandwidth, highlighting the advantage of hardware-aligned, vector-level concentration.\nGEMM-Tile Friendly Compression. Focus performs compression entirely within each GEMM tile, aligning with the compute flow of systolic arrays. As shown in Fig. 3(b), vector-level similarity is computed immediately after generating each tile, using on-chip logic with no off-chip access. This tile-local design preserves output regularity, introduces structured sparsity, and minimizes control and data movement overhead, making it naturally hardware-efficient.\nWe further adopt a block-wise scheduling strategy using sliding windows. Each block is processed in-stream, enabling local reuse and eliminating the need for global buffering. Our conflict-free memory layout (Sec. VI-B) supports parallel compression units without access contention, allowing Focus to scale with tile throughput at negligible latency.\nIn summary, Focus demonstrates effective hardware-algorithm co-optimization. Our vector-wise design improves redundancy detection and model fidelity, while streaming and tile-local execution ensure high hardware efficiency. This tightly integrated architecture makes Focus scalable, practical, and deployable for real-world VLM applications.\nIV Focus Architecture Overview\nFocus introduces a modular Focus Unit to improve compute and memory efficiency in VLMs. As shown in Fig. 4, the Focus Unit is integrated near the memory interface of a standard systolic-array accelerator, intercepting data between compute stages without altering the core compute pipeline. The Focus Unit consists of two streaming submodules:\n-\n‚Ä¢\nSemantic Concentrator (SEC): Performs token-level pruning in attention layers based on cross-modal Attention scores.\n-\n‚Ä¢\nSimilarity Concentrator (SIC): Performs vector-level redundancy elimination in fully connected (FC) layers, aligned with GEMM tiling.\nSEC reduces the image token sequence length from to . It evaluates token importance using existing attention maps and prunes low-relevance tokens early in the pipeline. Pruned tokens remain excluded in downstream layers, yielding cumulative savings in computation and memory access.\nSIC further eliminates fine-grained redundancy among vectors within each GEMM tile. It compares incoming vectors in a convolution-style window and replaces similar ones with index references to shared representatives. This reduces the number of vectors processed per tile while preserving correctness via index-based reconstruction.\nBoth SEC and SIC operate entirely on-chip, support streaming dataflow, and dynamically adapt to data sparsity. By targeting complementary forms of redundancy from semantic and structural, Focus delivers efficient and scalable acceleration for Vision-Language Models. We detail the hardware implementation of SEC and SIC in Sections V and VI, respectively.\nV Semantic Concentrator\nThe Semantic Concentrator (SEC) enhances inference efficiency by selectively retaining semantically important visual tokens based on language context. It operates in the attention layers and consists of three tightly coordinated yet modular components, as shown in Fig. 5: The importance analyzer that estimates the importance of visual tokens based on cross-modal attention. A lightweight top- sorter that identifies the most important image tokens on the fly. An offset encoder that enables lossless index tracking for streaming token recovery.\nV-A Streaming Importance Analyzer\nThe SEC integrates directly into the attention computation pipeline. As shown in Fig. 5(1), for each attention head, it compute a matrix containing four blocks: image-to-image (), image-to-text, text-to-image (), and text-to-text. We extract the Text-to-Image block () as the cross-modal importance matrix , where and represent the number of image and text tokens, respectively. To estimate the importance of each image token over heads, we compute the maximum attention score it receives from any text token and all heads: . This results in an importance vector of shape across all heads. An on-chip buffer of 25 KB is used to store the importance vector.\nAs depicted in Fig. 5(2), the importance analyzer uses parallel max units to process the output of the attention SoftMax (provided by the special function unit). To match throughput, max units processes attention scores concurrently. This streaming design supports two dataflows: Parallel (spatial) stream: Attention columns are streamed directly into max units. Orthogonal (temporal) stream: Attention rows are buffered locally, enabling column-wise reduction.\nThis fully streaming design ensures minimal area and latency overhead. Since no global operations are needed, the analyzer is decoupled from the main compute path and incurs negligible runtime cost.\nV-B Top- Bubble Sorter\nOnce the importance vector is computed, the system must identify the top- most relevant tokens. To avoid sorting all tokens globally, SEC adopts a pipelined bubble sorter as shown in Fig. 5(4). By chaining the max units used earlier, we construct an -way streaming bubble sorter. This structure incrementally refines the top- tokens, allowing us to compute top- selection over the candidates in cycles, substantially more efficient than full sorting.\nCrucially, this process is fully overlapped with the computation of image attention (), which dominates the overall runtime. Let the image attention (within GEMM) require cycles, where is the head dimension, is the number of heads, and is the PE array size. The ratio of attention to the sorting operation is: In typical configurations, reaches into the thousands (e.g., 3584), while is much smaller (e.g., 32) and . Therefore, the sorting operation completes well before the finishes, ensuring that the SEC remains off the critical path and introduces no runtime bottleneck. A scheduling diagram is shown in Fig. 5 bottom to better understand the overlapping.\nV-C Semantic Pruning and Offset Encoding\nAfter identifying the top- most important tokens, we apply semantic pruning for the input of the subsequent attention operation, . As shown in Fig. 5(5), only the retained tokens are loaded and processed in , eliminating the need for any memory access or computation on pruned tokens.\nIn pure pruning mode, no additional metadata is needed. However, to support later stages (e.g., similarity concentration), we must record the position of retained tokens for spatial-temporal information. For this, the SEC generates localized offset encodings. The offset encoder, shown in Fig. 5(5), operates in a sliding window over the retained tokens. For each token, it records a small integer representing its offset to the previous token. This compact encoding is sufficient to restore positional alignment for future operations (e.g., similarity matching). The encoder‚Äôs computation is local and streaming, requiring only lightweight registers and no global memory access.\nOverall, the entire Semantic Concentrator, including the analyzer, sorter, and encoder, is fully modular and incurs minimal overhead. SEC selectively retains the most informative visual tokens using cross-modal attention, top- selection, and compact position encoding. It operates transparently within the attention, requires no additional global memory access, and introduces negligible runtime or area overhead.\nVI Similarity Concentrator\nAfter semantic-level token pruning, subsequent FC layers operate seamlessly on the reduced token set, as the pruning is token-aligned and preserves structural layout. However, semantic pruning alone does not address fine-grained redundancy at the vector level. In contrast, Similarity Concentration targets vector-wise redundancy by matching and merging similar output vectors within local regions across frames.\nDifferent from semantic pruning, where unimportant tokens are discarded, similar vectors often carry essential information and thus require accurate removal and later reconstruction. To support this, the similarity process includes two core components: Similarity Gather: removes similar vectors and constructs a compact output. Similarity Scatter: restores the original full layout using a similarity map.\nVI-A Similarity Gather\nIn this section, we first detail the design of Similarity Gather, which efficiently operates in a fully streaming fashion.\nGEMM Tiling. As shown in Fig. 6(1), Similarity Gather operates on the output of GEMM111Similarity Gather on output of FFN, O projection, and PV GEMM. Assume the input has dimension , the weight matrix is , and the output is . Due to limited on-chip resources, we adopt a tiling strategy widely used in modern accelerators [chen2016eyeriss, jouppi2017datacenter]. Specifically, the input and weight tiles are of size and , and the output tile is , where we typically set and . The PE array performs one output tile at a time, producing -length output vectors, where in our implementation. These vectors are then streamed to the Similarity Gather logic for processing when an output tile gets ready.\nBlock-wise Addressing. To exploit spatiotemporal redundancy, we adopt a convolution-style layout over two adjacent frames, as illustrated in Fig. 6(2). A sliding window spans both spatial and temporal dimensions with a stride of 1, forming a block that contains 8 vectors, 4 from each frame. Each element in the block is an -dimensional output vector produced by the GEMM operation, and the block serves as a localized comparison group for redundancy detection.\nTo efficiently support this structure, GEMM outputs are dynamically reorganized into the convolution-style layout using a dedicated reordering module, as detailed in Sec. VI-B. Within each block, the vector with the highest index (e.g., token ID 32) is selected as the key vector and compared against the other 7 vectors in the same block (e.g., token IDs 1 through 31) to identify potential redundancy.\nVector-wise Similarity Matching. As shown in Fig. 6(3), each key vector is streamed into the Similarity Matcher, which performs localized comparisons to determine whether the vector is redundant. We adopt cosine similarity to compare two vectors and of length 32:\nThanks to the regularity of the convolution-style layout, each token can precompute its L2-norm () and store it in a buffer. This allows the matcher to perform similarity comparisons using only a single dot-product unit and a small number of low-overhead element-wise operations. Furthermore, the vector-level granularity reduces the normalization length to 32, greatly simplifying the hardware design compared to token-wise similarity matching. In contrast, prior accelerators such as AdapTiV [yoo2024adaptiv] and CMC [song2024cmc] compute similarity at the token level, requiring expensive global memory access and full-sequence comparisons. By operating at the vector level within localized blocks, Focus achieves significantly lower matching overhead while maintaining semantic fidelity.\nIn practice, most of these operations are already supported by the Special Function Unit (SFU), which is commonly used for RMSNorm [zhang2019root] and SoftMax computations. Compared to these more complex operations, cosine similarity is lightweight and well-suited for hardware acceleration. Although the matcher could reuse existing SFU logic, for fairness, we implement it as a separate module and include its area and energy in our evaluation. The total overhead remains minimal, accounting for <1% of the systolic array design.\nIt is worth noting that similarity matching is not on the critical path of GEMM, as comparisons are performed only once per output tile. For a tile with vectors, each requiring 7 pairwise comparisons and 1 L2-norm computation (based on the block structure), the matcher needs at most cycles to process the tile. In contrast, GEMM requires cycles, where is the hidden dimension and is the number of PE rows. In our setup, with and , GEMM takes cycles per tile, far exceeding the cost of similarity matching. Only when does the matcher approach the critical path.\nTo address this corner case, we can scale the design by deploying multiple matcher units in parallel. Our convolution-style layout inherently supports conflict-free parallel access, allowing similarity matching to be fully overlapped with GEMM computation without introducing additional latency.\nSimilarity Collection. Once similarity matching completes, each vector has two outcomes: No match: The vector is unique and added to the concentrated output buffer. Match found: The vector matches a previously stored one (e.g., token 32 matches token 31), and we reuse the index of the matched token.\nTo support lossless reconstruction, we maintain a Similarity Map of size per tile. This map records, for each of the original output vectors, the index of its representative in the compact buffer. For instance, if token 32 matches token 31, we assign token 32 the index ‚Äú9‚Äù from token 31. After processing all vectors in a tile, only the deduplicated vectors and the similarity map are written back to DRAM. This significantly reduces memory bandwidth and storage.\nAll stages of this pipeline, including reordering, matching, and mapping, are performed on-chip, in a streaming fashion, without global synchronization or off-chip overhead. This localized similarity removal aligns naturally with GEMM tiling and preserves high data locality throughout execution.\nVI-B Convolution-style Layouter\nWe now describe the design of the convolution-style layouter, which addresses two key challenges in enabling efficient block-level similarity matching after semantic pruning: (1) recovering token positions and (2) avoiding memory access conflicts during parallel execution.\nChallenge 1: Recovering Token Positions after Pruning. Semantic pruning disrupts the spatial structure of tokens by removing unimportant entries, making it nontrivial to identify the 2D position of retained tokens in the original frame. To enable meaningful comparisons across adjacent frames, we must reconstruct each token‚Äôs (Frame, Height, Width) coordinate after pruning.\nAs shown in Fig. 7(1), we achieve this using the offset encoding generated during the semantic pruning stage (see Sec. V-C). This offset, streamed alongside the GEMM output, allows us to recover the original spatial location of each token. Tokens are then reorganized into a structured 3D tensor layout following the FHW (Frame‚ÄìHeight‚ÄìWidth) order to support localized block grouping.\nChallenge 2: Avoiding Memory Conflicts in Parallel Matching. To form a spatiotemporal block, vectors are drawn from multiple rows, columns, and frames. A naive layout may introduce bank conflicts or require data duplication across SRAM banks, an approach used by traditional CNN accelerators [chen2016eyeriss] but with significant memory overhead (up to 8 replication).\nTo eliminate these conflicts, we propose a conflict-free convolution-style layout, shown in Fig. 7(2), which deterministically maps each token to a unique bank and offset based on its FHW position. Given frame index , row , and column , the memory bank and address are computed as:\nwhere is the width of the frame. This mapping guarantees that all [ADDRESS_REMOVED] memory banks and can be read simultaneously without contention.\nKey Insight: Unlike traditional approaches that duplicate inputs to avoid access conflicts, our layout achieves fully parallel, conflict-free access without any data replication. This enables streaming similarity matchers to operate in parallel across tiles and spatial regions, scaling throughput without modifying the GEMM pipeline. The layouter thus plays a critical role in supporting parallel similarity execution and maintaining high utilization.\nVI-C Similarity Scatter\nGEMM Tiling for Concentrated Vectors. As shown in Fig. 8(1), Similarity Scatter operates on the concentrated vectors generated from earlier stages. Since only a subset of the original tokens is retained in each tile (), the input to this GEMM stage is logically sparse but structurally dense. To maintain compatibility with standard systolic-array architectures, GEMM is performed using a conventional tiling scheme with dimensions and . The GEMM execution follows a two-level nested loop structure: The outer loop adopts an output-stationary dataflow, keeping the output tile resident on-chip to accumulate results across the dimension. The inner loop follows a weight-stationary strategy, loading one weight sub-tile into the PE array while streaming in a sub-tile of concentrated input vectors.\nEach inner loop iteration computes partial products and generates one -dimensional partial sum vector per cycle. Our vector size 32 matches with the tile size and array height , ensuring full utilization of the PE array. These vectors are streamed out and accumulated over successive iterations to form the final tile result. The key advantage arises from the reduced number of active input vectors (), which significantly lowers the computational workload. However, since different sub-tiles may have different subsets of concentrated vectors, each possibly representing multiple original tokens, direct accumulation would produce incorrect outputs due to semantic aliasing.\nSimilarity Scatter and Gather. To resolve this, we introduce the Similarity Scatter module, illustrated in Fig. 8(2). After each GEMM step, the generated partial sums are streamed into a temporary buffer. Using the similarity map from previous layer‚Äôs the gather phase (see Sec. VI-A), each partial sum is replicated and redistributed to its associated original token indices, reconstructing the full output. This scattered output is then accumulated into an output-stationary buffer spanning all outer loop iterations. To maintain throughput parity, we employ a -wide accumulator (e.g., 64 when ), enabling concurrent accumulation of reconstructed vectors and streaming outputs. The reconstruction process is performed in-place, incurs negligible overhead, and does not require additional memory allocation.\nUpon completing all outer loop iterations, the fully accumulated output tile is passed to the Similarity Gather unit (see Sec. VI-A), shown in Fig. 8(3). This final stage is invoked only once per tile after GEMM concludes and lies entirely off the critical compute path.\nIn summary, by executing GEMM on a compact set of concentrated vectors, Focus achieves substantial compute savings. Through the Similarity Scatter module, it efficiently reconstructs full output tiles with minimal accuracy loss, and the final gather stage removes vector-level redundancy. This hardware-oriented, vector-granular compression strategy ensures high compute efficiency while preserving model fidelity. Our evaluation shows that the additional logic is lightweight and does not impact GEMM throughput, making it a key enabler of Focus‚Äôs performance advantage.\nVII Evaluation\nVII-A Methodology\nEvaluation Models and Datasets. We evaluate Focus using three representative VLMs with video understanding and reasoning capabilities: Llava-OneVision-7B (Llava-OV) [li2024llava], Llava-Video-7B (Llava-Vid)[zhang2024video], and MiniCPMV-2.6 (MiniCPM) [yao2024minicpm]. These models are tested on three widely adopted video understanding benchmarks: VideoMME (VMME) [fu2025video], MVBench (MVB) [li2024mvbench], and MLVU [zhou2025mlvu]. These datasets include diverse video types and durations, enabling a holistic evaluation of model capabilities across comprehension, temporal reasoning, and multimodal alignment. We use open-source models obtained from HuggingFace Transformers [wolf2020transformers] and perform evaluation via the lmms-eval [zhang2024lmmsevalrealitycheckevaluation] multimodal benchmarking framework to ensure consistency and fairness.\nBaselines. We compare Focus against two state-of-the-art architectures: AdapTiV [yoo2024adaptiv], a vision transformer accelerator, and CMC [song2024cmc], an accelerator optimized for video transformers. We extend their designs to make them compatible with VLMs. CMC performs inter-frame similarity checks, whereas AdapTiV focuses on intra-frame similarity detection; both exclude text tokens. We also compare with the vanilla systolic array [kung1979systolic] architecture for a base reference. In addition to hardware baselines, we also compare with FrameFusion [fu2024framefusion], a state-of-the-art token pruning algorithm tailored for efficient VLMs with video inputs.\nAlgorithm Implementation of Focus and Baselines. We implement the algorithm of our proposed Focus method in PyTorch [paszke2019pytorch]. For the baselines, we faithfully reproduce the token pruning algorithm from AdapTiV and CMC, carefully tuning their hyperparameters for application to VLMs. For FrameFusion, we adopt the official open-source implementation without modification. All algorithms are executed on an NVIDIA A100 GPU [nvidia2020a100] using FP16 precision for fair and consistent comparison.\nArchitecture Implementation of Focus and Baselines. Our Focus architecture setup is shown in Tbl. I. To evaluate architectural performance, we develop a cycle-accurate simulation framework based on SCALEsim-v2 [samajdar2018scale]. The simulator accepts layer-wise sparse traces generated from specific models and datasets in our PyTorch implementation, enabling precise modeling of cycles and memory access. We implement the Focus architecture in SystemVerilog and generate the on-chip SRAMs using the TSMC N28HPC+ Memory Compiler. The RTL is synthesized with a target clock period of 1.32 ns (757 MHz) under the worst-case slow‚Äìslow (SS) corner (0.81V, 125¬∞C), achieving [ADDRESS_REMOVED] negative slack (WNS) and providing a 34% timing margin for place-and-route at 500 MHz. The resulting area is reported from post-synthesis analysis, and the on-chip power is obtained from post-synthesis simulation using Synopsys Design Compiler. Off-chip DRAM energy is modeled with DRAMsim3 [li2020dramsim3] for device-level power. For a fair comparison, we also implement the core logic of all baseline accelerators in SystemVerilog and evaluate their area and energy using the same toolchain as Focus.\nVII-B Algorithmic Accuracy and Theoretical Sparsity\nTo evaluate the effectiveness of the multilevel concentration technique in Focus, we compare both model accuracy and the achieved computational sparsity against baseline methods. The computation sparsity is calculated through the ratio of the number of operations using the method to the number of operations required by the systolic array with original input. The results are presented in Tbl. II.\nFocus consistently achieves the highest accuracy across most evaluated scenarios, outperforming both software-only methods and hardware-based approaches. Compared to the original, uncompressed models, the average accuracy degradation with Focus is only 1.20%, demonstrating its ability to preserve semantic fidelity.\nIn addition to maintaining high accuracy, Focus also achieves the highest computational sparsity across all models and datasets. Specifically, Focus achieve sparsity of 80.19 on average, delivering 37.37% and 31.98% higher sparsity than AdapTiV and CMC, respectively, and outperforms FrameFusion by over 10.19% in sparsity.\nVII-C Performance and Energy Evaluation\nWe compare Focus against baseline methods, including the vanilla systolic array (SA), Adaptiv, and CMC, across multiple VLM models and datasets. The architectural setup for all baselines and Focus is detailed in Tbl. III. We maintain the same frequency, technology node, number of processing elements, operand bit width, and DRAM bandwidth across all designs. We further compare against the performance on an NVIDIA Jetson Orin Nano GPU [nvidia_jetson_orin_nano_2023], evaluated with and without the FrameFusion algorithm. The performance and energy results are presented in Fig. 9 (a) and (b).\nPerformance. Focus achieves significant performance improvements across all benchmarks. On average, it delivers a 4.47 speedup over the vanilla systolic array, which process dense input. This improvement stems from the ability of multi-level concentration to aggressively compress input tokens, getting 80.2% sparsity.\nCompared to AdapTiV, Focus achieves a 2.60 average speedup. While AdapTiV effectively detects and prunes nearby redundant visual tokens, it operates at a coarser granularity. In contrast, Focus performs vector-wise similarity removal, enabling finer-grained redundancy elimination.\nAgainst CMC, Focus achieves a 2.35 speedup. While CMC leverages external video codecs to perform wide-range redundancy search, this approach is often inefficient due to a high rate of mismatches. In contrast, Focus efficiently identifies sufficient redundancy within localized blocks using its on-chip similarity matcher.\nCompared with the GPU, our design achieves a speedup over the GPU and a speedup over the GPU running with FrameFusion. This improvement stems from our architecture‚Äôs ability to achieve higher computational utilization than the GPU. Moreover, Focus attains higher sparsity than FrameFusion due to its finer-grained redundancy removal, which is difficult to exploit on GPU Tensor Cores.\nEnergy Efficiency. As shown in Fig. 9(b), we report the total energy consumption of Focus and baseline designs, normalized to the vanilla systolic array (SA). The energy breakdown includes three components: on-chip core, on-chip buffer, and off-chip memory.\nCompared to SA, GPU, AdapTiV, CMC, and GPU with FrameFusion, Focus achieves average energy efficiency improvements of 4.67, 17.09 2.98, 3.29, and 5.13, respectively. These results highlight that Focus delivers significant savings across both computation and memory under constrained on-chip budget. This efficiency gain stems from our architecture‚Äôs ability to sparsify output of GEMM on-chip immediately after output tile generation, ensuring that all subsequent off-chip memory transactions operate on compressed data. A detailed analysis of memory access reduction is presented in Sec. VII-F.\nArea and Power Analysis. The area and power consumption of Focus and the baselines are also summarized in Tbl. III. The power statistics is derived on Llava-Video-7B with VideoMME dataset. Our Focus design occupies 3.21 mm2 of on-chip area and consumes 736 mW of power, both of which are lower than those of Adaptiv and CMC.\nFocus is smaller than CMC, as the external video codec used by CMC incurs substantial hardware overhead. Compared to Adaptiv, which adopts a lightweight similarity detection mechanism, Focus remains more efficient due to its streaming SEC that operates on localized input. Despite its enhanced functionality, Focus introduces only a 2.7% increase in area and a 0.9% increase in power consumption relative to the systolic array architecture. These results highlight the efficiency and low overhead of the Focus unit, which delivers significant performance benefits within a modest hardware budget.\nTo gain a deeper understanding of the overhead introduced by Focus, we present a detailed breakdown in Fig. 9(c). We observe that the proposed Semantic Concentrator and Similarity Concentrator are both highly lightweight, accounting for only 1.9% and 0.8% of the overall area, respectively. These two units also contribute negligibly to the overall power consumption. This demonstrates that SEC and SIC are well-suited for resource-constrained scenarios.\nOverall Insights. Beyond speedup and energy gains, Focus establishes a new paradigm for redundancy-aware VLM acceleration through tight algorithm‚Äìhardware co-design. At the token level, it performs on-the-fly Top- detection via streaming processing, handling sparsity in real time with minimal cost. At the block level, a block-wise sliding window propagates local similarity using only on-chip resources, reducing memory and buffer demand. At the vector level, Focus applies vector-wise similarity pruning with a gather‚Äìscatter scheme to control fine-grained irregular access and fully exploit sparsity. Together, these techniques translate algorithmic sparsity into tangible performance gains with minor hardware complexity.\nVII-D Design Space Exploration\nTo evaluate the impact of key architectural parameters in Focus, we conduct a comprehensive design space exploration. We focus on four primary factors, varying each individually while fixing the others to their default values to isolate their effects. Note that architectural parameters, other than the number of scatter accumulators, may also affect model accuracy. We evaluate accuracy under these variations and observe that the impact is generally negligible, allowing us to safely prioritize performance in our design exploration. All measurements are taken on the Llava-Video-7B model, using either the VideoMME or MLVU dataset.\nGEMM Tile Size. As shown in Fig. 10(a), we sweep the tile size from the full input height down to 32. As the tile size decreases, the end-to-end latency steadily increases. This trend arises because similarity gathering operates per tile. When a 2√ó2√ó2 block crosses tile boundaries, Focus only compares tokens within the same tile as the key token. For example, when the first token of a tile is the key, its neighbors outside the tile are unavailable for comparison. With smaller tile sizes (e.g., ), such boundary-crossing cases become more frequent, causing potentially similar vectors to be treated as distinct due to the limited comparison scope.\nWhile larger tiles offer better compression, they require more on-chip buffer to store intermediate results, increasing area and power consumption. We observe a trade-off between latency and buffer usage. From the latency‚Äìbuffer curve in Fig. 10(a), a tile size of 1024 emerges as an optimal design point. It incurs only 19% higher latency compared to the full-height tile while substantially reducing buffer requirements to a practical level.\nVector Size. Vector size determines the granularity of similarity concentration and directly impacts the sparsity and operation counts. To assess this, we measure the number of operations of a layer in two main components of Focus: (1) MAC operations in the main systolic array, and (2) accumulation operations in the outer accumulator during Similarity Scatter.\nAs shown in Fig. 10(b), reducing the vector size leads to fewer operations in the systolic array. This is because smaller vectors enable finer-grained similarity comparisons, allowing more aggressive redundancy removal and reducing the input size to the PE array. However, smaller vector sizes also increase the number of K-dimension iterations, requiring more frequent accumulation, which in turn raises the operation count in the accumulator.\nBeyond operation count, the systolic array dimension must be equal to or less than the vector size to utilize the benefits of fine-grained input. Taking both operational efficiency and hardware compatibility into account, we identify a vector size of 32 as an optimal design point, achieving strong compression while maintaining high utilization of the systolic array.\nSimilarity Concentrator Block Size. The block size used in the SIC directly impacts the spatial and temporal context available for similarity detection. We vary the block size along both the temporal (frame) and spatial (height and width) dimensions to examine its impact on performance. As shown in Fig. 10(c), the three-digit labels on the x-axis denote block sizes across these dimensions (e.g., 122 indicates f=1,h=2,w=2). We observe that enlarging the block size in either temporal or spatial dimensions reduces latency, as larger blocks provide broader context for similarity detection. Notably, extending the block size along the temporal dimension yields a more pronounced latency reduction compared to spatial extensions, which we attribute to the strong inter-frame similarity inherent in video inputs. We find that a block size of 2√ó2√ó2 is sufficient to provide strong performance.\nScatter Accumulator. The number of accumulators in similarity scatter affects throughput and pipeline efficiency. Ideally, accumulation should finish before the next output tile arrives from the systolic array. As shown in Fig. 10(d), using 64 accumulators achieves near-peak performance with only a 5% latency overhead compared to a larger 160-accumulator design, with diminishing returns beyond that point. This configuration also simplifies buffer design.\nSemantic Pruning Configuration. In our Semantic Pruning scheme, the value of ‚Äúk‚Äù in top-k pruning is determined by multiplying the original number of image tokens by a predefined retention ratio. We search multiple layer-wise retention configurations and select the one offering the best sparsity‚Äìaccuracy trade-off, which is adopted in our design. The final setup is summarized in Tbl. I, where pruning is applied to five selected layers whose retention ratios differ from the preceding layer. Future work may further enhance this strategy by dynamically adapting to input contexts, e.g., using a post-softmax attention threshold or top-p pruning [lin2025twilight], though such adaptation can introduce runtime variations across inputs.\nVII-E Ablation Study\nTo assess the contribution of each component in Focus, we perform an ablation study on Llava-Video-7B and report speedup, as shown in Fig. 11. We incrementally enable the SEC and SIC, comparing results against a dense systolic baseline and CMC [song2024cmc]. When only the SEC is enabled, Focus achieves a 3.15 speedup over the uncompressed systolic baseline and a 1.58 speedup over CMC. This demonstrates that semantic-aware pruning remove a large fraction of irrelevant visual tokens based on textual guidance, outperforming prior token-pruning strategies.\nEnabling the vector-wise SIC further boosts speedup by an additional 1.44. This highlights the ability of SIC to exploit residual redundancy among retained tokens at a finer vector granularity, beyond what semantic pruning alone can uncover. SEC and SIC together yield a 4.53 speedup over the dense baseline and 2.26 over CMC, confirming the effectiveness and efficiency of the Focus design.\nVII-F Memory Access\nWe analyze the off-chip memory traffic and average input matrix size of Focus compared to baseline designs. As shown in Fig. 12(a) and (b), Focus achieves the lowest DRAM access and input matrix size across all methods. Compared to the dense systolic array, we compress the input matrix by 5.6 and reduce memory traffic by 4.9.\nThis reduction stems from the joint effect of the SEC and SIC, which sparsifies the input at both token and vector levels. Additionally, the output of each FC layer is immediately compressed on-chip before being written to memory, so only compressed activations are transferred to DRAM, minimizing total memory access.\nCompared to both CMC and AdapTiV, Focus achieves significantly higher input compression and lower DRAM access: 3.0 and 2.2 higher compression ratios, and 3.7 and 2.2 DRAM traffic reduction, respectively.\nCMC relies on codec-based similarity detection over wide temporal windows and full-token representations, requiring large uncompressed regions to be staged in DRAM before processing. This leads to redundant memory transfers, as data must be written and read again for similarity detection. Similarly, AdapTiV performs local token pruning but still processes on whole-token granularity. By avoiding these limitations through lightweight, streaming-compatible similarity matching, Focus achieves superior memory efficiency with minimal overhead.\nVII-G Synergy with Quantization\nFocus is fully compatible with standard quantization techniques. We integrate Focus with INT8 quantization using bitsandbytes [dettmers20218], and the results in Tbl. IV show the impact on accuracy and sparsity compared to FP16. INT8 causes an average accuracy drop of 0.5% and a sparsity change of 0.13% relative to FP16. Although this loss is slightly higher than the 0.02% degradation in the dense model, it is reasonable since Focus and quantization jointly compress the model. Overall, the accuracy drop remains minor, and Focus effectively maintains its redundancy-removal capability under quantization, demonstrating strong synergy for efficient VLM inference.\nVIII Discussion\nVIII-A Generalization Ability of Focus\nWe further examine the generalization ability of Focus. Although Focus is originally designed for video-based VLMs, it can be directly extended to image-based VLMs by treating a single image as a one-frame video. While temporal similarity is no longer present in this setting, substantial semantic redundancy and spatial similarity remain. As shown in Tbl. V, evaluations on image-based VLMs [li2024llava, bai2025qwen2] across multiple datasets [goyal2017making, liu2024mmbench, zhang2021mme] demonstrate notable speedups over both the systolic-array baseline and AdapTiV, with only minor accuracy degradation. These results indicate that Focus effectively removes redundancy beyond the video domain.\nMoreover, Focus can potentially be extended to Vision‚ÄìLanguage‚ÄìAction (VLA) [kim2024openvla, intelligence2504pi0] models for embodied AI applications. VLA models share similar input modalities with VLMs, including image or video inputs paired with text. Therefore, we believe the SEC and SIC in Focus could effectively eliminate redundant information in VLA inputs, making this a promising direction for future exploration.\nVIII-B Worst- and Best-Case Analysis\nSince sparsity varies with video content, we analyze two extreme scenarios to verify robustness. In the worst case, when no similarity exists across frames or patches, sparsity drops near zero. The design preserves the full tile length (), with buffers sized for maximum data without overflow. In the best case, abundant similarity yields highly compressed tiles and small , slightly wasting buffer space and underutilizing the systolic array but remaining correct. We further aggregate the frequency of different tile lengths (i.e. number of vectors) and measure systolic-array utilization (Fig. 13). These extremes are rare, one increases latency, the other lowers utilization, but the system maintains an average utilization of 92.2%, confirming robust performance across diverse inputs.\nVIII-C Related Works\nAlgorithm Optimizations. As a major paradigm for multimodal reasoning, VLMs have attracted significant attention, resulting in a rapidly growing body of work on improving inference efficiency. A large class of recent methods focuses on exploiting redundancy in visual tokens to accelerate VLM inference [fu2024framefusion, fastv, huang2025prunevid, shen2025fastvid, liu2025keyframe, wang2025corematching, liu2025nvila]. FrameFusion [fu2024framefusion] compares and merges similar tokens across adjacent frames, while PruneVid [huang2025prunevid] performs token clustering and merges tokens within the same cluster. By removing or compressing redundant tokens through diverse algorithmic strategies, these approaches effectively reduce token counts and achieve notable speedups on GPUs.\nDespite their effectiveness, these methods operate exclusively at the token level and are implemented as software-only optimizations. They are primarily designed for execution on general-purpose GPUs and do not consider how redundancy manifests at finer granularities or how it can be efficiently exploited from a bottom-up hardware perspective.\nArchitecture Design. From the hardware architecture perspective, to the best of our knowledge, there is no dedicated accelerator specifically designed for VLM inference. Existing architectural efforts instead target efficiency optimizations for LLMs and ViTs, which share the transformer backbone with VLMs. These works predominantly rely on sparsity [wang2021spatten, dong2023heatvit, you2023vitcod, wang2025lad, guo2020accelerating, wei2025prosperity, wei2025phi] and quantization [guo2023olive, chen2025bitmod, guo2022ant, hu2025m, cheng2025ecco, guo2025transitive]. In terms of sparsity, SpAtten [wang2021spatten] introduces token- and head-level pruning for transformers, while LAD [wang2025lad] optimizes key‚Äìvalue cache pruning during the decoding phase of LLMs. HeatViT [dong2023heatvit] leverages attention maps in ViTs to prune visual tokens. In addition, several works propose hardware-friendly quantization architectures: Olive [guo2023olive] introduces an outlier‚Äìvictim pair format integrated into processing element (PE) arrays, and BitMoD [chen2025bitmod] enables fine-grained data-type adaptation through bit-serial processing.\nWhile these techniques can be applied to VLMs, they are not explicitly designed for multimodal workloads and therefore may fail to fully capture the unique redundancy patterns introduced by cross-modal interactions. In contrast, Focus is the first architecture specifically tailored for VLM inference. By exploiting cross-modal semantic redundancy and detecting fine-grained vector-level similarity, Focus enables efficient streaming execution and achieves superior hardware efficiency beyond token-level or modality-agnostic optimizations.\nIX Conclusion\nWe present Focus, a streaming concentration architecture that jointly optimizes algorithm and hardware for efficient VLM inference. Our Multilevel Concentration strategy removes redundancy at the semantic, block, and vector levels, while our hardware design performs in-place compression aligned with GEMM tiling and streaming execution. Focus achieves up to 2.35 speedup and 3.29 energy efficiency improvement over state-of-the-art baselines, with only 2.7% area overhead in a systolic-array accelerator. By tightly co-designing compression logic with accelerator architecture, Focus enables scalable, high-performance deployment of VLMs on both edge and cloud platforms, and paves the way for future hardware-aware multimodal systems.\nAcknowledgment\nThis work was supported in part by NSF-2112562, NSF-2328805, and ARO W911NF-23-2-0224. The authors sincerely thank the anonymous reviewers for their constructive feedback and valuable suggestions that greatly improved the quality of this work. The authors also express their gratitude to Jonathan Ku, Bowen Duan, Yiming Li, and Dr. Tingjun Chen for their technical support and insightful discussions.\nAppendix A Artifact Appendix\nA-A Abstract\nThis artifact provides a complete implementation of Focus, a streaming concentration architecture for efficient vision-language model (VLM) inference. The artifact includes three main components: (1) Algorithm implementation of Focus and baseline methods (CMC, Adaptiv, FrameFusion) for multiple VLMs, including LLaVA-Video, LLaVA-OneVision, MiniCPM-V, and Qwen2.5-VL; (2) Cycle-accurate hardware simulator with energy/power estimation and design space exploration capabilities; (3) RTL implementation in Verilog/SystemVerilog. The artifact enables the reproduction of all key results, including accuracy evaluations on the VideoMME, MLVU, MVBench, VQAv2, MME, and MMBench datasets, performance/energy simulations across various design configurations. Generated traces and simulation outputs can be used to reproduce all figures and tables in the evaluation section.\nA-B Artifact check-list (meta-information)\n-\n‚Ä¢\nAlgorithm: Focus multilevel concentration (semantic, block, vector levels), CMC, Adaptiv, FrameFusion baselines\n-\n‚Ä¢\nProgram: Python 3.11+ with PyTorch 2.6.0+\n-\n‚Ä¢\nCompilation: Python package installation via pip, third-party code compilation with g++\n-\n‚Ä¢\nModel: LLaVA-Video-7B-Qwen2, MiniCPM-V-2.6, LLaVa-OneVision-qwen2-7b-ov, Qwen2.5-VL-7B-Instruct\n-\n‚Ä¢\nDataset: VideoMME, MLVU, MVBench (video); VQAv2, MME, MMBench (image)\n-\n‚Ä¢\nRun-time environment: Ubuntu 22.04.2 LTS, CUDA 12.1, PyTorch 2.6.0, Conda environment, HuggingFace Hub access\n-\n‚Ä¢\nHardware: NVIDIA datacenter GPU (A100), multi-core CPU x86_64 processor\n-\n‚Ä¢\nExecution: Bash scripts for trace generation, Python scripts for simulation and evaluation, Jupyter notebooks for plotting\n-\n‚Ä¢\nMetrics: Model accuracy, sparsity ratio, latency (cycles), energy (mJ), power (W), area (mm¬≤)\n-\n‚Ä¢\nOutput: CSV files with accuracy/sparsity metrics, PyTorch trace files (.pth), simulation result CSVs, Jupyter notebooks for plotting\n-\n‚Ä¢\nExperiments: Trace generation, accuracy evaluation, hardware simulation, design space exploration\n-\n‚Ä¢\nHow much disk space required (approximately)?: 128GB (models + datasets + traces + codes)\n-\n‚Ä¢\nHow much time is needed to prepare workflow (approximately)?: 1 hour (installation + model download)\n-\n‚Ä¢\nHow much time is needed to complete experiments (approximately)?: 6 hours without accuracy evaluation, 480 hours with accuracy evaluation.\n-\n‚Ä¢\nPublicly available?: [URL_REMOVED]\n-\n‚Ä¢\nCode licenses (if publicly available)?: MIT License\n-\n‚Ä¢\nData licenses (if publicly available)?: The datasets are publicly available through their original licensing terms.\n-\n‚Ä¢\nWorkflow automation framework used?: Bash scripts, Python entry points, Jupyter notebooks\n-\n‚Ä¢\nArchived (provide DOI)?: [URL_REMOVED]\nA-C Description\nA-C1 How to access\nThe artifact is available as a Git repository at [URL_REMOVED] Clone the repository and initialize submodules\nA-C2 Hardware dependencies\n-\n‚Ä¢\nGPU: NVIDIA GPU with 80GB HBM (e.g. A100).\n-\n‚Ä¢\nCPU: x86_64 processor\n-\n‚Ä¢\nStorage: 128GB+ available disk space\nA-C3 Software dependencies\nThe experiments rely on the following software components.\n-\n‚Ä¢\nUbuntu 22.04+ (tested on Ubuntu 22.04 LTS)\n-\n‚Ä¢\nPython 3.11+\n-\n‚Ä¢\nPyTorch 2.6.0 with CUDA support\n-\n‚Ä¢\nTransformers 4.48.2 (or 4.49.0 for Qwen2.5-VL)\n-\n‚Ä¢\nAccelerate 0.29.1+\n-\n‚Ä¢\nFlash-attention 2.7.4.post1\n-\n‚Ä¢\ng++ compiler\n-\n‚Ä¢\nHuggingFace CLI and account (for model/dataset access)\nA-C4 Data sets\nVideoMME, MLVU, MVBench, VQAv2, MME, MMBench\nA-C5 Models\nThe artifact supports multiple pre-trained VLMs accessible via HuggingFace:\n-\n‚Ä¢\nLLaVA-Video-7B-Qwen2\n(lmms-lab/LLaVA-Video-7B-Qwen2) -\n‚Ä¢\nMiniCPM-V-2.6\n(openbmb/MiniCPM-V-2_6) -\n‚Ä¢\nLLaVA-OneVision\n(lmms-lab/llava-onevision-qwen2-7b-ov) -\n‚Ä¢\nQwen2.5-VL\n(Qwen/Qwen2.5-VL-7B-Instruct)\nA-D Installation\nWe have well-documented README files to detail the installation instructions for each experiment at [URL_REMOVED]\nA-E Experiment workflow\nThe README file also specifies the detailed experimental workflow for obtaining the results reported in the paper.\nA-F Evaluation and expected results\nComprehensive README files are provided to document the evaluation procedures for accelerator latency, energy, area, and model accuracy. Expected results can be found in the directories simulator/example_sim_results and algorithm/example_output.\nA-G Methodology\nSubmission, reviewing, and badging methodology:"
  },
  {
    "article": "gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation\nAbstract\n-\n‚Ä¢\nExisting libraries for generating synthetic data for training ML-based PF and OPF solvers do not integrate all state-of-the-art techniques for producing diverse load, topology, and generator scenarios (see Figure 1), which limits the performance of ML-based solvers.\n-\n‚Ä¢\nPF data generation is often restricted to OPF-feasible points, hindering generalization to cases that violate operating limits (e.g., with branch overloads or voltage violations).\n-\n‚Ä¢\nOPF datasets have low diversity due to fixed generator cost functions, limiting generalization across varying costs.\n-\n‚Ä¢\nWe introduce gridfm-datakit , a Python library unifying stochastic yet realistic load and topology perturbations, arbitrary changes, and scaling to large grids (10,000 buses) to generate PF and OPF datasets.\n-\n‚Ä¢\nBy combining global load scaling from real-world profiles with localized noise, it generates data with realism and diversity beyond prior libraries.\n-\n‚Ä¢\nWe compare gridfm-datakit ‚Äôs data distributions with those of OPFData, OPF-Learn, PGLearn, and PF in Section 5.\n-\n‚Ä¢\nIt is available on GitHub under the Apache 2.0 license and can be installed via pip install gridfm-datakit.\n1 Getting Started\ngridfm-datakit can be installed from PyPI and used interactively or via the command line. Full documentation is available here. Users are kindly requested to cite this technical paper when publishing work that uses gridfm-datakit .\nInteractive Interface.\nThe interactive interface guides the user through the different settings for data generation. It can be launched directly in a Jupyter Notebook:\nCommand-Line Interface.\nFor automated or large-scale runs, the data generation routine can be executed from the following bash command line using a configuration file:\n2 Motivations\nThe absence of standardized, realistic, and diverse datasets for electric grids hinders the development of machine learning (ML) methods for steady-state transmission grid analysis [pglearn], motivating the design of gridfm-datakit .\n-\n‚Ä¢\nBarrier to entry. Generating large datasets is computationally expensive. Solving millions of OPF problems can require hundreds of CPU hours111For instance, OPFData [lovett2024opfdatalargescaledatasetsac] was created by solving 600,000 OPF cases for a 10k-bus grid using PowerModels.jl [powermodels]; we measured a convergence time of 17 s per case, implying over 118 CPU-days of computation for this grid alone. As another example, generating the PGLearn dataset took 29,876.65 CPU-hours, i.e., about 1245 CPU-days! [pglearn].\n-\n‚Ä¢\nFocus on small grids. Most ML models have been evaluated only on networks with fewer than 1,000 buses [DONON[PHONE_REMOVED], varbella2024powergraph, LOPEZGARCIA[PHONE_REMOVED], matteothesis]. While recent efforts have scaled to larger systems [arowolo2025generalizationgraphneuralnetworks, piloto2024canosfastscalableneural, lovett2024opfdatalargescaledatasetsac, LIN[PHONE_REMOVED], matteothesis, pfdelta], they remain rare due to the high cost of solving OPF problems on large instances.\n-\n‚Ä¢\nLack of reproducibility and benchmarking. Most studies rely on custom, unpublished data pipelines with differing assumptions on loads, dispatch, and topology [DONON[PHONE_REMOVED], varbella2024powergraph, LOPEZGARCIA[PHONE_REMOVED]], hindering reproducibility and fair comparison. Limited incentives to generate diverse or complex scenarios also make it unclear whether reported gains arise from model design or data choices. gridfm-datakit provides a unified platform for producing larger datasets to support emerging standardized benchmarks for PF solvers, such as those in PF [pfdelta]222The authors welcome collaboration to extend [pfdelta] with gridfm-datakit datasets for larger grids and for OPF..\nAs a result, progress on critical challenges (such as transferability and generalization to unseen grid states), as discussed in a recent review paper [Khaloie], is, for now, limited.\n3 Limitations of Current Libraries\nExisting libraries show several limitations that reduce their usability:\n-\n‚Ä¢\nIncomplete perturbation modeling. State-of-the-art methods for generating diverse and realistic load, topology, generator dispatch, and admittance scenarios are scattered across more than ten Python and Julia libraries, and even the latest ones don‚Äôt include all of them, as shown in Figure 1. In particular, most datasets only consider contingencies, although real grids can see ten or more branches switch status per day.\n-\n‚Ä¢\nPF datasets are limited to points within operating limits. Most PF libraries only generate points that are feasible for the AC Optimal Power Flow (ACOPF) problem, excluding operating states that violate its inequality constraints (e.g., voltage magnitude or branch limits). This prevents using such datasets to train PF solvers that need to handle cases that violate these constraints. PF solves this issue by employing an OPF formulation that neglects operating limits, but this leads to clusters of points showing the same violations on a few buses, as discussed in Section 5.\n-\n‚Ä¢\nOPF datasets use fixed generator costs. OPF libraries (e.g., OPFData [lovett2024opfdatalargescaledatasetsac], PGLearn [pglearn], OPFLearn [opflearn]) use fixed generator cost coefficients, limiting the diversity of generator dispatch and the ability for models trained on these datasets to generalize across different cost conditions.\nWe additionally provide a detailed comparison of all existing libraries in Appendix A. In what follows, we show how gridfm-datakit overcomes these limitations to generate OPF data, as well as PF data that can include points violating the OPF inequality constraints.\n4 Design and workflow\n4.1 Grid, Size, and Scaling\ngridfm-datakit scales to networks with up to 30,000 buses for PF and 10,000 buses for OPF using PowerModels.jl [powermodels]. Grids can be imported from the MATPOWER format (.m) [matpower], and all grids from the PGLib dataset [pglib] are supported. This makes our library the most scalable Python-based solution for AC PF and OPF data generation. As shown in Table 1, data generation is fast: generating 200,000 PF samples for case24 and case118 took less than 10 minutes and 20 minutes, respectively. For OPF data, this required about 1 hour and 2 hours, respectively, using only 20 cores and 32 GB of RAM.\n4.[ADDRESS_REMOVED] (O)PF libraries and papers (e.g., Graph Neural Solver [DONON[PHONE_REMOVED]], OPFData [lovett2024opfdatalargescaledatasetsac], PowerFlowNet [LIN[PHONE_REMOVED]]) apply uncorrelated synthetic load perturbations, typically scaling each nominal bus load independently with uniform or Gaussian noise, leading to very little load diversity. PGLearn [pglearn] introduces a global scaling factor applied to all buses, with additional local bus-level noise, yet both are sampled from uniform distributions. PowerGraph [varbella2024powergraph] leverages real aggregated load profiles to derive time-dependent global scaling factors, but omits local noise, resulting in limited spatial diversity. Finally, OPF-Learn [opflearn] and PF [pfdelta] sample directly from the feasible load space, producing scenarios that may be unrealistically uncorrelated across scenarios and buses, and computationally expensive to generate for large networks333This sampling technique prevents PF and OPF-Learn from generating data for grids larger than 2,000 buses and from producing large datasets for 2,000-bus grids..\nWe introduce a hybrid load perturbation strategy that combines global scaling from real aggregated time series (e.g., aggregated load profiles obtained from the EIA [EIA_ERCOT_2025]) with local multiplicative noise at each bus. Let and denote the nominal active and reactive powers of load . At each time step , a global scaling factor is derived from an aggregated profile scaled into a feasible range 444 is obtained by incrementally increasing the nominal active and reactive power loads in 10% steps until OPF no longer converges. is then set to , where is a parameter defaulting to , as in PGLearn [pglearn].. The perturbed loads are then given by\nwhere introduce per-bus variation.\nThis approach jointly preserves spatial correlation and realism (via shared global scaling factors), captures temporal realism (via real load profiles), and ensures spatial diversity (via local noise), as shown in Figure 1.\n4.3 Topology and Admittance Perturbations\nWhile libraries are restricted to contingencies, i.e., single-line, transformer, or generator outages [pfdelta, opflearn, lovett2024opfdatalargescaledatasetsac], datakit supports arbitrary perturbations. This can be done in two different ways: by exhaustively enumerating all valid topologies with up to disconnected components (lines/transformers/generators), or by randomly sampling topologies by disabling up to components, while ensuring feasibility (i.e., no islanding).\nWe introduce admittance perturbations by randomly scaling the resistance and reactance of branches using scaling factors sampled from a uniform distribution in the range , where is a user-defined parameter.\n4.4 Generator Setpoints\nWe provide two data modes: one intended for generating datapoints for training OPF solvers (OPF mode), with cost-optimal dispatches that satisfy all operating limits (OPF-feasible), and one intended for training PF solvers (PF mode), which produces power flow data where one or more operating limits ‚Äì the inequality constraints defined in OPF, e.g., voltage magnitude or branch limits ‚Äì may be violated.\nOPF mode.\nGenerator setpoints are obtained by solving an ACOPF problem after topology perturbations. The resulting operating points satisfy all limits and are cost-optimal for the chosen generator cost coefficients.\nUnlike typical OPF data libraries that use fixed generator costs [opflearn, pglearn], gridfm-datakit increases dispatch diversity by permuting cost coefficients or applying user-defined random scaling factors before solving the OPF. This supports training models that generalize across different cost or market conditions.\nPF mode.\nSeveral strategies have been proposed in the literature to generate PF data that include points outside normal operating limits. PowerFlowNet [LIN[PHONE_REMOVED]], TypedGNN [LOPEZGARCIA[PHONE_REMOVED]], and Graph Neural Solver [DONON[PHONE_REMOVED]] directly sample voltage magnitudes and generator powers within their admissible ranges. However, this approach restricts scenarios to normal operating limits, prevents the use of realistic load scenarios (since the load is then set according to the sampled dispatch), and results in similar total system load across samples. PF [pfdelta] instead removes inequality constraints during OPF solving, but this causes most violations to cluster in a few locations, as shown in Figure 4.\nIn gridfm-datakit, generator setpoints are first computed on the base topology (with load and admittance perturbations) using ACOPF. After fixing these setpoints, topology perturbations are applied, and an AC power flow is solved to obtain the new system state. Because generator dispatch is not re-optimized after the topology change, some samples violate operating limits defined by the OPF inequality constraints, such as branch overloads, voltage limit violations, branch angle difference violations, reactive power bound violations, and active power bound violations at the slack bus.\nThese cases emerge naturally from realistic perturbations rather than from artificial constraint removal or direct sampling. This approach provides a balanced mix of points within and outside normal operating limits, reflecting realistic system behavior where OPF determines generator dispatch under nominal conditions and unexpected changes lead to violations.\n5 Data Diversity\nWe assess the diversity of datasets generated with gridfm-datakit against existing PF and OPF libraries555To ensure a fair comparison with other libraries, we remove at most one line, transformer, or generator in each sample (as in all other libraries except OPF-Learn). The parameter of the local load perturbations is set to 0.2 (as in PGLearn, and OPFData), and the range parameter of the global scaling factor is set to 0.4 (as in PGLearn). Generator costs are permuted as in PF, and the parameter of the admittance perturbation is set to 0.2 (a feature absent in other libraries).. Figure 2 shows, for each feature, the mean normalized Shannon entropy, an information-theoretic measure analogous to metric in [hedgeopf2025]. More details on the computation of this metric are provided in Appendix C.\nPower flow data.\nAmong recent and publicly available PF libraries, only PF is suitable for comparison, as it is the only one that includes out-of-operating-limit scenarios (contrary to, e.g., PowerFlowNet). gridfm-datakit is less diverse for loads (, ) because PF samples the feasible load space directly, whereas gridfm-datakit applies global load scaling from real profiles plus local noise, balancing spatial and temporal correlation with diversity. Uniform sampling yields high variability in power factors across buses and samples. In contrast, gridfm-datakit perturbs power factors only through local noise, while the global scaling jointly applied to active and reactive loads prevents large variations.\ngridfm-datakit exhibits slightly lower diversity than PF for ‚Äì expected since PF solves OPF without inequality constraints, which naturally leads to a higher proportion of samples outside normal operating limits. However, this higher diversity in PF comes at the cost of realism and balance between points within and outside nominal operating limits (e.g., more than 75% of samples violate reactive power limits at buses 69 and 84 in Figure 4).\nWe show in Figure 4 the mean normalized entropy for branch flow features and in Figure 5 the distribution of branch loadings for both datasets. Both show similar entropy, but gridfm-datakit and PF differ in the balance between out-of-limit and in-limit scenarios: in gridfm-datakit , of the branches are overloaded (loading 1) across all scenarios, and of the scenarios have at least one branch overloading. In PF, all scenarios have overloads (as all inequality constraints are removed), and of all branches are overloaded.\nOptimal power flow data.\nWe compare gridfm-datakit with OPFData, OPF-Learn and PGLearn. gridfm-datakit shows similar diversity to OPFData and PGLearn for load features (, ), though with more realistic load distributions due to the hybrid load perturbation strategy. OPF-Learn shows higher diversity for load, since it samples directly from the feasible load space (leading to the aforementioned issues of unrealistic power factor variations, and lack of spatial and temporal correlation). For all other features, gridfm-datakit demonstrates substantially greater diversity, primarily driven by the permutation of generator cost coefficients and the use of admittance perturbations, which are absent in other datasets. Notably, gridfm-datakit achieves a normalized mean entropy for that is 2.3 times higher than PGLearn and 2.8 times higher than OPFData, although all datasets still struggle to achieve high diversity for , which is the main decision variable in OPF.\n5.1 Outputs, Data Validation, and Benchmarking\nOutputs.\nBenchmarking.\nFast linear models such as DC-PF and DC-OPF often serve as baselines for neural PF/OPF solvers. To avoid re-running external solvers, gridfm-datakit computes DC-PF and DC-OPF for every generated sample and stores the solutions and the per-sample runtime directly in the dataset.\nData validation and statistics.\nThe CLI command gridfm_datakit validate path/to/dataset checks all grid-model assumptions, recomputes constraint satisfaction, and verifies consistency between computed quantities (flows, balance, loading) and those stored in the dataset, providing a reference implementation of these core calculations.\n6 From data to foundation models using gridfm-graphkit\ngridfm-graphkit [Graphkit] is a Python library designed as part of the GridFM project [hamann2024perspectivefoundationmodelselectric] to enable training, fine-tuning, and deployment of foundation models for electric power grids using datasets generated with gridfm-datakit . It provides seamless conversion of power grid data into PyTorch Geometric [PyG] data objects, where buses are represented as nodes and transmission lines as edges.\nBuilt on PyTorch Lightning and PyTorch Geometric, gridfm-graphkit offers a modular framework for training Graph Neural Networks and Graph Transformers. It supports self-supervised pre-training via masked feature reconstruction combined with a physics-informed loss enforcing AC power balance equations.\nThe library is optimized for large-scale, multi-GPU training and supports zero-shot evaluation on unseen topologies, as well as fine-tuning for downstream tasks such as power flow or contingency analysis. Post-processing utilities compute line loadings, detect thermal limit violations, and identify critical components under perturbed conditions.\nThrough its modular design, gridfm-graphkit can be easily extended with new models, loss functions, and learning tasks, providing a unified environment for developing graph-based foundation models for the electric grid.\nLimitations and Future Work\nPlanned features include:\n-\n‚Ä¢\nSupport for topology variations in PF prior to defining the generator setpoints (using OPF), to increase the diversity of operating points without raising the proportion of constraint violations.\n-\n‚Ä¢\nSupport for bus-level load profiles, in addition to system-level aggregated ones (e.g., from ENTSO-E).\n-\n‚Ä¢\nSampling of subgraphs from existing grids to generate more diverse topologies.\n-\n‚Ä¢\nOpen-sourcing of large scale synthetic dataset produced by gridfm-datakit on HuggingFace GridFM.\n-\n‚Ä¢\nSaving dual solutions in the dataset for primal-dual learning [primaldual] or LMP prediction.\nAcknowledgements\nThe authors would like to thank Johannes Schmude, Marcus Freitag, Tom Theis, Maxim Lysak, Martin Mevissen, Naomi Simumba, and H√©ctor Maeso-Garc√≠a from IBM; Vincent Mai and Javad Bayazi from Hydro-Qu√©bec; Blazhe Gjorgiev from ETH Zurich; Jochen Stiasny and Olayiwola Arowolo from TU Delft for their valuable discussions and feedback.\nAppendix A Comparison of Data Generation Methods and Libraries\nAppendix B Details on Data Generation Runtime\nData generation time corresponds to processing 10,000 load scenarios and creating 20 power flow topology variants for each, yielding a total of 200,000 samples. The computations were performed on a cluster equipped with Intel(R) Xeon(R) Gold 6258R CPUs @ 2.70 GHz and AMD EPYC 7763 CPUs @ 2.45 GHz. For most datasets, 20 cores were used, except for OPF GOC 2k (60 cores) and GOC 10k (100 cores). 32 GB of RAM sufficed for all datasets except GOC 10k, which required 256 GB. The default linear solver provided with Ipopt.jl was used; however, [powermodels] reports that runtime can be reduced by 2‚Äì6 using the HSL ma57 solver.\nWhen generating PF data, runtime is primarily driven by the number of load scenarios: an OPF is solved once per load scenario on the base topology, while only PF (which is significantly faster than OPF) is solved for each of the 20 topology variants. Consequently, increasing the number of topology variants per load scenario is an efficient way to obtain more samples. In OPF mode, data generation takes longer since OPF is solved for each topology variant.\nAppendix C Computation of the Mean Normalized Shannon Entropy\nWe quantify dataset diversity using the mean normalized Shannon entropy. Motivations for using such a metric are discussed in [hedgeopf2025]. Standard deviation may overestimate variability when a feature takes only a few discrete values (e.g., switching between zero and its upper bound). In contrast, Shannon entropy measures the distributional uncertainty of a feature: it is low when samples concentrate on a small portion of the admissible domain and increases only when the empirical distribution spreads across multiple well-populated regions.\nHistogram Domains.\nEntropy is computed from a discretized approximation of the empirical distribution at each bus. We construct fixed-range histograms using the interval for , and empirical per-bus minima and maxima (different for PF and OPF) across datasets for (operational limits for these cannot be used since they can be violated in the case of PF). We use consistent domains across datasets to ensure that entropy values are comparable and not driven by binning inconsistencies.\nPer-Bus Shannon Entropy.\nA 100-bin histogram over the specified domain yields empirical probabilities over the bins. The Shannon entropy at that bus is\nwith .\nAveraging and Normalization.\nEntropies are averaged across buses and normalized by the maximum value , yielding the mean normalized Shannon entropy,\nValues near zero indicate that the dataset exhibits little variability, whereas values approaching one correspond to broad and nearly uniform exploration of the feature‚Äôs admissible range."
  },
  {
    "article": "AI for Music\nAdapting Speech Language Model to\nSinging Voice Synthesis\nAbstract\nSpeech Language Models (SLMs) have recently emerged as a unified paradigm for addressing a wide range of speech-related tasks, including text-to-speech (TTS), speech enhancement (SE), and automatic speech recognition (ASR). However, the generalization capability of large-scale pre-trained SLMs remains underexplored. In this work, we adapt a 1.7B parameter TTS pretrained SLM for singing voice synthesis (SVS), using only a 135-hour synthetic singing corpus, ACE-Opencpop. Building upon the ESPNet-SpeechLM, our recipe involves the following procedure: (1) tokenization of music score conditions and singing waveforms, (2) multi-stream language model token prediction, (3) conditional flow matching-based mel-spectrogram generation. (4) a mel-to-wave vocoder. Experimental results demonstrate that our adapted SLM generalizes well to SVS and achieves performance comparable to leading discrete token-based SVS models. Project page with sample and code is available111https://tsukasane.github.io/SLMSVS/.\n1 Introduction\nLarge language models (LLMs) have attracted considerable attention in recent years due to their ability to unify representations across diverse data modalities. This unifying capability enables a single model architecture to scale effectively and generalize across a broad range of tasks. By adopting consistent paradigms for data processing and prediction, LLMs can be efficiently adapted to downstream applications, even in low-resource scenarios.\nIn the speech domain, prior research has primarily pursued two approaches: (1) fine-tuning language models pretrained on text for speech-related tasks, or (2) training language models directly on speech data. The latter approach, often referred to as Speech Language Models (SLMs), tends to capture fine-grained acoustic characteristics more effectively due to its native exposure to audio signals. However, SLMs are inherently data-intensive, requiring large-scale paired datasets. Consequently, most existing pre-training efforts focus on well-resourced tasks such as text-to-speech (TTS) and automatic speech recognition (ASR).\nIn contrast, singing voice synthesis (SVS) presents additional challenges. The input consists of richly structured musical scores, including phoneme-level lyrics, precise duration annotations, and MIDI notes. The output is vocal singing that must be both musically and phonetically faithful to these conditions. Compared to TTS, publicly available SVS datasets are far more limited due to restrictive licensing and the labor-intensive nature of score annotation.\nTo explore the generalization capability of SLMs, we propose adapting a TTS-pretrained SLM to the SVS task. We first tokenize the input music score and target singing waveforms as shown in Fig. 1, formulating a multi-stream token prediction task to fine-tune the LM. The predicted tokens, including SSL and multi-layer codec tokens, are able to be separated and decoded to a waveform using the pretrained codec decoder. However, our primary experiment shows that the raw predicted tokens are noisy, as shown in Tab. 2, with resulting waveforms exhibiting temporal discontinuities, particularly at token boundaries, leading to perceptual glitches and unnatural transitions 1. Moreover, as the codec model espnet-codec is pretrained on speech data, it lacks the ability to faithfully resynthesize singing, resulting in a performance upper bound set by the decoder side.\nTo alleviate the unsatisfactory performance introduced by the codec decoder and the noisy tokens, we use a conditional flow matching model, converting the source Gaussian noise to the target mel spectrogram conditioned on the codec, and additionally train a vocoder hifigan that is consistent with the codec STFT parameters. This optimization enables high-quality singing synthesis while addressing the limitations posed by data scarcity. Considering the expressiveness of synthesized singing, we strengthen the condition of pitch information again through the flow matching process, which improves the melodious fidelity.\nEmpirical results demonstrate that this second-stage refinement improves synthesis quality, yielding smoother transitions and enhanced pitch accuracy. Overall, our framework enables SLM-based SVS to achieve performance comparable to leading discrete SVS systems. A detailed introduction of related works is attached in appendix A.\n2 Methodology\nGiven the music score , SVS targets to generate a human singing phrase that align with , where is the number of samples in the waveform, , , represents information about phoneme, pitch, and duration over the sequence of the same length . We first introduce the SVS data tokenization, then formulate the SVS fine-tuning on SLM, and lastly demonstrate the conditional flow refinement pipeline.\n2.1 SVS Data Tokenization\nOur pipeline is built upon Espnet-SpeechLM tian2025espnetspeechlm; opuslm2025tian. For SVS, we introduce a new modality called svs_lb, which consists of frame-level pitch, duration, and phoneme conditions. Each unit in svs_lb is a two-element tuple, including a phoneme token and a pitch token with svs prefix. We use the repeat time of (phn, pitch) tuple to implicitly represent duration, which is calculated as: , where and represent the end and start time of an element in . This process aligns the annotation with the audio codec sample rate. We show our SVS data tokenization pipeline in Figure 1. Our prompt includes the svs_lb and spk_prompt; the target tokens are set to be the concatenation of codec and SSL tokens of singing waveforms.\n2.2 Language Model Formulation\nFor audio representation, we use two types of tokens: the high-level semantics tokens obtained from SSL, and the low-level acoustic tokens from the audio codec. We follow the pre-trained TTS model to use a multi-stream discrete audio representation for each frame, where stands for the number of streams, is the total frame number. Specifically, we concatenate the audio codec tokens and the SSL tokens along the stream dimension, intending to balance their advantages for prediction and acoustic reconstruction. Then, we only keep the codec tokens for decoding. Task and template definitions are shown in Figure 2.\nIn SLM, tasks can be uniformly formulated as predicting the target sequence based on the input condition sequence. Adapting this template to SVS, we have the inputs , which is the frame-level music score unit, speaker prompt ; and the target frame-level singing feature . As a token classification task, the fine-tuning objective is to maximize the posterior likelihood using cross-entropy loss.\n2.3 Flow-Based Refinement\nFlow models are a class of generative models that learn a velocity field capable of transporting samples from a source distribution to a target distribution. In this work, we adopt flow matching to train the flow model efficiently and scalably. During training, the model regresses the velocity field along a probabilistic interpolation path by sampling intermediate timesteps . At inference, a sample from the source distribution is provided, and the trained velocity field is used to transport it towards the corresponding sample in the target distribution.\nLet the source distribution be denoted as , where is a standard Gaussian distribution. The corresponding target samples are drawn from , where is the distribution of mel features obtained from the target waveforms from STFT. The goal of the flow model is to learn a smooth mapping from to through a continuous-time velocity field, which follows a continuous-time Markov process. The evolution of a sample over time is governed [AUTHOR_NAME_REMOVED] a small time increment . The instantaneous velocity of a point along its trajectory is defined as:\nwhere is the velocity field at time . For flow matching, we adopt a linear interpolation path between source and target samples. This formulation corresponds to the optimal transport path under a kinetic energy minimization constraint,\nWe fuse the LM-predicted codec and pitch signal as additional conditions to make the flow controllable. Let denote the conditioning input, which contains and other optional choices. Let represent the parameters of the conditional flow model. The training objective of Conditional Flow Matching (CFM) is to minimize the squared error between the ground-truth velocity and the model-predicted velocity at a set of points sampled from the path from source distribution to target distribution:\nwhere is the predicted velocity field function. During inference, we solve the corresponding ordinary differential equation (ODE) using a numerical ODE solver, which starts from , and integrates the velocity field forward in time to obtain the target sample in .\nThe ultimate sample is further converted to a waveform using a vocoder,\n3 Experiment\n3.1 Corpus and Parameters Setups\nACE-Opencpop is a synthetic corpus that inherits the song list from 5.2-hour Mandarin female singing corpus Opencpop, but curates the singing of 30 additional singers using ACE Studio with manual tuning, resulting in the largest, 135-hour opensourced SVS corpus. Detailed parameters setup for SLM fine-tuning and flow matching are shown in appendix B.\n3.2 Results Analysis and Ablations\nExplanation of the abbreviations used in Tab. 2 and Tab. 2. XiaoiceSing uses music score information to predict discrete tokens and train a vocoder on discrete tokens to waveform. TokSing also builds on a discrete NAR architecture, but further introduces a melody predictor and a music enhancer to improve pitch precision. CD Resynthesis denotes directly encoding and decoding singing waveforms using a speech-pretrained codec model. Flow1 refers to flow matching conditioned on the LM-predicted codec features, while Flow2 conditions on both the LM-predicted codec features and the pitch tensor. +CD indicates that the flow output is in the codec embedding space and is converted to waveforms via a pretrained codec decoder. +Voc indicates that the flow output is in the mel-spectrogram space and is converted to waveforms via a vocoder.\nMetrics used in Tab. 2 and Tab. 2. F0_RMSE and F0_CORR hayashi2020espnet measure the root mean squared error and correlation between the fundamental frequency (F0) of the synthesized and reference singing signals, focusing on pitch accuracy. MCD kubichek1993mel quantifies the spectral distance between the generated and ground-truth audio using mel-cepstral coefficients. PER denotes phoneme error rate. SingMOS singmos and Sheet-SSQA huang2024mos is a pseudo MOS predictor based on a 5-point mean opinion score scale.\nQuantitative Analysis. As shown in Tab. 2, our SLM-based SVS pipeline achieves performance comparable to state-of-the-art discrete SVS models. Compare with XiaoiceSing lu2020xiaoicesing, our model performs better in pitch-related f0 metrics and overall singing quality, as indicated by pseudo MOS. While the pitch accuracy (reflected by the f0-related metrics) slightly lags behind TokSing wu2024toksing, the overall singing quality matches or even surpasses it, demonstrating the effectiveness of our architectural design and the strong downstream generalization capability of SLM. The ablation study highlights the impact of using mel versus codec features as the flow output space: mel features appear easier to model, as indicated by the greater improvement over the LM+CD baseline. Also, as the speech-pretrained codec model leads to information loss in resynthesis already, it sets an upper bound for using the codec feature to form the flow output space. Moreover, incorporating pitch information into flow matching yields a modest gain in pitch fidelity, which is shown by the comparison between LM + Flow1 + Voc and LM + Flow2 + Voc.\n4 Conclusion\nIn this paper, we present a pipeline that adapts a TTS-pretrained SLM for the SVS task, achieving performance on par with state-of-the-art discrete SVS methods. This demonstrates the strong generalizability of SLMs in low-resource downstream settings and points to promising directions for future multi-task SLM research.\nAcknowledgement. Experiments of this work used the Bridges2 at PSC and Delta/DeltaAI NCSA computing systems through allocation CIS210014 from the ACCESS program, supported by NSF grants 2138259, 2138286, 2138307, 2137603, and 2138296.\nAppendix A Related Works\nA.[ADDRESS_REMOVED] witnessed significant advancements in recent years, primarily driven by increased model scale, enhanced data availability, and improved training paradigms. Their ability to learn unified representations across modalities has enabled progress on a wide range of tasks.\nRecent works have extended this paradigm to the audio domain, resulting in speech language models (SLMs) that treat speech as a sequence of discrete tokens, analogous to natural language abouelenin2025phi; chu2024qwen2; ding2025kimi; wang2023neural; kharitonov2023speakreadprompthighfidelity. SLMs demonstrate strong transferability from general pretraining to specific downstream tasks, using techniques like fine-tuning ding2025kimi; chu2024qwen2 and LORAs abouelenin2025phi, while pretraining is typically conducted on large-scale, general-purpose corpora, domain adaptation is feasible through fine-tuning on curated task-specific datasets.\nPrevious works also explore using a language model to generate music zhang2025inspiremusicintegratingsuperresolution; bai2024seedmusicunifiedframeworkhigh. However, training the language model on singing requires a large amount of data, which is always closed-source and hard to access by the community. Our work adapt TTS-pretrained SLM to a low-resource SVS setting.\nA.2 Singing Voice Synthesis\nSinging voice synthesis (SVS) aims to generate expressive and intelligible singing voices from structured musical inputs, typically including phoneme-level lyrics, MIDIs, and note durations. Compared to TTS, SVS requires a higher degree of temporal precision and pitch accuracy to maintain musicality. This renders SVS more sensitive to the alignment between input conditions and the generated acoustic output. Early approaches in SVS were based on concatenative and HMM-based methods saino2006hmm, while recent work has shifted towards neural vocoder-based systems lu2020xiaoicesing, including encoder-decoder architectures and end-to-end frameworks visinger2.\nRecent studies have begun leveraging large language models (LLMs) for singing voice synthesis (SVS) and text-to-song generation, enabling more flexible and controllable singing beyond traditional alignment-based methods. Prompt-Singer promptsinger introduces a prompt-based SVS system that controls vocal style (e.g., timbre, range) via natural language, using a decoder-only transformer with a range‚Äìmelody decoupled pitch representation for intuitive style manipulation. MelodyLM / TTSong TTSong advances toward fully text-driven song generation by predicting melody representations from lyrics and textual descriptions, integrating LLM-based melody modeling with diffusion-based accompaniment synthesis. LLFM-Voice LLFMvoice unifies expressive speech and singing synthesis using an LLM front-end and flow-matching acoustic model, achieving smoother emotional expression and higher-fidelity vocal rendering.\nOur work differs by leveraging a general-purpose speech language model, pre-trained on TTS data, and adapting it to SVS through token-level modeling and conditional refinement, enabling the complex SVS to be a subtask of a unified model.\nA.3 Flow-Based Models\nFlow-based generative models, such as RealNVP dinh2016density and Glow kingma2018glow, are a class of invertible neural networks that learn data distributions through a sequence of bijective transformations. These classical flow models enable exact likelihood estimation and efficient sampling, and have been successfully applied to high-fidelity speech and audio generation tasks, including vocoding prenger2019waveglow, speech enhancement strauss2023improved, and expressive speech synthesis popov2021grad.\nIn contrast, flow-matching approaches tong2023conditional are ODE-based methods conceptually closer to diffusion models (which are SDE-based). Instead of performing explicit density estimation like classical flows, flow-matching trains conditional flows via velocity field learning, providing a scalable and flexible framework for conditional generation.\nIn this work, we adopt a conditional flow-matching model to generate mel-spectrogram features from Gaussian noise, conditioned on LM-predicted codec embeddings as well as musical pitch labels. This approach refines the acoustic realism and pitch fidelity of generated singing voices while avoiding the computational overhead of traditional flow-based likelihood estimation.\nAppendix B Model Parameter Setups\nB.1 Language Model Finetuning.\nWe finetune the model using DeepSpeed with mixed-precision (FP16) training, Adam optimizer (, learning rate ), and ZeRO stage-2 optimization for memory efficiency. A Warmup‚ÄìCosine learning rate scheduler with total steps (minimum LR ratio ) is employed. Gradient clipping is set to . Contiguous memory and communication-overlap optimizations are enabled to ensure stable and scalable training.\nB.2 Flow Matching.\nWe train the model for a total of epochs with a batch size of . The learning rate is scheduled to decay linearly from to between steps and ."
  },
  {
    "article": "Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty\nAbstract\nSinging voice synthesis (SVS) has seen remarkable advancements in recent years. However, compared to speech and general audio data, publicly available singing datasets remain limited. In practice, this data scarcity often leads to performance degradation in long-tail scenarios, such as imbalanced pitch distributions or rare singing styles. To mitigate these challenges, we propose uncertainty-based optimization to improve the training process of end-to-end SVS models. First, we introduce differentiable data augmentation in the adversarial training, which operates in a sample-wise manner to increase the prior uncertainty. Second, we incorporate a frame-level uncertainty prediction module that estimates the posterior uncertainty, enabling the model to allocate more learning capacity to low-confidence segments. Empirical results on the Opencpop and Ofuton-P, across Chinese and Japanese, demonstrate that our approach improves performance in various perspectives.\nI Introduction\nSinging voice synthesis (SVS) aims to generate natural and expressive singing vocals conditioned on input lyrics, pitch, and duration [cook1996singing, hono2021sinsy, kenmochi2007vocaloid]. The task has gained increasing popularity due to growing demands for digital human applications [yu2017talking, kearney2016design] and the rapid advancement of deep generative modeling techniques [NIPS2014_f033ed80, kingma2013auto, song2020denoising]. Compared to text-to-speech (TTS) [tomoki2022espnettts, cosyvoice2024du] or general music generation [bai2024seedmusicunifiedframeworkhigh, zhang2025inspiremusicintegratingsuperresolution], SVS presents unique challenges: it requires accurate alignment with linguistic content while also capturing fine-grained prosody, making it particularly complex for neural models to learn effectively. Furthermore, the development of SVS is hindered by limited data availability: SVS models tend to underperform in long-tail scenarios, such as high-pitch phrases or underrepresented singing styles. In contrast to speech or music datasets, publicly available singing corpora remain scarce. Licensing songs from professional artists is often difficult, and annotating MIDI scores requires expert-level effort.\nPrevious work has explored data augmentation and corpus expansion to address these data scarcity issues [guo2022singaug, shi2024singing, ghosh2024synthio, wang2024prompt, yu2024visinger2+, ren2020deepsinger, shi2021sequence, saino2006hmm, wang2022opencpop, ogawa2021tohoku]. The motivation is to provide more data variation, enabling the model to leverage samples with higher uncertainty (i.e., diverse pitch range or rare musical style) and update its gradient more efficiently. However, these approaches typically aim to avoid introducing excessive noise or artifacts to ensure that the augmented data remains close to the original distribution. While this helps preserve generation quality, it may also prevent generalizability to more diverse or challenging singing patterns. The data expansion and conservative augmentations introduce variations in a fixed scale or small range, and constrain the optimization to modifying the data. However, equipping the model with a better awareness of its own uncertainty can enhance robustness even under limited data conditions, by guiding learning toward more challenging regions, which is a potential that remains underexplored.\nIn this work, we introduce two uncertainty-based optimization methods to improve the training process: (1) a differentiable augmentation module in adversarial training to increase the prior uncertainty, and (2) a uncertainty prediction module that estimates the model‚Äôs posterior uncertainty using the frame-level latents. The prior means formulate without introducing the true value, and the posterior indicates with the true value, which we have a detailed explanation on Sec. III. Together, these techniques enrich the training process and help the model better handle long-tail cases. As a result, we observe consistent performance improvements over the baseline. We provide a detailed analysis in Sec. IV-D to better understand how the impact of each component varies depending on the characteristics of the corpus. Additionally, these uncertainty formulations can be seamlessly combined with existing SVS models, which commonly employ GAN-based vocoders [kong2020hifi, yamamoto2020parallel] or two-stage joint adversarial training [asystematic], and with reconstruction-based objectives.\nIn summary, our contributions include:\n-\n‚Ä¢\nWe are the first to apply differentiable data augmentation to SVS, enabling stronger input perturbation while preserving the target distribution unchanged for generation.\n-\n‚Ä¢\nWe propose a posterior uncertainty modulation for sample-level audio interval, demonstrating its effectiveness in enhancing robustness.\n-\n‚Ä¢\nWe integrate these uncertainty-based techniques into an SOTA end-to-end SVS model, achieving consistent improvements across multiple metrics.\nWe provide singing samples from different systems on a website: [URL_REMOVED]\nII Related Work\nII-A Singing Voice Synthesis\nEarly deep learning-based singing voice synthesis (SVS) systems typically adopt a two-stage pipeline [blaauw2020sequence, lu2020xiaoicesing, chen2020hifisinger, liu2022diffsinger]: the model first predicts intermediate acoustic features, which are then converted into waveforms using a vocoder. While this approach has proven effective, it often suffers from information loss between stages and limited expressiveness in the final output. More recently, end-to-end SVS models such as VISinger [zhang2022visinger] and VISinger2 [zhang2022visinger2] have gained increasing attention. These models leverage a combination of conditional variational autoencoders (CVAEs)[kingma2013auto] and generative adversarial networks (GANs)[NIPS2014_f033ed80] to directly generate waveforms from music score inputs. This fully end-to-end architecture simplifies the training pipeline and improves synthesis fidelity. VISinger2+ [yu2024visinger2+] concatenates discrete representations obtained from pre-trained SSL models [hsu2021hubert, li2023mert] to the input mel-spectrogram, further improving the performance in musical details and singer similarity in a multi-singer corpus [shi2024singing]. Despite these advancements, SVS remains a challenging task due to several factors, (1) fluctuating pitches in techniques such as vibrato. (2) sensitive timing alignment when dealing with expressive timing or tempo variations. (3) expressiveness for underrepresented singing styles, emotions, or vocal techniques. Our uncertainty-based training method helps address these issues, improving the musicality and expressiveness of the generated audio while being more data-efficient and robust to long-tail cases.\nII-B Data Augmentation\nPrevious works have explored several augmentation and data expansion strategies to mitigate the data scarcity issue. Singaug [guo2022singaug] introduces pitch augmentation by simultaneously shifting the music score and the waveform by a certain number of semitones. They also propose feature space mixup and add a corresponding loss based on the mixed acoustic target. Other works [choi2022melody, wang2024prompt] leveraged out-of-domain speech data or low-quality singing recordings with background noise as auxiliary training resources. There are also approaches that uses commercial SVS systems, such as ACE Studio, to synthesize stylized singing and use in training [kenmochi2007vocaloid, shi2024singing].\nWhile these methods can ameliorate data scarcity, the methods target generation tasks typically prioritize maintaining the original training distribution. In speech understanding tasks, however, previous works have shown that simple augmentation to the speech data distribution could significantly improve the speech recognition performance [park2019specaugment]. To apply this similar concept to the synthesis domain, we propose to use a differentiable singing augmentation method that introduces more substantial perturbations to the end-to-end SVS training process. The model is provided with diverse real-fake feature pairings, increasing the data variation and therefore prior uncertainty. In addition, we leverage an uncertainty predictor to improve the model‚Äôs awareness of posterior uncertainty and improve its robustness.\nII-C Uncertainty Prediction\nUncertainty prediction is widely used across machine learning domains to enhance model performance in various tasks [shi2022investigation, kendall2017uncertainties, kuleshov2018accurate, gawlikowski2023survey, hu2024cg]. Uncertainty can generally be categorized into data uncertainty (aleatoric) and model uncertainty (epistemic) [gawlikowski2023survey]. Recent works on NeRF and Gaussian Splatting for 3D reconstruction [hu2024cg, huang2024gaussianmarker, pan2022activenerf, shen2022conditional] frequently incorporate uncertainty estimation as a core component. In these contexts, covariance matrices are commonly used to capture data uncertainty, representing the sensitivity of rendering results to input perturbations or spatial variance. On the other hand, Jacobian-based methods are often employed to measure model uncertainty, quantifying the network‚Äôs confidence in its predictions. However, applying Jacobian-based uncertainty estimation to audio signals is challenging, as the outputs are long temporal sequences, leading to unstable or excessive measurements. Previous work [shi2022investigation] on target-speaker recognition introduces a neural uncertainty estimator that leverages confidence scores from the speaker extraction module to enhance recognition performance, demonstrating the potential of using auxiliary predictors to modulate uncertainty. Similarly, prior studies in classification and regression [kendall2017uncertainties, kuleshov2018accurate] have used error-based proxies to model posterior uncertainty more effectively. Motivated by these approaches, we propose to modulate both prior uncertainty and posterior uncertainty in our SVS framework, aiming to enable more efficient and robust training.\nIII Formulation\nConditioned on the music score , SVS targets to generate a human singing phrase that align with , where is the length of the waveform, , , represents information about phoneme, pitch, and duration over sequences of the same length . Training data is the singing phrase in the source distribution. We formulate the augmented SVS as a multi-stage training with a variety of strategies. In this section, we first introduce the baseline framework, then elaborate on differentiable augmentation, and lastly detail the architecture and mathematical definition of the uncertainty predictor.\nIII-A VISinger2 Framework\nVISinger2 [zhang2022visinger2] is an end-to-end SVS model built upon CVAE and GAN architecture as shown in Fig. 1. The input waveform is first converted to a mel-spectrogram. The posterior encoder111Notably, the prior (w/o true value) and posterior (w/ true value) in uncertainty are defined differently from the prior (w/o known distribution) and posterior (w/ known distribution) used in CVAE explanation., as shown in the upper-left of Fig. 1, takes the mel-spectrogram and extracts the latent representation .\nwhere denotes the posterior distribution. The decoder is trained to reconstruct the waveform.\nThe prior encoder, as shown at the bottom-left of Fig. 1, takes the music score and a prior sampled from a standard Gaussian (or flow model modified distribution) to generate a prior latent .\nThen, the shared decoder also reconstructs the waveform using the prior latent as\nThe prior latent distribution in Eq. (3) is drawn to the posterior latent distribution in Eq. (1) using the KL loss. Then, the decoder of the CVAE serves as a generator , where several discriminators take the posterior sample and prior sample for adversarial training.\nIII-B Differentiable Augmentation\nAdversarial Training\nGAN-based end-to-end models are highly susceptible to mode collapse when training on limited amounts of data. The discriminator tends to memorize the training samples instead of learning meaningful features to distinguish the real and generated samples. Differentiable augmentation has been utilized in image generation [zhao2020differentiable] to enhance data diversity in StyleGAN [karras2019style] training. We also adopt this technique for SVS to facilitate continuous updates to the generator. In the baseline model VISinger2 [zhang2022visinger2], three types of discriminators are used, namely Multi-Resolution Spectrogram Discriminator (MRSD), Multi-Period Discriminator (MPD), and Multi-Scale Discriminator (MSD) as shown on the right side of Fig. 1. They capture features in the frequency and time domains across multiple scales. To simplify the notation, we use to collectively represent these three discriminators. The adversarial loss in this case is formulated as:\nwhere the discriminators are trained to distinguish between real and synthesized samples generated from different latents, and the generator (also as the decoder of CVAE) aims to deceive the discriminator.\nAugmentation Methods\nPrevious augmentations in automatic speech recognition (ASR) and speech enhancement (SE) include masking, adding noise, and warping [park2019specaugment, ghosh2024synthio]. These aggressive augmentations can be adapted in adversarial training, without shifting the original data distribution of [zhao2020differentiable]. We use to denote these augmentation operations. We introduce the formulation of differentiable masking and adding noise, then show the result of using them individually or in combination in V. In practice, these techniques can be selected based on task- or dataset-specific characteristics.\nWe denote the encoders of the three discriminators as , then the posterior features and prior features extracted by are:\nwhich corresponds to the right part of Fig. 1. As observed in [zhao2020differentiable], applying augmentations directly to posterior features can lead the model to learn the distribution of rather than , introducing a distributional shift in generated samples. On the other hand, if augmentation is applied solely during the discriminator‚Äôs forward pass, the discriminator may only learn to distinguish between and , rather than focusing on the and distinction. Therefore, we apply augmentation to both and during the generator and discriminator forward pass as and . The training flow is shown as the orange and black arrows in Fig. 1. To simplify the notations, we denote and all as start from here. We explain the operations as follows.\nRather than corrupting the entire feature segment, we only target a randomly selected partition. For the time dimension, we first compute the augmented target length based on a predefined ratio , then select a random start index\nwhere is the temporal length of the feature. Adding a mask along the time dimension to the sample interval can be formulated as:\nwhere is a binary mask , with fixed value . corrupts the randomly selected interval along the time dimension to obtain by the operation, which is the masked feature. This operation is differentiable to and also to the input .\nSimilarly, we define the binary mask at frequency bin as . The target frequency interval is also randomly selected. The masked STFT representation is obtained by element-wise multiplication of the original feature\nThe operation corrupts a randomly selected band of frequencies in the signal and is differentiable with respect to . In practice, when we choose masking for differentiable augmentation, we use a frequency dimension mask on the output feature of the MRSD encoder, and a time dimension mask on outputs of the MPD and MSD encoders.\nAdding noise is another way to inject controlled perturbations into the waveform input during training. It enables the model to learn from a wider range of acoustic variations. Given an input feature , adding noise to the feature can be formulated as:\nIn the randomly selected interval, is the noise sampled from a standard Gaussian multiplied by a noise scaling factor . The operation adds this noise to .\nFinally, the noisy segment is padded and stitched back into the original features to form the final augmented batch. This local perturbation adds noise variations while retaining most of the original feature content, improving the model‚Äôs ability to generalize to unseen scenarios. The masking and adding noise strategy can also be used simultaneously, which we will discuss in Sec. V.\nAs shown in Fig. 1, the augmentation introduces diverse real-fake feature pairs and optimizes the data uncertainty, allowing the generator to efficiently update. We denote this as a prior uncertainty, which is distributed according to and . Incorporating it with adversarial training keeps consistency in the learning target and enhances the model‚Äôs ability to generalize.\nIII-C Uncertainty Prediction\nThe posterior uncertainty refers to the error proxies, which show the confidence of the model prediction. Rather than being merely undesirable, it can be explicitly modeled to improve prediction reliability [gawlikowski2023survey, kendall2017uncertainties]. Given variable-length frame-level latent representations, we introduce a convolution-based uncertainty predictor to estimate the uncertainty level across the generated sample-wise outputs.\nAs defined in Eq. (3), the frame-wise latent is passed to the generator (or the decoder of CVAE) and further converted to the prior sample as in Eq. (4). We define a predictor that takes the latent as input and predicts the uncertainty .\nThe frame-level latent is first linearly interpolated to a fixed interval length, then passes through two 1D convolutional layers for feature transformation, and finally uses a fully connected layer to predict a single scalar value for each time step, referring to uncertainty.\nThis additional predictor requires a pre-trained model that is already capable of generating reasonable output, so that error prediction can be meaningful. In training, this uncertainty is supervised by the ground truth L2 distance between the input and the reconstructed sample .\nis the total number of samples in an interval. In this manner, the uncertainty predictor is trained to quantify the model‚Äôs confidence with the true value provided, thus it is posterior.\nIV Experiment\nIV-A Training Strategies\nBaseline Architecture\nOur training process consists of multiple stages, with the following annotation conventions: + denotes the simultaneous application of components, while & indicates a resumed training stage. B represents the baseline model, whereas D and U correspond to the differentiable augmentation and uncertainty prediction, respectively, both of which are designed to enhance training process. The main experiments are conducted on the Opencpop corpus over 200 training epochs.\nWe use VISinger2 [zhang2022visinger2] implemented in Muskits-ESPnet [wu2024muskits, shi2022muskits] as our baseline. The prior encoder in Eq. (3) conditioned on music score comprises 6 blocks, each featuring a 2-head relative self-attention mechanism and a 1D-convolutional feed-forward network (FFN). The decoder component in Eq. (2) upsamples the features. Weight normalization is applied in both the decoder and the posterior encoder. We do not use the flow network to transform the prior distribution so that it is a standard Gaussian as in Eq. (3). The discriminator part incorporates multi-period, multi-scale, and multi-frequency components as detailed in Sec. III-B. The generator and discriminator, as in Eq. (6) and Eq. (5) are trained using the AdamW optimizer [AdamW2019Loshchilov] with an initial learning rate of 2.0e-4 and an exponential learning rate scheduler with a gamma of 0.998. More detailed configuration refers to ESPnet [watanabe2018espnet] config222https://github.com/espnet/espnet/blob/master/egs2/opencpop/svs1/conf/tuning/train_visinger2.yaml.\nSystem Annotations\n-\n‚Ä¢\nB: The baseline model.\n-\n‚Ä¢\nB+D: We add noise differentiable augmentation from scratch. The augmentation region is randomly selected as 10% of the total interval along the temporal dimension.\n-\n‚Ä¢\nB&U: We first train the baseline for 20 epochs, then resume training the whole baseline model with an additional frame-to-sample uncertainty predictor. A uncertainty prediction loss is added using a weight of 10.0, working on the forward generator pass.\n-\n‚Ä¢\nB&U&(U+D): We follow B&U for the first 80 epochs, then add D to the end of training.\nIV-B Corpora\nWe follow the official split and resample all utterances to 44.1 kHz. The corpora are introduced as follows:\nOpencpop[wang2022opencpop]. It is a Mandarin singing corpus performed by one female professional singer. It consists of 100 popular songs with corresponding MIDI annotations. All songs are recorded at 44.1kHz and have a total duration of 5.2 hours.\nOfuton-P[ofutonp]. It is a Japanese singing corpus performed by one male singer, containing 46 songs with a total duration of 61 minutes. Most of the songs are children‚Äôs rhymes. The original recording is in 96kHz, with some of them resampled to 44.1kHz before release.\nIV-C Evaluation Metrics\nWe employ both objective metrics and subjective ratings to evaluate the performance of each system through VERSA toolkit [shi2025versa, shi2024versaversatileevaluationtoolkit]. We briefly introduce each metric as follows.\nLog_F0_RMSE [hayashi2020espnet] measures the root mean squared error between the logarithmic fundamental frequency (F0) of the synthesized and reference singing signals, focusing on pitch accuracy. MCD [kubichek1993mel] quantifies the spectral distance between the generated and ground-truth audio using mel-cepstral coefficients. Semitone accuracy measures pitch differences between the generated sample and ground truth. VUV evaluates whether the model correctly predicts the voiced or unvoiced status of each frame. Sheet-SSQA [huang2024mos] is a pseudo MOS predictor based on a 5-point mean opinion score (MOS) scale. evaluates the intelligibility and correctness of the synthesized lyrics, as rated by human listeners. measures the perceived quality of melody reproduction. MOS assesses the overall naturalness of the synthesized singing voice perceived by human listeners.\nFor the subjective evaluation, we use MOS across different dimensions. We randomly select [ADDRESS_REMOVED] 20 recruited labelers rate the same set of utterances generated by different systems on a scale from 1 to 5. All labelers are native Mandarin speakers, ensuring their ability to accurately assess the quality of the synthesized lyrics.\nIV-D Quantitative Analysis\nWe label the best performance in each metric in Table I using bold font, while marking the second place using blue font. Comparing B and B+D, we observe a notable improvement in pitch and timbre-related metrics, suggesting that introducing greater variation in short intervals helps the model capture finer melodic details, which mitigates the issues mentioned in Sec. II-A. Noising part of the interval encourages the model to focus on other subtle artifacts that influence the perceived realism of the prior and posterior sample pairs. This enables the model to generate more expressive pitch contours and richer timbral characteristics, improving data uncertainty and obtaining a better model training process, which proves the hypothesis in Sec. III-B.\nAdding the uncertainty predictor, as shown by results in B&U, improves the perception quality in terms of pseudo MOS and MOS, in both the lyrics and melody, also overall naturalness. This shows formulating posterior uncertainty as the gap between ground truth samples and the synthesized samples, and adding a prediction module helps in efficient training. Minimizing this performance gap as an additional loss enables the model to better understand which part it mainly struggles with and improve its performance accordingly.\nCollectively using differentiable augmentation and uncertainty prediction is shown to be optimal. Since this strategy has the maximum number of best or best+second-best scores across the 8 evaluation metrics and 4 systems. Notably, it improves the baseline performance in (1) pitch, shown by Log_F0_RMSE, (2) timbre, shown by MCD, (3) duration, shown by VUV, and (4) human perception, shown by MOS, demonstrated in Tab. I, mitigating the problems mentioned in Sec. II-A. The MOS improvement is especially significant given that the baseline is already the SOTA SVS method and does well in common cases, which further validates the effectiveness of our strategies in long-tail cases.\nIV-E Qualitative Analysis\nWe show a spectrum example of the same utterance synthesized by the B and our best-performing system B&U&(U+D) in Fig. 2. Zooming in on the region highlighted by the green dashed box, we can observe that the upper spectrum with differentiable augmentation and uncertainty prediction better constructs the high-energy boundary, which is a start point of a phoneme. Without a faithful modeling of this stylized region, the baseline utterance sounds ambiguous and has worse lyrics delivery. We include more examples on the website.\nV Ablation Study\nV-A Differentiable Augmentation Strategies\nIn our main experiment, we adopt additive noise as a differentiable augmentation method. For ablation studies, we explore three strategies: additive noise, masking, and their combination, as introduced in Sec. III-B. In all cases, augmentation is applied to 10% of the input features. Masking is performed either along the time or frequency dimension, depending on the discriminator type, with masked values set to zero. As shown in Table II, all three strategies lead to performance improvements. Among them, using noise alone yields the best results (achieving the highest number of best or second-best scores). As noise introduces a more general form of perturbation in the feature space, encouraging the model to ignore the perturbed regions and instead focus on learning more subtle and discriminative patterns.\nV-B Generalization to Other Languages\nWe further conduct experiments on the Japanese singing corpus Ofuton-P, using differentiable augmentation and an uncertainty predictor. As described in Sec. IV-B, this corpus is significantly smaller, approximately 20% the size of Opencpop. It mainly consists of traditional Japanese children‚Äôs songs, which are generally simple, feature repetitive phonemes, and exhibit a narrow pitch range. We train the model for 200 epochs.\nThe results, shown in Table III, validate the generalizability of our proposed strategies to a new corpus in another language. It demonstrates consistent improvements in Log_F0_MSE and Semitone, indicating enhanced pitch accuracy in the synthesized outputs. A slight degradation in MCD is observed, likely due to a trade-off between pitch precision and timbre fidelity. In addition, we see improvements in duration prediction and pseudo-MOS, reflected by VUV and Sheet-SSQA, suggesting enhanced temporal alignment and increased alignment with human perceptual preferences.\nVI Conclusion\nIn this paper, we propose two uncertainty-driven strategies to enhance the training process of end-to-end singing voice synthesis (SVS) models. We incorporate prior uncertainty through differentiable data augmentation and introduce a posterior uncertainty predictor to improve robustness in long-tail scenarios. Extensive experiments demonstrate the effectiveness and generalizability of our proposed methods. In the future we will explore their collective effect with other augmentations.\nVII Acknowledgement\nExperiments of this work used the Bridges2 at PSC and Delta/DeltaAI NCSA computing systems through allocation CIS210014 from the ACCESS program, supported by NSF grants 2138259, 2138286, 2138307, 2137603, and 2138296."
  },
  {
    "article": "Children‚Äôs National Hospital, Washington, DC, USA 22institutetext: Universidad Polit√©cnica de Madrid and CIBER-BBN, ISCIII, Madrid, Spain 33institutetext: School of Medicine and Health Sciences,\nGeorge Washington University, Washington, DC, USA\nAdaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble\nAbstract\nRobust and generalizable segmentation of brain tumors on multi‚Äëparametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor‚Äë and lesion‚Äëspecific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion‚Äëlevel performance metrics determine the influence of each model in the ensemble and optimize post‚Äëprocessing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.\n* These authors contributed equally.\n1 Introduction\nIn the United States, cancer is the second leading cause of death overall and the primary cause among individuals younger than 85 years old. Brain and other central nervous system tumors are the top cause of cancer death in children and adolescents under 20, and brain tumors also lead cancer mortality in men aged 20‚Äì39. In 2025, about two million new cancer cases and 618,000 cancer-related deaths are projected in the U.S. [cancer-stats-2025]. Early, accurate diagnosis is essential to improve outcomes, but wide variation in tumor appearance across imaging devices and population makes consistent assessment difficult, highlighting the need for reliable quantitative diagnositc and prognostic tools.\nIn neuro-oncology, accurate segmentation of brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is a fundamental step for diagnosis, treatment planning, and longitudinal monitoring. Yet manual contouring remains labor-intensive and prone to inter‚Äëobserver variability, motivating robust automated solutions in the clinical workflow.\nThe international Brain Tumor Segmentation (BraTS) Challenge [brats2021, bakas1, bakas2, bakas3, medperf, brats2015], organized in conjunction with the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference, has driven and benchmarked segmentation algorithmic innovation since MICCAI 2012. In its 2025 edition, BraTS expanded into a suite of challegnes that cover a broader range of tumors and tasks, earning recognition as one of MICCAI‚Äôs three Lighthouse Challenges.\nRecent deep learning models such as nnU‚ÄëNet [nnunet], MedNeXt [mednext], Swin UNETR [swinunetr2, swinunetr], and emerging Mamba‚Äëbased hybrids [umamba, segmambav2] have defined the current state of the art (SOTA) in medical image segmentation. As the SOTA models have become more stable and robust across diverse tasks, their reported Dice scores have converged within narrow margins [nnunet_revisited], signaling a performance plateau. While ever more complex architectures are possible, they would offer diminishing added values.\nBraTS‚Äëwinning solutions have demonstrated that model-independent and GPU-free heuristics, including ensemble voting [maani2023advanced], size‚Äëaware connected component filtering and ET‚Äëto‚ÄëNCR relabelling [capellan2023model], and adaptive refinement schemes [jiang2024magnetic, parida2024adult], can substantially boost segmentation performance. Instead of pursuing ever‚Äëdeeper or more intricate networks, we believe that the next gains will arise from target-specific pipeline design. Robust pre‚Äëprocessing such as radiomic‚Äëguided fold splitting mitigates sampling bias; lesion‚Äëaware ensemble weighting exploits complementary backbone strengths; and subtype‚Äëspecific post‚Äëprocessing removes residual artifacts.\nWe therefore introduce and apply a flexible, modular, backbone‚Äëindependent pipeline to multiple BraTS 2025 Lighthouse Challenges to show its versatility. This end‚Äëto‚Äëend strategy consistently outperforms any single backbone, demonstrating that thoughtful pipeline engineering remains promising once model performance plateaus.\n2 Methods\n2.1 Dataset\nOur segmentation pipeline was applied to four BraTS 2025 Lighthouse tasks: multi-consortium international pediatric brain tumor segmentation (PED; train = 261, val = 91) [PEDarxiv2024, PEDarxiv], preoperative meningioma tumor segmentation (MEN; train = 1000, val = 141) [MENarxiv], meningioma radiotherapy segmentation (MEN-RT; train = 500, val = 70) [MENarxiv2024], and segmentation of pre- and post-treatment brain metastases (MET; train = 1296, val = 179) [METarxiv]. Each case contains co-registered isotropic pre-contrast T1-weighted (T1), contrast-enhanced T1-weighted (T1CE), T2-weighted (T2), and T2-weighted fluid-attenuated inversion recovery (T2-FLAIR) MRI sequences. In MEN-RT cohort, only T1CE is available and in its original image space.\nReference standard annotations of the tumor sub-regions were created and approved by expert neuroradiologists. Concretely the following tumor annotations and subregions were defined for each of the Lighthouse challenges considered in this work.\n-\n‚Ä¢\nPED: enhancing tumor (ET), non-enhancing tumor (NET), cystic component (CC), peritumoral edema (ED), tumor core (TC=ET+NET+CC), and whole tumor (WT=TC+ED).\n-\n‚Ä¢\nMEN: ET, non-enhancing tumor core (NETC), surrounding non-enhancing FLAIR hyperintensity (SNFH), TC (ET+NETC), and WT (TC+SNFH).\n-\n‚Ä¢\nMEN-RT: gross tumor volume (GTV).\n-\n‚Ä¢\nMET: ET, NETC, SNFH, resection cavity (RC; delineates the resection of region within the brain in post-treatment cases), TC (ET+NETC), and WT (TC+SNFH).\nFor more details, please refer to the challenge website: [URL_REMOVED]\n2.2 Segmentation Pipeline\nOur pipeline uses MRI radiomic features and lesion‚Äëwise metrics to adapt the workflow to each tumor subtype. As summarized in Fig 1, it comprises four stages: (i) training data preparation: stratified fold splitting based on radiomic feature clustering; (ii) model training; (iii) model selection and ensembling; and (iv) adaptive post‚Äëprocessing, including optimal removal of small connected components and label redefinition.\n2.3 Stratified Training Data Preparation\nIn five-fold cross-validation (5-fold CV) training, stratified and balanced folds outperform purely random fold splits. While stratification by tumor volume has been explored, we argue that radiomic features-capturing the full heterogeneity of tumor appearance-provide a more robust basis for fold assignment. For each case in the training set, we computed 14 shape- and 93 appearance-based radiomic features per MRI sequence based on WT mask provided by reference annotation, using PyRadiomics [van2017computational], following the protocol of Jiang et al. [jiang2023automatic].\nTo reduce the large number of radiomic features, we keep only the principal components that explain of the variance and then partition the cases with -means clustering using reduced features [jiang2024enhancing, jiang2024magnetic]. The optimal number of clusters is determined by maximizing the silhouette coefficient in the training folds. Each cluster is randomly split into five folds, yielding the final CV sets for training. Fig. [ADDRESS_REMOVED] relevant for cluster separation.\n2.4 Model Training\nBased on the reported performance in [nnunet_revisited] and our previous experience, we selected three state-of-the-art models: nnU-Net V2, nnU-Net with residual encoder (nnU-Net ResEnc M), and MedNeXt M (k=3, 17.6M parameters, 248 GFlops). Each model was trained using a 5-fold CV strategy with input image patches of 128 128 128 voxels. The nnU-Net variants and MedNeXt-M were trained using same settings, employing label-wise softmax activation, a class-weighted loss function combining Dice and cross-entropy losses, optimized by the stochastic gradient descent (SGD) with Nesterov momentum (initial learning rate=0.01, momentum=0.99, weight decay=3e-5), for 200 epochs on NVIDIA A100 (40 GB) GPUs. The implementation is available through the official frameworks repositories: [URL_REMOVED] and [URL_REMOVED]\n2.5 Label-wise Metrics as Objective Function F\nAs model training relies on a loss function, each downstream stage (model selection, ensembling, and post‚Äëprocessing) also needs a quantitative objective so the pipeline can be customized automatically to each tumor type and region. This objective function can simply be the Dice similarity coefficient (DSC). However, in reality or in the context of challenges, DSC alone is insufficient because segmentation performance is evaluated with several metrics, including DSC, normalized surface distance (NSD) and Hausdorff distance (HD), which sit on different scales and can not be added directly. Also, integrating scores across tumour sub‚Äëregions (ET, TC, WT etc.) compounds another challenge.\nWe therefore adopt the ranking strategy proposed by BraTS [LaBella2025-oh]. For every candidate prediction obtained on the training set using 5-fold CV, we compute lesion‚Äëwise metrics for all sub‚Äëregions, rank the predictions case‚Äëwise, and average these ranks across metrics and regions. The resulting internal CV score is a single value (smaller is better) that combines all lesion-wise metrics, aligns with the official BraTS leaderboard and remains robust to outliers. Our implementation is open‚Äësource and available at github.com/Pediatric-Accelerated-Intelligence-Lab/BraTS-Unofficial-Ranker.\n2.6 Model Selection and Ensemble\nEach trained model is ranked by the approach in Section 2.5 with a score . Then we used a weighted model ensemble strategy when adding the averaged 5-fold probability output from each model. The weight of each model is defined as . The weights for each model are summarized in Table 1.\nWe also evaluated the STAPLE [staple] ensemble strategy. It lagged behind the weighted ensemble when we used the three 5-fold-averaged predictions on the training data. However, STAPLE performed better on the validation set for MEN-RT task because treating each fold‚Äôs model as an independent candidate yielded 15 predictions, which markedly enhanced DSC, NSD, and rank for the MEN‚ÄëRT validation cohort.\n2.[ADDRESS_REMOVED]-processing\nAfter ensembling, we recompute stratification clusters on the predicted masks, enabling cluster‚Äë and label‚Äëspecific thresholds for adaptive post‚Äëprocessing.\nPost-processing for the removal of isolated connected components (PP1-CC): To remove small isolated CCs that are likely false or noisy predictions, we perform a grid search across clusters and labels over volumes from 0 to 500 voxels in 25‚Äëvoxel increments. For each threshold, we filter the predicted mask and compute the internal ranking score, and finally select the threshold that achieves the best averaged rank. This cluster‚Äë and label-specific filter eliminates fragments that would otherwise be counted as false‚Äëpositive lesions.\nPost-processing for label redefinition (PP2-LBLREDEF): A second adaptive PP fine‚Äëtunes the consistency of tumor sub-regions (labels). We correct systematic label confusions in a data‚Äëdriven way. After PP1-CC, we build a confusion matrix (Fig. 3) over all predicted masks again to identify pairs of frequently swapped labels. For every such pair (labelx,labely), we search, within each cluster, for the threshold on the volume ratio labelx/WT that maximizes internal CV metrics (e.g. rank). If a case with predicted mask falls below this threshold, all labelx voxels are converted to labely. This ratio‚Äëbased PP2-LBLREDEF enforces anatomically plausible label volumes and improves the performance of the BraTS metrics at the lesion level.\nAt inference, a new test case is assigned to its nearest radiomic cluster, after model ensemble and the cluster‚Äëspecific post‚Äëprocessing threshold values are then applied. Fig. 4 shows the optimal thresholds identified by CV-based grid search.\n3 Results\nThe evaluation of the model prediction on the validation set was performaed on the Synapse platform. The models were assessed for each of the tumor regions using the lesion-wise DSC and NSD with boundary threshold of 1.0 mm.\nQuantitative results of our models across the validation and testing data for each challenge are shown in Tables 2 and 3, for PED, MET, MEN, and MEN-RT, respectively. These evaluations were performed automatically by the challenge‚Äôs digital platform, with no access to the reference standard annotations on the validation set and no access to any testing data including images and labels. Figure 5 illustrated qualitative results on validation cases for PED, MET, MEN, and MEN-RT, respectively.\n4 Discussion\nOur experiments confirm that carefully engineered pre‚Äë and post‚Äëprocessing can still deliver tangible gains even when base architectures have plateaued. Notably, some gains in Dice or NSD were limited on validation set but became evident under 5-fold CV, underscoring the danger of optimizing solely on a limited and sometimes skewed validation set.\nBecause the training set is much larger than the validation set, the latter can present a skewed tumor‚Äëtype distribution and thus provide an unreliable signal for hyper‚Äëparameter and threshold tuning. We therefore recommend selecting models and thresholds by CV on the training set, using validation scores only as a sanity check. In the same spirit, the internal rank metric‚Äîaggregating Dice and NSD across tumor regions, proved more robust than any single metric, especially when candidate models were numerous. However, its sensitivity to the number of candidates (rank scores can be very close if only three candidates) suggests that alternative weighting schemes merit exploration.\nWe previously tested that stratified fold splitting based on radiomic clusters yielded higher CV scores than random splits, indicating that heterogeneity‚Äëaware sampling reduces overfitting. Additional ablations on multiple BraTS tasks are planned to quantify this effect. We also found that STAPLE fusion benefited the MEN‚ÄëRT task when each fold was treated as a separate candidate, indicating that larger model pools can regularize ensembles.\nLimitations include our reliance on simple volume thresholds for PP1 and PP2. Future work will replace these heuristics with radiomic‚Äëdriven criteria analogous to those used for stratification, further tailoring post‚Äëprocessing to both tumor morphology and appearance.\nOpen‚Äësourcing our ranker and keeping all components model-independent, we aim to make the pipeline easy to deploy in clinical workflows. When its utility is evaluated in more clinical studies, the pipeline would have the potential for larger clinical impact beyond BraTS benchmarks.\nFinally, to facilitate reproducibility and extend the utility of our approaches, we have made the complete pipeline publicly available as easy-to-use Docker containers and a webapp for all the tasks. The Docker images are hosted at: [URL_REMOVED] and the webapp is accessible at: [URL_REMOVED]\n5 Conclusion\nThis work introduces a tumor- and lesion‚Äëaware ensemble pipeline that consistently segments four distinct brain tumour cohorts in the BraTS 2025 Lighthouse Challenge. By selecting and weighting models according to lesion‚Äëlevel accuracy and adding subtype‚Äëspecific processing, the framework achieves strong segmentation performance relying on state-of-the-art network backbones. Its modular design makes it easy to adopt and extend, while the resulting quantitative tumor measurements could support clinical diagnosis, treatment planning, and longitudinal monitoring.\n5.0.1 Acknowledgements\nThis work was supported by the National Cancer Institute (UG3 CA236536), the Spanish Ministerio de Ciencia e Innovaci√≥n, the Agencia Estatal de Investigaci√≥n, NextGenerationEU grants PDC2022-133865-I00 and PID2022-141493OB-I00, and the EUCAIM project co-funded by the European Union (Grant Agreement #101100633). The authors acknowledge the Universidad Polit√©cnica de Madrid for providing computing resources on the Magerit Supercomputer."
  },
  {
    "article": "A Semantics for Belief in Simplicial Complexes\nAbstract\nWe provide a novel semantics for belief using simplicial complexes. In our framework, belief satisfies the KD45 axioms and rules as well as the ‚Äúknowledge implies belief‚Äù axiom (); in addition, we adopt the (standard) assumption that each facet in our simplicial models has exactly one vertex of every color. No existing model of belief in simplicial complexes that we are aware of is able to satisfy all of these conditions without trivializing belief to coincide with knowledge. [KaSC, SimpBel] We also address the common technical assumption of ‚Äúproperness‚Äù for relational structures made in the simplicial semantics literature, namely, that no two worlds fall into the same knowledge cell for all agents; we argue that there are conceptually sensible belief frames in which this assumption is violated, and use the result of [Note1] to bypass this restriction. We conclude with a discussion of how an alternative ‚Äúsimplicial sets‚Äù framework could allow us to bypass properness altogether and perhaps provide a more streamlined simplicial framework for representing belief.\n1 Introduction\nInterpreting multi-agent epistemic logic using simplicial complexes is a recent and thriving area of research. [SimpBel, KaSC, FA, HG, SimpSet, SimpDEL, Death, DoA] Our goal in this paper is to extend this approach to accommodate not only knowledge but also belief; this turns out to involve some subtleties, the resolutions of which are interesting in their own right.\nTo set the stage, we begin by briefly reviewing the basic notion of a simplicial complex and the intended epistemic interpretation. Informally, a simplex can be pictured as a multi-dimensional triangle; that is, a collection of vertices where every vertex is connected to every other vertex by an edge. So with three vertices we obtain a standard triangle, four gives a tetrahedron, and in general, many vertices gives an -dimensional triangle. A simplicial complex can be thought of as a collection of simplices with some perhaps sharing vertices, edges, or larger faces. The simple example below depicts a -dimensional simplex () and a -dimensional simplex () glued together at a single vertex ():\nFormally, a simplicial complex is a set closed under subsets; that is, if , then . Intuitively, is the set of all triangles in the simplicial complex. Closure under subsets corresponds to the idea that any triangle (of any dimension) contains all of the lower-dimensional triangles that make it up; for example, a -dimensional triangle contains its edges (which are -dimensional triangles), and these edges contain their endpoint vertices (which are -dimensional triangles). Accordingly, every element is called a face, the elements in such an are called vertices, and those faces that are maximal with respect to set inclusion are called facets. For example, the diagram above corresponds to the simplicial complex\nwhich has facets and .\nThe basic idea behind interpreting epistemic logic using simplicial complexes is to associate vertices with agents and to view facets as analogous to possible worlds [KaSC, SimpDEL]. Intuitively, a vertex associated to agent represents a particular ‚Äúperspective‚Äù of , with the simplicial structure encoding which agent perspectives are epistemically compatible with which others. Thus, whereas in standard relational semantics ‚Äôs knowledge is given by quantifying over those worlds that ‚Äúconsiders possible‚Äù (via an accessibility relation), in the simplicial setting this quantification is instead over those facets that share a vertex associated to agent . We formalize all the details in Section 2.\nAs noted, the goal of this paper is to generalize this semantics to provide a model for multi-agent belief instead of (or in addition to) knowledge. A key obstacle to this endeavor is that simplicial semantics hardcodes factivity () in a way that relational semantics does not. Naturally, models for belief ought not validate the corresponding principle (), since agents can have false beliefs.\nIn relational semantics, knowledge is typically weakened to belief by passing to a subset of those worlds considered possible. More precisely, if denotes the set of all worlds epistemically accessible from , then a reasonable candidate for a relation representing doxastic accessibility (i.e., beliefs) is one where (so also ). This ensures that knowledge implies belief () and allows agents to potentially believe falsehoods even though they cannot know falsehoods (because need not be reflexive even though is assumed to be).\nSomewhat surprisingly, the obvious analogous approach does not work out in the simplicial setting. Suppose our model begins with a simplicial complex in the background (representing knowledge, as usual), and adds a subcomplex to represent the beliefs of each agent‚Äîso to determine an agent‚Äôs beliefs we would only quantify over facets in . Then, as desired, for a given facet in , it might be the case that is true at yet at all facets in which are -accessible from , is false. This produces a counterexample to factivity for belief. However, at all facets in , every agent will have factive beliefs, and as a consequence this model does not allow agents to consider it possible that any other agents have false beliefs; this is formalized in Proposition 2.1.\nOur solution to this issue is to enrich the model with multiple ‚Äúbelief‚Äù subcomplexes‚Äîone for each agent. This solves the problem outlined above, and leads naturally to the question of how these enriched simplicial models are related to standard relational models for knowledge and belief. This question, too, is not entirely straightforward, since the standard translation between simplicial and relational frameworks requires a ‚Äúproperness‚Äù condition that turns out to be quite restrictive when it comes to representing beliefs. We discuss this translation, and its restrictiveness, in Section 3, and then leverage (a slight generalization of) the work in [Note1] to resolve the issue.\nWe are not the first to propose a representation of belief using simplicial complexes. Both ‚ÄúKnowledge and Simplicial Complexes‚Äù [KaSC] as well as ‚ÄúSimplicial Belief‚Äù [SimpBel] tackle the same idea. While each of these approaches is interesting in its own right, they differ substantially from our models both in formal implementation and in the overall logic of belief that they validate. We outline these frameworks and discuss their relationship to our models in Section 5.\nThe rest of the paper is organized as follows. In Section 2, we provide the core definitions, introduce our new model, and prove that in this framework multiple complexes are needed to allow the right kind of inter-agent uncertainty. In Section [ADDRESS_REMOVED] relational models for knowledge and belief and our simplicial representation, with particular attention to the properness condition. Section 4 applies this translation to prove soundness and completeness. Section 5 concludes with a discussion of related and future work.\n2 Semantics for Belief\nThe literature on simplicial semantics divides roughly into two approaches to defining valuations for propositional atoms. Loosely speaking, the first approach assigns truth values directly to the facets [Death, FA, SimpSet], while the second ‚Äúvertex-based‚Äù approach assigns them (in a partial way) to the vertices and then ‚Äúlifts‚Äù them to facets. [DoA, KaSC, SimpDEL, HG] While we feel that the latter approach is more interesting from a logical and epistemic perspective, the former is much more technically straightforward and will allow us to better illustrate the specific novelty of the belief models we wish to present in this work, so that is the approach we adopt here.\nLet be a finite set of agents. A simplicial frame is a tuple where is a set of nodes, assigns an agent to each node (this is sometimes called the coloring function), and is a simplicial complex. Let denote the set of facets of . As in much of the previous literature [KaSC, SimpDEL], we henceforth restrict attention to simplicial frames that satisfy the uniquely colored facets (UCF) condition: for each and each , there is exactly one node such that . Note that this implies that all facets of contain exactly nodes, one for each agent.\nLet be a countable set of propositional atoms. A simplicial model is a simplicial frame together with a valuation function ; intuitively, tells us which facets is true at. In this setting, the language recursively defined by\nwhere and , can be interpreted in simplicial models as follows:\nwhere , that is, maps each facet to the unique -colored vertex it contains. The other Boolean connectives are defined in the usual way.\nThis is the standard simplicial semantics for multi-agent knowledge: agent knows at facet just in case is true at all facets that share an -colored node with . Note that the factivity of knowledge follows immediately: if , then since trivially , the semantic clause for forces us to conclude that .\nTo incorporate belief‚Äîwhich of course we don‚Äôt want to be factive‚Äîwe add in additional simplicial complexes. A simplicial belief model is a tuple , where is a simplicial model and each is a nonempty simplicial complex contained in ; we further assume that each complex satisfies the UCF condition. Note that this guarantees that . Expand the language recursively to include unary belief modalities for each agent and call the result . Given a simplicial belief model , we can then enrich the semantics with the following clause:\nSo this is just like the clause for knowledge, except for each agent we quantify only over facets in rather than all the facets of . Because of this, belief is not in general forced to be factive, since at any we can have but (for example, if ).\nAs discussed in Section 1, a natural question that arises is why we need a different simplicial complex for each agent . After all, simplicial models for multi-agent knowledge make do with just one. The issue is made clear in the following proposition.\nProposition 2.1.\nLet be a simplicial belief model with . Then .\nProof.\nLet , and consider any such that . If , then by definition for all such that we must have . Since and by assumption, we know that , so in fact we must have . This shows that and thus , as desired. ‚àé\nIn other words, when and coincide, agent becomes incapable of considering agent fallible with respect to their beliefs (and, of course, vice-versa). This is not an issue for multi-agent knowledge because knowledge is not fallible at all (so, naturally, agents know this).\n3 Connection to Relational Semantics\nIt is well-known [KaSC] that a broad class of relational models for multi-agent knowledge can be transformed into simplicial models in a truth-preserving manner. We briefly review this transformation here in order to illustrate how it works‚Äîand where it fails‚Äîwhen belief is added into the mix.\nRecall that a relational model (over ) is a tuple where is a nonempty set of possible worlds, each is a binary relation on called an accessibility relation, and is a valuation. Semantics are given in the usual way:\nwhere . A relational frame is a model without the valuation.\nWe restrict our attention to relational models where each is an equivalence relation‚Äîsuch models validate factivity (), positive introspection (), and negative introspection ()‚Äîand refer to these as relational models for introspective knowledge. Since these principles are also validated by all simplicial models, we must impose them on relational structures in order to have any hope of a truth-preserving transformation. We write to denote the equivalence class of with respect to .\nGiven a relational model as above, we define a simplicial frame as follows:\n-\n‚Ä¢\n-\n‚Ä¢\n-\n‚Ä¢\n,\nwhere is projection to the first coordinate (returning the equivalence class associated with each node). Intuitively, each equivalence class captures a certain epistemic ‚Äúperspective‚Äù of agent , and thus we take our nodes to be all such equivalence classes (labelled by the agent in question). The simplicial structure is then determined by grouping precisely those nodes that correspond to equivalence classes with nonempty intersection, since these are the perspectives that can all obtain simultaneously (i.e., at the worlds in the intersection). Thus, each face of corresponds to nonempty intersection of equivalence classes from distinct agents.\nProposition 3.1.\nThe simplicial frame satisfies UCF.\nProof.\nWe must show that each facet contains exactly one node for each agent. First observe that no face in can contain more than one -colored node, since if and are distinct, then and must also be distinct and thus have empty intersection.\nNow suppose that for some , does not contain an -colored node. By definition, there exists some ; it follows immediately that , and thus . This shows that is not a facet (since it‚Äôs not maximal), which in turn implies that any facet must contain an -colored node for every . ‚àé\nThe latter half of the proof above suggests a natural way to associate worlds in with facets in the simplicial frame we have constructed: namely, define by\nIt is not hard to see that is surjective, but unfortunately, it may not be injective: precisely when, for all , . In this case there is no clear way to define a valuation on that ‚Äúsimulates‚Äù the valuation in , since two worlds that disagree about the truth value of , say, may be associated to the same facet.\nThis motivates the following definition that has become standard in the literature [KaSC]: a relational model is called proper if, for all , . From this it easily follows that is injective. Define and let be the simplicial model . We then have:\nTheorem 3.2.\nLet be a proper relational model. For all formulas , for all ,\nProof.\nSee Appendix A. ‚àé\nThe existing literature has largely taken properness to be a technical condition, introduced in order to facilitate translations from relational to simplicial models. [KaSC, DoA, Death, SimpDEL] Whether it is a reasonable restriction, in the sense of being epistemically motivated or defensible, is debatable (and actively debated by the authors of this paper!).111One could argue that it eliminates a kind of ‚Äúredundancy of perspectives‚Äù, though this depends on a further philosophical supposition that every relevant fact about a world is captured in some agent‚Äôs perspective. What is less contentious is that once we incorporate belief into the models, properness is far too strong a requirement. A concrete example will help to illustrate this.\nA relational model for introspective knowledge and belief (over ) is just a relational model for introspective knowledge supplemented with new relations , one for each ; we require that each be serial and constant on -equivalence classes: that is, if then . These new relations are used to interpret belief modalities in the language in the usual way, namely:\nUnder these semantics, the properties of listed above ensure that our models validate knowledge implies belief (), consistency of belief (), strong positive introspection for belief (), and strong negative introspection for belief (.\nConsider the model depicted in Figure 1 for three agents ; the red, blue, and green arrows display the relations , , and , respectively. In particular, at all worlds , agent considers only and possible, agent is sure that the true world is , and agent is sure that the true world is . Note that we have not explicitly depicted the relations , , or , but because each of these must be an equivalence relation containing the corresponding belief relation, there is only one possibility: they are all the complete relation. Thus, the corresponding relational model for introspective knowledge is not proper and so cannot be translated into a simplicial model.\nHowever, there does not seem to be anything trivial or redundant about this model of belief; as such, an inability to translate it into the simplicial setting is a substantive restriction. Fortunately, while this model itself cannot be so translated, there does exist an equivalent (in the sense of bisimilarity) model that can be. This follows from a modest extension of the following result:\nTheorem 3.3 ([Note1], Proposition 2.4).\nLet be a relational model. Then there is a relational model which is proper and a surjective, bounded morphism from to . Furthermore, if is transitive (respectively, Euclidean, serial, reflexive, symmetric), then is also transitive (respectively, Euclidean, serial, reflexive, symmetric).\nRoughly speaking, by expanding this result to apply also to belief, we can translate the model in Figure 1 into a proper model that ‚Äúsimulates‚Äù it via an appropriate surjective, bounded morphism. This will yield the model depicted in Figure 2.\nIn this case, the promised bounded morphism maps to , thus ensuring that these points always have the same theory and preserving the logical information we started with. Moreover, because the relational model in Figure 2 is proper, we are able to transform it into a simplicial model using (a slight generalization of) Theorem 3.2. The result of this transformation is shown in Figure 3.\nIn the next section we fill in the details omitted above‚Äînamely, generalizing Theorems 3.3 and 3.2 to relational models for knowledge and belief‚Äîand use these results to establish a novel completeness result.\nBefore turning to the technical details, it is worth pausing to reflect on the properness condition in light of this example. Intuitively, the model shown in Figure 1 encodes a single perspective for each agent: the set of doxastically accessible worlds is constant for each of , , and , after all. However, with only one perspective for every agent, simplicial semantics only has the resources to produce a single facet, which is not enough to capture this scenario. As such, any technique for translating relational models into simplicial models must sometimes proliferate agential perspectives; indeed, Figure 2 has 9 worlds instead of 3 and intuitively includes 3 ‚Äúperspectives‚Äù for each agent, each of which is a ‚Äúcopy‚Äù of that agent‚Äôs original, single perspective (up to formula satisfaction). In Section 5, we discuss how the more general notion of simplicial sets may be a useful framework for avoiding this redundancy of representation.\n4 Soundness and Completeness\nRecall that relational models for introspective knowledge and belief validate knowledge implies belief (), strong positive introspection (), and strong negative introspection (. Let FULL be the axiom system obtained by adding these three axiom schemes to the system S5 for knowledge combined with KD45 for belief.222For an overview of these standard axiom systems, see for example [Bjorndahl2024].\nTheorem 4.1.\nFULL is sound and complete with respect to the class of all simplicial belief models.\nProof.\nWe prove the result in much the same way as has been done for comparable results in previous literature. [KaSC, SimpDEL] Soundness is straightforward so we focus on completeness. We first generate a canonical relational model for introspective knowledge and belief, using maximal FULL-consistent sets in the usual way. Next we transform this canonical model into an equivalent, proper model using a generalization of the result from [Note1]. Finally, we translate this proper model into a simplicial belief model using essentially the same technique as in Section 3.\nLet be the collection of maximal FULL-consistent sets. For arbitrary , we define as follows: if and only if for all formulas , if , then . Similarly, let be given by if and only if for all formulas , if , then . Lastly, define . Set .\nIt is well known that with the above definitions for relations, and the fact that the S5 axioms for knowledge are included in FULL, each is an equivalence relation. Indeed, it is straightforward to check that is a relational model for introspective knowledge and belief. For example, the fact that forces . We will show here in detail that each is constant on -equivalence classes, and leave the remaining, easy checks to the reader. Suppose . Suppose further that , and . From strong positive introspection it follows that . Since , we have that , and therefore . Since , it follows that . Since was arbitrary, this shows that , which suffices to show that is constant on -equivalence classes.\nIt is easy to show, in the standard way, that if and only if . Thus refutes every non-theorem of FULL. Our next step is to find a proper relational model for introspective knowledge and belief that does the same; for this we adapt the technique from [Note1].\nSince our language is countable, it follows that is of size continuum. Let be a bijection. Define and fix a distinguished ; for all , define\nand set\nFinally, define , and let . The rough idea here is that we are making -many copies of the model , but for one special agent, , we ‚Äúskew‚Äù their accessibility relations so they connect worlds across copies rather than within copies. This guarantees that (except for reflexive edges), agent ‚Äôs accessibility relations never coincide with any other agent‚Äôs, which in turn ensures the model is proper. For a more detailed exposition of the intuition behind this construction, see [Note1].\nSince for all , the definitions above guarantee that also for all . It‚Äôs also easy to check that for all , is an equivalence relation and is serial and constant on -equivalence classes. The main result of [Note1] now guarantees is proper (with respect to the relations ) and moreover, that the map given by projection to the first coordinate is a surjective, bounded morphism (again, with respect to the relations). It is easy to check that is also a bounded morphism with respect to the relations. It follows that every formula refuted in is also refuted in , as desired.\nThe final step is to transform into an equivalent simplicial belief model. Because is proper and each is an equivalence relation, we can define , , , and as we did above, in the lead-up to Theorem 3.2. What‚Äôs missing are the belief subcomplexes, which we define as follows:\nrecalling that is the bijective correspondence between worlds in and facets in . Specifying the facets of of course uniquely determines the simplex itself; it also guarantees that is a UCF subcomplex of . Of course, given any proper relational model for introspective knowledge and belief, , we can use the above definitions to produce a simplicial belief model . We now extend Theorem 3.2 in this context to the full language including belief modalities.\nLemma 4.2.\nLet be a proper relational model for introspective knowledge and belief (over ). For all formulas , for all ,\nProof.\nThe proof is identical to that of Theorem 3.2 except for the inductive step for the belief modalities, which must be added. So suppose inductively, as in that proof, that for all , . We wish to show that for all , .\nFix and suppose . Let be such that . It follows that and . Because is constant on equivalence classes, ; thus, , and so . By the inductive hypothesis, we have that . It follows that , as desired.\nSuppose now that . Let be such that . By the Euclidean property we have that . It follows that . Moreover, implies that , and so . It follows that . By the inductive hypothesis, we have that . Therefore, , as desired. ‚àé\nWe can now establish completeness. Suppose for simplicial belief models we have , and suppose for contradiction that . Let be a maximal FULL-consistent set extending (obtained, as usual, via Lindenbaum‚Äôs Lemma). Then . It then follows from the above that (for any ), , a contradiction. ‚àé\n[ADDRESS_REMOVED] to two existing approaches. In ‚ÄúKnowledge and Simplicial Complexes‚Äù [KaSC], standard UCF simplicial models are augmented with idempotent functions , one for each agent . These functions are used to define belief via the following clause:\nLoosely speaking, whereas a node represents a ‚Äúperspective‚Äù of agent for the purposes of evaluating knowledge modalities at facets containing , for the purposes of evaluating belief modalities at facets containing , this semantics instead uses the node . In other words, agent ‚Äôs beliefs at are given by her knowledge at . Because each is idempotent, it is easy to check that the modalities satisfy the KD45 axioms.\nHowever, this notion of belief has essentially no relationship to knowledge; in particular, is not valid. A natural solution to this might be to strengthen the semantics so as to validate this formula. Unfortunately, this results in triviality, for it requires the following to hold for all :\nunder the UCF assumption, this containment does not hold in general unless is the identity function. In this case, of course, belief and knowledge coincide. Thus, this model for belief cannot satisfy both UCF and the validity of ‚Äúknowledge implies belief‚Äù without trivializing belief.\nWe find a quite different approach to defining belief in simplicial complexes in ‚ÄúSimplicial Belief‚Äù. [SimpBel] The idea here depends on breaking the UCF assumption so that facets may contain one or more -colored nodes, and then using the number of nodes a given facet contains to determine whether or not it is ‚Äúaccessible‚Äù for the purposes of evaluating belief. Specifically, given , let count the number of -colored nodes in , and for any , define\nWrite if and only if such that , and define . These relations are used to define belief via the following clause:\nThis says that if is true at all facets whose intersection with contains an -colored node, and such that the number of -colored nodes in is bounded above by the number in , then is true at .333The paper also provides a stronger notion of belief, taking the belief relation to access worlds with a minimum number of perspectives, rather than all those with fewer than the starting world. The same result we show for the given definition also applies to this one. The rough intuition here, as articulated in [SimpBel], is that facets containing more -perspectives are less plausible (to agent ) than those containing fewer, since is in some sense less certain of their own state of mind in the former.\nIt‚Äôs easy to see that this definition validates . And it is certainly interesting from a formal perspective to relax the UCF assumption and leverage this extra freedom to define belief. However, the epistemic interpretation of facets with multiple -perspectives remains somewhat murky. Unfortunately, if we re-impose the UCF condition, this of course implies that for all , , making the total relation on , hence . In this case again we see that belief and knowledge coincide.\nIn contrast to these approaches, our framework provides a definition of belief in simplicial complexes satisfying all three of: (1) UCF, (2) validity of is valid, and (3) is not valid. This has a few further upshots. The usual techniques for translating frame models into simplicial models apply very cleanly (indeed, almost trivially) in our setting, yielding soundness and completeness with respect to the simple FULL axiom system (Theorem 4.1). The only wrinkle in the proof is the condition that the underlying relational frame be proper. This is circumvented using the result in [Note1], where it is shown that any improper relational model can be translated into an equivalent, proper one. This result, too, is easily lifted to the setting where there are both knowledge and belief relations.\nAs mentioned above, the existing literature has largely taken properness as a technical condition, introduced in order to facilitate translations from relational to simplicial models. [KaSC, DoA, Death, SimpDEL] However, as noted at the end of Section 3, in order to ‚Äúmake‚Äù a model proper we have to multiply worlds in a way that produces substantial redundancy. As a result, the structures become harder to parse‚Äîboth the relational models and their simplicial correlates. We suspect, for example, that few will find the simplicial model in Figure 3 as easy to understand as the relational model in Figure 1.\nIt is natural to wonder, then, if there is any way to avoid imposing properness altogether. One promising approach utilizes the notion of simplicial sets.444The idea of using simplicial sets in the context of simplicial models was introduced in [SimpSet]. Roughly speaking, simplicial sets generalize simplicial complexes by allowing us to count faces with multiplicity, similar to the generalization of a set to a multiset, or a graph to a multigraph. For instance, in a simplicial complex, the triangle is uniquely determined by the three nodes it contains, whereas in a simplicial set, there could be multiple triangles on the same underlying set of nodes . Visually, one may picture multiple triangular ‚Äúmembranes‚Äù connecting these nodes. Informally, we may define a simplicial set as a ‚Äúmultiset of multisets closed under subsets‚Äù.\nUsing this structure, one can capture the model in Figure 1 with a simplicial set together with subsimplicial sets , , and , as depicted in Figure 4.\nIntuitively, the node corresponds to the (unique!) perspective that agent has in Figure 1, namely the worlds and . Similarly, corresponds to the view that only is possible, and corresponds to only seeing . But we now have the freedom to associate not one but three facets with this set of perspectives. The black facet corresponds with , the world accessible to none of the three agents. The purple facet belongs to and , but not , corresponding to , the world accessible to agents and but not . And similarly the orange facet belongs to and , but not , corresponding to . Thus we see that simplicial sets give us the resources to represent non-proper models without artificially proliferating perspectives; this provides a promising direction for future research in simplicial semantics and particularly in the representation of belief therein.\nAppendix A Proofs\nProof of Theorem 3.2.\nInduction on formulas. Let . First suppose ; then , so , which yields , as desired. Conversely, suppose ; then . Because is injective, we must then have , which yields as desired.\nThe case for is trivial.\nNow suppose inductively that for all , and . We wish to show that this equivalence holds also for and for . For the former, observe that if and only if implies , which (by the inductive hypothesis) is true if and only if implies . And of course this is true if and only if , as desired.\nAt last we turn to the knowledge modalities. Suppose . Fix arbitrary such that . By definition of , . Because is surjective, we can find such that . So , and therefore , meaning . We know from our initial supposition that for all , . So, by the inductive hypothesis, we have , hence ; since was chosen arbitrarily, this yields .\nConversely, suppose . By definition, for all such that , . Let . Then, since , we must have , so . By the inductive hypothesis, then, , and since was chosen arbitrarily, we conclude .\n‚àé"
  }
]